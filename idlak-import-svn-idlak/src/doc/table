<tr> <td> \ref bin/align-equal.cc "align-equal" </td><td> <pre> Write equally spaced alignments of utterances (to get training started)
Usage:  align-equal <tree-in> <model-in> <lexicon-fst-in> <features-rspecifier> <transcriptions-rspecifier> <alignments-wspecifier>
e.g.: 
 align-equal 1.tree 1.mdl lex.fst scp:train.scp ark:train.tra ark:equal.ali </pre> </td> </tr>
<tr> <td> \ref bin/align-equal-compiled.cc "align-equal-compiled" </td><td> <pre> Write an equally spaced alignment (for getting training started)Usage:  align-equal-compiled <graphs-rspecifier> <features-rspecifier> <alignments-wspecifier>
e.g.: 
 align-equal-compiled 1.mdl 1.fsts scp:train.scp ark:equal.ali </pre> </td> </tr>
<tr> <td> \ref bin/acc-tree-stats.cc "acc-tree-stats" </td><td> <pre> Accumulate statistics for phonetic-context tree building.
Usage:  acc-tree-stats [options] model-in features-rspecifier alignments-rspecifier [tree-accs-out]
e.g.: 
 acc-tree-stats 1.mdl scp:train.scp ark:1.ali 1.tacc </pre> </td> </tr>
<tr> <td> \ref bin/show-alignments.cc "show-alignments" </td><td> <pre> Display alignments in human-readable form
Usage:  show-alignments  [options] <phone-syms> <model> <alignments-rspecifier>
e.g.: 
 show-alignments phones.txt 1.mdl ark:1.ali
See also: ali-to-phones </pre> </td> </tr>
<tr> <td> \ref bin/compile-questions.cc "compile-questions" </td><td> <pre> Compile questions from the text format into the Kaldi Questions class.
The text format has a set of integers per line 'p1 p2 ...' corresponding to sets of phones.
Arbitrary questions may be provided using the --keyed-questions option.
The --context-width option is only for phonetic context, and not the 
total number of keys when using the --keyed-questions option.

Usage:  compile-questions [options] <topo> <questions-text-file> <questions-out>
e.g.: 
 compile-questions topo.txt questions.txt questions.qst </pre> </td> </tr>
<tr> <td> \ref bin/cluster-phones.cc "cluster-phones" </td><td> <pre> Cluster phones (or sets of phones) into sets for various purposes
Usage:  cluster-phones [options] <tree-stats-in> <phone-sets-in> <clustered-phones-out>
e.g.: 
 cluster-phones 1.tacc phonesets.txt questions.txt </pre> </td> </tr>
<tr> <td> \ref bin/compute-wer.cc "compute-wer" </td><td> <pre> Compute WER by comparing different transcriptions
Takes two transcription files, in integer or text format,
and outputs overall WER statistics to standard output.
Optionally, the third argument can be used to obtain detailed statistics

Usage: compute-wer [options] <ref-rspecifier> <hyp-rspecifier> [<stats-out>]

E.g.: compute-wer --text --mode=present ark:data/train/text ark:hyp_text
or: compute-wer --text --mode=present ark:data/train/text ark:hyp_text - | \\
   sort | uniq -c </pre> </td> </tr>
<tr> <td> \ref bin/make-h-transducer.cc "make-h-transducer" </td><td> <pre> Make H transducer from transition-ids to context-dependent phones, 
 without self-loops [use add-self-loops to add them]
Usage:   make-h-transducer <ilabel-info-file> <tree-file> <transition-gmm/acoustic-model> [<H-fst-out>]
e.g.: 
 make-h-transducer ilabel_info  1.tree 1.mdl > H.fst </pre> </td> </tr>
<tr> <td> \ref bin/add-self-loops.cc "add-self-loops" </td><td> <pre> Add self-loops and transition probabilities to transducer.  Input transducer
has transition-ids on the input side, but only the forward transitions, not the
self-loops.  Output transducer has transition-ids on the input side, but with
self-loops added.  The --reorder option controls whether the loop is added before
the forward transition (if false), or afterward (if true).  The default (true)
is recommended as the decoding will in that case be faster.
Usage:   add-self-loops [options] transition-gmm/acoustic-model [fst-in] [fst-out]
e.g.: 
 add-self-loops --self-loop-scale=0.1 1.mdl HCLGa.fst HCLG.fst
or:  add-self-loops --self-loop-scale=0.1 1.mdl <HCLGa.fst >HCLG.fst </pre> </td> </tr>
<tr> <td> \ref bin/convert-ali.cc "convert-ali" </td><td> <pre> Convert alignments from one decision-tree/model to another
Usage:  convert-ali  [options] old-model new-model new-tree old-alignments-rspecifier new-alignments-wspecifier
e.g.: 
 convert-ali old.mdl new.mdl new.tree ark:old.ali ark:new.ali </pre> </td> </tr>
<tr> <td> \ref bin/compile-train-graphs.cc "compile-train-graphs" </td><td> <pre> Creates training graphs (without transition-probabilities, by default)

Usage:   compile-train-graphs [options] <tree-in> <model-in> <lexicon-fst-in> <transcriptions-rspecifier> <graphs-wspecifier>
e.g.: 
 compile-train-graphs tree 1.mdl lex.fst ark:train.tra ark:graphs.fsts </pre> </td> </tr>
<tr> <td> \ref bin/compile-train-graphs-fsts.cc "compile-train-graphs-fsts" </td><td> <pre> Creates training graphs (without transition-probabilities, by default)
This version takes FSTs as inputs (e.g., representing a separate weighted
grammar for each utterance)
Note: the lexicon should contain disambiguation symbols and you should
supply the --read-disambig-syms option which is the filename of a list
of disambiguation symbols.
Warning: you probably want to set the --transition-scale and --self-loop-scale
options; the defaults (zero) are probably not appropriate.
Usage:   compile-train-graphs-fsts [options] <tree-in> <model-in> <lexicon-fst-in>  <graphs-rspecifier> <graphs-wspecifier>
e.g.: 
 compile-train-graphs-fsts --read-disambig-syms=disambig.list\\
   tree 1.mdl lex.fst ark:train.fsts ark:graphs.fsts </pre> </td> </tr>
<tr> <td> \ref bin/arpa2fst.cc "arpa2fst" </td><td> <pre> Converts an ARPA format language model into a FST
Usage: arpa2fst [opts] (input_arpa|-)  [output_fst|-] </pre> </td> </tr>
<tr> <td> \ref bin/make-pdf-to-tid-transducer.cc "make-pdf-to-tid-transducer" </td><td> <pre> Make transducer from pdfs to transition-ids
Usage:   make-pdf-to-tid-transducer model-filename [fst-out]
e.g.: 
 make-pdf-to-tid-transducer 1.mdl > pdf2tid.fst </pre> </td> </tr>
<tr> <td> \ref bin/make-ilabel-transducer.cc "make-ilabel-transducer" </td><td> <pre> Make transducer that de-duplicates context-dependent ilabels that map to the same state
Usage:   make-ilabel-transducer ilabel-info-right tree-file transition-gmm/model ilabel-info-left [mapping-fst-out]
e.g.: 
 make-ilabel-transducer old_ilabel_info 1.tree 1.mdl new_ilabel_info > convert.fst </pre> </td> </tr>
<tr> <td> \ref bin/show-transitions.cc "show-transitions" </td><td> <pre> Print debugging info from transition model, in human-readable form
Usage:  show-transitions <phones-symbol-table> <transition/model-file> [<occs-file>]
e.g.: 
 show-transitions phones.txt 1.mdl 1.occs </pre> </td> </tr>
<tr> <td> \ref bin/ali-to-phones.cc "ali-to-phones" </td><td> <pre> Convert model-level alignments to phone-sequences (in integer, not text, form)
Usage:  ali-to-phones  [options] <model> <alignments-rspecifier> <phone-transcript-wspecifier|ctm-wxfilename>
e.g.: 
 ali-to-phones 1.mdl ark:1.ali ark:phones.tra
or:
 ali-to-phones --ctm-output 1.mdl ark:1.ali 1.ctm
See also: show-alignments lattice-align-phones </pre> </td> </tr>
<tr> <td> \ref bin/ali-to-post.cc "ali-to-post" </td><td> <pre> Convert alignments to posteriors
Usage:  ali-to-post [options] <alignments-rspecifier> <posteriors-wspecifier>
e.g.:
 ali-to-post ark:1.ali ark:1.post </pre> </td> </tr>
<tr> <td> \ref bin/weight-silence-post.cc "weight-silence-post" </td><td> <pre> Apply weight to silences in posts
Usage:  weight-silence-post [options] <silence-weight> <silence-phones> <model> <posteriors-rspecifier> <posteriors-wspecifier>
e.g.:
 weight-silence-post 0.0 1:2:3 1.mdl ark:1.post ark:nosil.post </pre> </td> </tr>
<tr> <td> \ref bin/acc-lda.cc "acc-lda" </td><td> <pre> Accumulate LDA statistics based on pdf-ids.
Usage:  acc-lda [options] <transition-gmm/model> <features-rspecifier> <posteriors-rspecifier> <lda-acc-out>
Typical usage:
 ali-to-post ark:1.ali ark:- | lda-acc 1.mdl "ark:splice-feats scp:train.scp|"  ark:- ldaacc.1 </pre> </td> </tr>
<tr> <td> \ref bin/est-lda.cc "est-lda" </td><td> <pre> Estimate LDA transform using stats obtained with acc-lda.
Usage:  est-lda [options] <lda-matrix-out> <lda-acc-1> <lda-acc-2> ... </pre> </td> </tr>
<tr> <td> \ref bin/ali-to-pdf.cc "ali-to-pdf" </td><td> <pre> Converts alignments (containing transition-ids) to pdf-ids, zero-based.
Usage:  ali-to-pdf  [options] <model> <alignments-rspecifier> <pdfs-wspecifier>
e.g.: 
 ali-to-pdf 1.mdl ark:1.ali ark, t:- </pre> </td> </tr>
<tr> <td> \ref bin/ali-to-hmmstate.cc "ali-to-hmmstate" </td><td> <pre> Converts alignments (containing transition-ids) to hmm states.
Usage:  ali-to-hmmstate  [options] <model> <alignments-rspecifier> <hmmstate-wspecifier>
e.g.: 
 ali-to-pdf 1.mdl ark:1.ali ark, t:- </pre> </td> </tr>
<tr> <td> \ref bin/est-mllt.cc "est-mllt" </td><td> <pre> Do update for MLLT (also known as STC)
Usage:  est-mllt [options] <mllt-mat-out> <stats-in1> <stats-in2> ... 
e.g.: est-mllt 2.mat 1a.macc 1b.macc ... 
where the stats are obtained from gmm-acc-mllt
Note: use compose-transforms <mllt-mat-out> <prev-mllt-mat> to combine with previous
  MLLT or LDA transform, if any, and
  gmm-transform-means to apply <mllt-mat-out> to GMM means. </pre> </td> </tr>
<tr> <td> \ref bin/build-tree.cc "build-tree" </td><td> <pre> Train decision tree
Usage:  build-tree [options] <tree-stats-in> <roots-file> <questions-file> <topo-file> <tree-out>
e.g.: 
 build-tree treeacc roots.txt 1.qst topo tree </pre> </td> </tr>
<tr> <td> \ref bin/build-tree-two-level.cc "build-tree-two-level" </td><td> <pre> Trains two-level decision tree.  Outputs the larger tree, and a mapping from the
leaf-ids of the larger tree to those of the smaller tree.  Useful, for instance,
in tied-mixture systems with multiple codebooks.

Usage:  build-tree-two-level [options] <tree-stats-in> <roots-file> <questions-file> <topo-file> <tree-out> <mapping-out>
e.g.: 
 build-tree-two-level treeacc roots.txt 1.qst topo tree tree.map </pre> </td> </tr>
<tr> <td> \ref bin/decode-faster.cc "decode-faster" </td><td> <pre> Decode, reading log-likelihoods (of transition-ids or whatever symbol is on the graph)
as matrices.  Note: you'll usually want decode-faster-mapped rather than this program.

Usage:   decode-faster [options] <fst-in> <loglikes-rspecifier> <words-wspecifier> [<alignments-wspecifier>] </pre> </td> </tr>
<tr> <td> \ref bin/decode-faster-mapped.cc "decode-faster-mapped" </td><td> <pre> Decode, reading log-likelihoods as matrices
 (model is needed only for the integer mappings in its transition-model)
Usage:   decode-faster-mapped [options] <model-in> <fst-in> <loglikes-rspecifier> <words-wspecifier> [<alignments-wspecifier>] </pre> </td> </tr>
<tr> <td> \ref bin/vector-scale.cc "vector-scale" </td><td> <pre> Scale a set of vectors in a Table (useful for speaker vectors and per-frame weights)
Usage: vector-scale [options] <in-rspecifier> <out-wspecifier> </pre> </td> </tr>
<tr> <td> \ref bin/copy-transition-model.cc "copy-transition-model" </td><td> <pre> Copies a transition model (this can be used to separate transition 
 models from the acoustic models they are written with.
Usage:  copy-transition-model [options] <transition-model or model file> <transition-model-out>
e.g.: 
 copy-transition-model --binarhy=false 1.mdl 1.txt </pre> </td> </tr>
<tr> <td> \ref bin/rand-prune-post.cc "rand-prune-post" </td><td> <pre> Randomized pruning of posteriors less than threshold
Note: for posteriors derived from alignments, threshold must be
greater than one, or this will have no effect (speedup factor will
be roughly the same as the threshold)
Usage:  rand-prune-post [options] <rand-prune-value> <posteriors-rspecifier> <posteriors-wspecifier>
e.g.:
 rand-prune-post 5.0 ark:- ark:- </pre> </td> </tr>
<tr> <td> \ref bin/phones-to-prons.cc "phones-to-prons" </td><td> <pre> Convert pairs of (phone-level, word-level) transcriptions to
output that indicates the phones assigned to each word.
Format is standard format for archives of vector<vector<int32> >
i.e. :
utt-id  600 4 7 19 ; 512 4 18 ; 0 1
where 600, 512 and 0 are the word-ids (0 for non-word phones, e.g.
optional-silence introduced by the lexicon), and the phone-ids
follow the word-ids.
Note: L_align.fst must have word-start and word-end symbols in it

Usage:  phones-to-prons [options] <L_align.fst> <word-start-sym> <word-end-sym> <phones-rspecifier> <words-rspecifier> <prons-wspecifier>
e.g.: 
 ali-to-phones 1.mdl ark:1.ali ark:- | \\
  phones-to-prons L_align.fst 46 47 ark:- 1.tra ark:1.prons </pre> </td> </tr>
<tr> <td> \ref bin/prons-to-wordali.cc "prons-to-wordali" </td><td> <pre> Caution: this program relates to older scripts and is deprecated,
for modern scripts see egs/wsj/s5/steps/{get_ctm,get_train_ctm}.sh
Given per-utterance pronunciation information as output by 
words-to-prons, and per-utterance phone alignment information
as output by ali-to-phones --write-lengths, output word alignment
information that can be turned into the ctm format.
Outputs is pairs of (word, #frames), or if --per-frame is given,
just the word for each frame.
Note: zero word-id usually means optional silence.
Format is standard format for archives of vector<pair<int32, int32> >
i.e. :
utt-id  600 22 ; 1028 32 ; 0 41
where 600, 1028 and 0 are the word-ids, and 22, 32 and 41 are the
lengths.

Usage:  prons-to-wordali [options] <prons-rspecifier> <phone-lengths-rspecifier> <wordali-wspecifier>
e.g.: 
 ali-to-phones 1.mdl ark:1.ali ark:- | \\
  phones-to-prons L_align.fst 46 47 ark:- 1.tra ark:- | \\
  prons-to-wordali ark:- \\
    "ark:ali-to-phones --write-lengths 1.mdl ark:1.ali ark:-|" ark:1.wali </pre> </td> </tr>
<tr> <td> \ref bin/copy-gselect.cc "copy-gselect" </td><td> <pre> Copy Gaussian indices for pruning, possibly making the
lists shorter (e.g. the --n=10 limits to the 10 best indices
See also gmm-gselect, fgmm-gselect
Usage: 
 copy-gselect [options] <gselect-rspecifier> <gselect-wspecifier> </pre> </td> </tr>
<tr> <td> \ref bin/copy-tree.cc "copy-tree" </td><td> <pre> Copy decision tree (possibly changing binary/text format)
Usage:  copy-tree [--binary=false] <tree-in> <tree-out> </pre> </td> </tr>
<tr> <td> \ref bin/scale-post.cc "scale-post" </td><td> <pre> Scale posteriors with either a global scale, or a different scale for  each utterance.
Usage: scale-post <post-rspecifier> (<scale-rspecifier>|<scale>) <post-wspecifier> </pre> </td> </tr>
<tr> <td> \ref bin/compute-mce-scale.cc "compute-mce-scale" </td><td> <pre> compute the scale of MCE, which is used to scale posteriors
Usage: compute-mce-scale [option] num-score-rspecifier den-score-rspecifier out-scale-wspecifier </pre> </td> </tr>
<tr> <td> \ref bin/get-silence-probs.cc "get-silence-probs" </td><td> <pre> This program takes two archives of Vector<BaseFloat>, representing
per-frame log-likelihoods for silence and non-silence models respectively.
It outputs per-frame silence probabilities in the same format.
To get non-silence probs instead, use --write-nonsil-probs Usage:  get-silence-probs [options] <silence-loglikes-rspecifier>  <nonsilence-loglikes-rspecifier> <silence-probs-wspecifier>
e.g.: get-silence-probs --silence-prior=0.9 --quantize=0.25 ark:sil.likes ark:nonsil.likes ark:sil.probs </pre> </td> </tr>
<tr> <td> \ref bin/post-to-weights.cc "post-to-weights" </td><td> <pre> Turn posteriors into per-frame weights (typically most useful after
weight-silence-post, to get silence weights)
See also: weight-silence-post, post-to-pdf-post, post-to-phone-post
get-post-on-ali
Usage: post-to-weights <post-rspecifier> <weights-wspecifier> </pre> </td> </tr>
<tr> <td> \ref bin/reverse-weights.cc "reverse-weights" </td><td> <pre> Modify per-frame weights by outputting 1.0-weight (if --reverse=true);
if --reverse=false, do nothing to them.
Usage: reverse-weights weights-rspecifier weights-wspecifier </pre> </td> </tr>
<tr> <td> \ref bin/dot-weights.cc "dot-weights" </td><td> <pre> Takes two archives of vectors (typically representing per-frame weights)
and for each utterance, outputs the dot product.
Useful for evaluating the accuracy of silence classifiers.
Usage: dot-weights <weights-rspecifier1> <weights-rspecifier2> <float-wspecifier> </pre> </td> </tr>
<tr> <td> \ref bin/sum-tree-stats.cc "sum-tree-stats" </td><td> <pre> Sum statistics for phonetic-context tree building.
Usage:  sum-tree-stats [options] tree-accs-out tree-accs-in1 tree-accs-in2 ...
e.g.: 
 sum-tree-stats treeacc 1.treeacc 2.treeacc 3.treeacc </pre> </td> </tr>
<tr> <td> \ref bin/weight-post.cc "weight-post" </td><td> <pre> Takes archives (typically per-utterance) of posteriors and per-frame weights,
and weights the posteriors by the per-frame weights

Usage: weight-post <post-rspecifier> <weights-rspecifier> <post-wspecifier> </pre> </td> </tr>
<tr> <td> \ref bin/post-to-tacc.cc "post-to-tacc" </td><td> <pre> From posteriors, compute transition-accumulators
The output is a vector of counts/soft-counts, indexed by transition-id)
Note: the model is only read in order to get the size of the vector

Usage: post-to-tacc [options] <model> <post-rspecifier> <accs>
 e.g.: post-to-tacc --binary=false 1.mdl "ark:ali-to-post 1.ali|" 1.tacc </pre> </td> </tr>
<tr> <td> \ref bin/copy-matrix.cc "copy-matrix" </td><td> <pre> Copy matrices, or archives of matrices (e.g. features or transforms)
Also see copy-feats which has other format options

Usage: copy-matrix [options] <matrix-in-rspecifier> <matrix-out-wspecifier>
  or: copy-matrix [options] <matrix-in-rxfilename> <matrix-out-wxfilename>
 e.g.: copy-matrix --binary=false 1.mat -
   copy-matrix ark:2.trans ark,t:-
See also: copy-feats </pre> </td> </tr>
<tr> <td> \ref bin/copy-vector.cc "copy-vector" </td><td> <pre> Copy vectors, or archives of vectors (e.g. transition-accs; speaker vectors)

Usage: copy-vector [options] (<vector-in-rspecifier>|<vector-in-rxfilename>) (<vector-out-wspecifier>|<vector-out-wxfilename>)
 e.g.: copy-vector --binary=false 1.mat -
   copy-vector ark:2.trans ark,t:-
see also: dot-weights, append-vector-to-feats </pre> </td> </tr>
<tr> <td> \ref bin/copy-int-vector.cc "copy-int-vector" </td><td> <pre> Copy vectors of integers, or archives of vectors of integers 
(e.g. alignments)

Usage: copy-int-vector [options] (vector-in-rspecifier|vector-in-rxfilename) (vector-out-wspecifier|vector-out-wxfilename)
 e.g.: copy-int-vector --binary=false foo -
   copy-int-vector ark:1.ali ark,t:- </pre> </td> </tr>
<tr> <td> \ref bin/sum-post.cc "sum-post" </td><td> <pre> Sum two sets of posteriors for each utterance, e.g. useful in fMMI.
To take the difference of posteriors, use e.g. --scale2=-1.0

Usage: sum-post <post-rspecifier1> <post-rspecifier2> <post-wspecifier> </pre> </td> </tr>
<tr> <td> \ref bin/sum-matrices.cc "sum-matrices" </td><td> <pre> Sum matrices, e.g. stats for fMPE training
Usage:  sum-matrices [options] <mat-out> <mat-in1> <mat-in2> ...
e.g.:
 sum-matrices mat 1.mat 2.mat 3.mat </pre> </td> </tr>
<tr> <td> \ref bin/draw-tree.cc "draw-tree" </td><td> <pre> Outputs a decision tree description in GraphViz format
Usage: draw-tree [options] <phone-symbols> <tree>
e.g.: draw-tree phones.txt tree | dot -Gsize=8,10.5 -Tps | ps2pdf - tree.pdf </pre> </td> </tr>
<tr> <td> \ref bin/copy-int-vector-vector.cc "copy-int-vector-vector" </td><td> <pre> Copy vectors of vectors of integers, or archives thereof

Usage: copy-int-vector-vector [options] vector-in-(rspecifier|rxfilename) vector-out-(wspecifierwxfilename) </pre> </td> </tr>
<tr> <td> \ref bin/thresh-post.cc "thresh-post" </td><td> <pre> Down-weight posteriors that are lower than a supplied confidence threshold.
(for those below the weight, rather than set to zero we downweight according
to the --weight option)

Usage:  thresh-post [options] <posteriors-rspecifier> <posteriors-wspecifier>
e.g.: thresh-post --threshold=0.9 --scale=0.1 ark:- ark:- </pre> </td> </tr>
<tr> <td> \ref bin/align-mapped.cc "align-mapped" </td><td> <pre> Generate alignments, reading log-likelihoods as matrices.
 (model is needed only for the integer mappings in its transition-model)
Usage:   align-mapped [options] <tree-in> <trans-model-in> <lexicon-fst-in> <feature-rspecifier> <transcriptions-rspecifier> <alignments-wspecifier>
e.g.: 
 align-mapped tree trans.mdl lex.fst scp:train.scp ark:train.tra ark:nnet.ali </pre> </td> </tr>
<tr> <td> \ref bin/align-compiled-mapped.cc "align-compiled-mapped" </td><td> <pre> Generate alignments, reading log-likelihoods as matrices.
 (model is needed only for the integer mappings in its transition-model)
Usage:   align-compiled-mapped [options] trans-model-in graphs-rspecifier feature-rspecifier alignments-wspecifier
e.g.: 
 nnet-align-compiled trans.mdl ark:graphs.fsts scp:train.scp ark:nnet.ali
or:
 compile-train-graphs tree trans.mdl lex.fst ark:train.tra b, ark:- | \\
   nnet-align-compiled trans.mdl ark:- scp:loglikes.scp t, ark:nnet.ali </pre> </td> </tr>
<tr> <td> \ref bin/latgen-faster-mapped.cc "latgen-faster-mapped" </td><td> <pre> Generate lattices, reading log-likelihoods as matrices
 (model is needed only for the integer mappings in its transition-model)
Usage: latgen-faster-mapped [options] trans-model-in (fst-in|fsts-rspecifier) loglikes-rspecifier lattice-wspecifier [ words-wspecifier [alignments-wspecifier] ] </pre> </td> </tr>
<tr> <td> \ref bin/latgen-faster-mapped-parallel.cc "latgen-faster-mapped-parallel" </td><td> <pre> Generate lattices, reading log-likelihoods as matrices, using multiple decoding threads
 (model is needed only for the integer mappings in its transition-model)
Usage: latgen-faster-mapped-parallel [options] trans-model-in (fst-in|fsts-rspecifier) loglikes-rspecifier lattice-wspecifier [ words-wspecifier [alignments-wspecifier] ] </pre> </td> </tr>
<tr> <td> \ref bin/hmm-info.cc "hmm-info" </td><td> <pre> Write to standard output various properties of HMM-based transition model
Usage:  hmm-info [options] <model-in>
e.g.:
 hmm-info trans.mdl </pre> </td> </tr>
<tr> <td> \ref bin/pdf-to-counts.cc "pdf-to-counts" </td><td> <pre> Reads int32 vectors (same format as alignments, but typically
actually representing pdfs, e.g. output by ali-to-pdf), and outputs
counts for each index (typically one per per pdf), as a Vector<float>.

Usage:  pdf-to-counts [options] <pdfs-rspecifier> <counts-wxfilname>
e.g.: 
 ali-to-pdf "ark:gunzip -c 1.ali.gz|" ark:- | \\
   pdf-to-counts --binary=false ark:- counts.txt </pre> </td> </tr>
<tr> <td> \ref bin/analyze-counts.cc "analyze-counts" </td><td> <pre> Computes element counts from integer vector table.
(e.g. for example to get pdf-counts to estimate DNN-output priors, for data analysis)
Verbosity : level 1 => print frequencies and histogram

Usage:  analyze-counts  [options] <alignments-rspecifier> <counts-wxfilname>
e.g.: 
 analyze-counts ark:1.ali prior.counts
 Show phone counts by:
 ali-to-phone --per-frame=true ark:1.ali ark:- | analyze-counts --verbose=1 ark:- - >/dev/null </pre> </td> </tr>
<tr> <td> \ref bin/extract-ctx.cc "extract-ctx" </td><td> <pre> Given the tree stats and the resulting tree, output a mapping of phones
in context (and pdf-class) to the pdf-id.  This can be used to link the
acoustic model parameters to their phonetic meaning.  Outputs lines such as
  "<pdf-id> <pdf-class> <left> <center> <right>" in case of tri-phones
e.g.: 
 extract-ctx treeacc tree
 extract-ctx --mono 48 tree </pre> </td> </tr>
<tr> <td> \ref bin/post-to-phone-post.cc "post-to-phone-post" </td><td> <pre> Convert posteriors to phone-level posteriors
See also: post-to-pdf-post, post-to-weights, get-post-on-ali

Usage: post-to-phone-post [options] <model> <post-rspecifier> <phone-post-wspecifier>
 e.g.: post-to-phone-post --binary=false 1.mdl "ark:ali-to-post 1.ali|" ark,t:- </pre> </td> </tr>
<tr> <td> \ref bin/post-to-pdf-post.cc "post-to-pdf-post" </td><td> <pre> This program turns per-frame posteriors, which have transition-ids as
the integers, into pdf-level posteriors
See also: post-to-phone-post, post-to-weights, get-post-on-ali

Usage:  post-to-pdf-post [options] <model-file> <posteriors-rspecifier> <posteriors-wspecifier>
e.g.: post-to-pdf-post 1.mdl ark:- ark:- </pre> </td> </tr>
<tr> <td> \ref bin/duplicate-matrix.cc "duplicate-matrix" </td><td> <pre> Copy tables of BaseFloat matrices, from one input to possibly multiple outputs,
with each element of the input written too all outputs.

Usage: duplicate-matrix [options] <matrix-rspecifier> <matrix-wspecifier1> [<matrix-wspecifier2> ...] </pre> </td> </tr>
<tr> <td> \ref bin/logprob-to-post.cc "logprob-to-post" </td><td> <pre> Convert a matrix of log-probabilities (e.g. from nnet-logprob) to posteriors
Usage:  logprob-to-post [options] <logprob-matrix-rspecifier> <posteriors-wspecifier>
e.g.:
 nnet-logprob [args] | logprob-to-post ark:- ark:1.post
Caution: in this particular example, the output would be posteriors of pdf-ids,
rather than transition-ids (c.f. post-to-pdf-post) </pre> </td> </tr>
<tr> <td> \ref bin/prob-to-post.cc "prob-to-post" </td><td> <pre> Convert a matrix of probabilities (e.g. from nnet-logprob2) to posteriors
Usage:  prob-to-post [options] <prob-matrix-rspecifier> <posteriors-wspecifier>
e.g.:
 nnet-logprob2 [args] | prob-to-post ark:- ark:1.post
Caution: in this particular example, the output would be posteriors of pdf-ids,
rather than transition-ids (c.f. post-to-pdf-post) </pre> </td> </tr>
<tr> <td> \ref bin/copy-post.cc "copy-post" </td><td> <pre> Copy archives of posteriors, with optional scaling
(Also see rand-prune-post and sum-post)

Usage: copy-post <post-rspecifier> <post-wspecifier> </pre> </td> </tr>
<tr> <td> \ref bin/matrix-logprob.cc "matrix-logprob" </td><td> <pre> Compute the log-prob of a particular (e.g.) pdf sequence, derived from a pdf alignment,
given log-probs in a matrix.  The log-probs are computed over the whole data and printed
as a logging message. Optionally also write out the original matrix.  This is for use in
neural net discriminative training (for computing objective functions).

Usage: matrix-logprob [options] <matrix-rspecifier> <pdf-ali-rspecifier> [<matrix-wspecifier1>] </pre> </td> </tr>
<tr> <td> \ref bin/matrix-sum.cc "matrix-sum" </td><td> <pre> Add matrices (supports various forms)

Type one usage:
 matrix-sum [options] <matrix-in-rspecifier1> [<matrix-in-rspecifier2> <matrix-in-rspecifier3> ...] <matrix-out-wspecifier>
  e.g.: matrix-sum ark:1.weights ark:2.weights ark:combine.weights
  This usage supports the --scale1 and --scale2 options to scale the
  first two input tables.
Type two usage (sums a single table input to produce a single output):
 matrix-sum [options] <matrix-in-rspecifier> <matrix-out-wxfilename>
 e.g.: matrix-sum --binary=false mats.ark sum.mat
Type three usage (sums single-file inputs to produce a single output):
 matrix-sum [options] <matrix-in-rxfilename1> <matrix-in-rxfilename2> ... <matrix-out-wxfilename>
 e.g.: matrix-sum --binary=false 1.mat 2.mat 3.mat sum.mat
See also: matrix-sum-rows </pre> </td> </tr>
<tr> <td> \ref bin/latgen-tracking-mapped.cc "latgen-tracking-mapped" </td><td> <pre> Generate lattices using likelihood matrices, using arc lattices from forward path.
Usage: latgen-tracking-mapped [options] model-in (fst-in|fsts-rspecifier) loglike-rspecifier arcs-rspecifier lattice-wspecifier [ words-wspecifier [alignments-wspecifier] ] </pre> </td> </tr>
<tr> <td> \ref bin/build-pfile-from-ali.cc "build-pfile-from-ali" </td><td> <pre> Build pfiles for neural network training from alignment.
Usage:  build-pfile-from-ali [options] <model> <alignments-rspecifier> <feature-rspecifier> 
<pfile-wspecifier>
e.g.: 
 build-pfile-from-ali 1.mdl ark:1.ali features 
 "|pfile_create -i - -o pfile.1 -f 143 -l 1"  </pre> </td> </tr>
<tr> <td> \ref bin/get-post-on-ali.cc "get-post-on-ali" </td><td> <pre> Given input posteriors, e.g. derived from lattice-to-post, and an alignment
typically derived from the best path of a lattice, outputs the probability in
the posterior of the corresponding index in the alignment, or zero if it was
not there.  These are output as a vector of weights, one per utterance.
While, by default, lattice-to-post (as a source of posteriors) and sources of
alignments such as lattice-best-path will output transition-ids as the index,
it will generally make sense to either convert these to pdf-ids using
post-to-pdf-post and ali-to-pdf respectively, or to phones using post-to-phone-post
and (ali-to-phones --per-frame=true).  Since this program only sees the integer
indexes, it does not care what they represent-- but of course they should match
(e.g. don't input posteriors with transition-ids and alignments with pdf-ids).
See http://kaldi.sourceforge.net/hmm.html#transition_model_identifiers for an
explanation of these types of indexes.

See also: weight-post, post-to-weights, reverse-weights

Usage:  get-post-on-ali [options] <posteriors-rspecifier> <ali-rspecifier> <weights-wspecifier>
e.g.: get-post-on-ali ark:post.ark ark,s,cs:ali.ark ark:weights.ark </pre> </td> </tr>
<tr> <td> \ref bin/tree-info.cc "tree-info" </td><td> <pre> Print information about decision tree (mainly the number of pdfs), to stdout
Usage:  tree-info <tree-in> </pre> </td> </tr>
<tr> <td> \ref bin/am-info.cc "am-info" </td><td> <pre> Write to standard output various properties of a model, of any type
(reads only the transition model)
Usage:  am-info [options] <model-in>
e.g.:
 am-info 1.mdl </pre> </td> </tr>
<tr> <td> \ref bin/acc-fullctx-stats.cc "acc-fullctx-stats" </td><td> <pre> Accumulate statistics for phonetic-context tree building.
Uses alignments with fully-expanded model names. Intended for parametric synthesis.

Usage:  acc-fullctx-stats [options] central-position features-rspecifier alignments-rspecifier tree-accs-out </pre> </td> </tr>
<tr> <td> \ref bin/fullctx-to-pdf.cc "fullctx-to-pdf" </td><td> <pre> Converts frame-level full-context alignments to pdf-ids.
Usage:  fullctx-to-pdf  [options] <tree> <alignments-rspecifier> <pdfs-wspecifier>
e.g.: 
 fullctx-to-pdf tree ark:1.ali ark:1.pdf.ali </pre> </td> </tr>
<tr> <td> \ref bin/convert-fullctx-ali.cc "convert-fullctx-ali" </td><td> <pre> Convert alignments from one decision-tree/model to another
Usage:  convert-fullctx-ali  [options] old-model new-model new-tree tid-ali-rspecifier fullctx-ali-respecifier new-tid-ali-wspecifier
e.g.: 
 convert-fullctx-ali old.mdl new.mdl new.tree ark:old.ali ark:full.ali ark:new.ali </pre> </td> </tr>
<tr> <td> \ref bin/make-fullctx-ali.cc "make-fullctx-ali" </td><td> <pre> Merges output from context extraction with a standard alignment
Usage: make-fullctx-ali model old-alignments-rspecifier 
 full-contexts-rspecifier fullcontext-alignments-wspecifier
e.g.: 
 make-fullctx-ali model.mdl ark:old.ali ark,t:cex.ark ark:new.ali </pre> </td> </tr>
<tr> <td> \ref bin/compile-ctx-questions.cc "compile-ctx-questions" </td><td> <pre> Compile keyed questions only from the text format into the Kaldi
Questions class.questions are of the format F ? N N ... where f is a field index and
N are possible integer values
Usage:  compile-questions [options] <keyed-questions-text-file>
<questions-out>
e.g.: 
 compile-ctx-questions questions.txt questions.qst </pre> </td> </tr>
<tr> <td> \ref bin/vector-sum.cc "vector-sum" </td><td> <pre> Add vectors (e.g. weights, transition-accs; speaker vectors)
If you need to scale the inputs, use vector-scale on the inputs

Type one usage:
 vector-sum [options] <vector-in-rspecifier1> [<vector-in-rspecifier2> <vector-in-rspecifier3> ...] <vector-out-wspecifier>
  e.g.: vector-sum ark:1.weights ark:2.weights ark:combine.weights
Type two usage (sums a single table input to produce a single output):
 vector-sum [options] <vector-in-rspecifier> <vector-out-wxfilename>
 e.g.: vector-sum --binary=false vecs.ark sum.vec
Type three usage (sums single-file inputs to produce a single output):
 vector-sum [options] <vector-in-rxfilename1> <vector-in-rxfilename2> ... <vector-out-wxfilename>
 e.g.: vector-sum --binary=false 1.vec 2.vec 3.vec sum.vec
See also: copy-vector, dot-weights </pre> </td> </tr>
<tr> <td> \ref bin/matrix-sum-rows.cc "matrix-sum-rows" </td><td> <pre> Sum the rows of an input table of matrices and output the corresponding
table of vectors

Usage: matrix-sum-rows [options] <matrix-rspecifier> <vector-wspecifier>
e.g.: matrix-sum-rows ark:- ark:- | vector-sum ark:- sum.vec
See also: matrix-sum, vector-sum </pre> </td> </tr>
<tr> <td> \ref bin/est-pca.cc "est-pca" </td><td> <pre> Estimate PCA transform; dimension reduction is optional (if not specified
we don't reduce the dimension; if you specify --normalize-variance=true,
we normalize the (centered) covariance of the features, and if you specify
--normalize-mean=true the mean is also normalized.  So a variety of transform
types are supported.  Because this type of transform does not need too much
data to estimate robustly, we don't support separate accumulator files;
this program reads in the features directly.  For large datasets you may
want to subset the features (see example below)
By default the program reads in matrices (e.g. features), but with
--read-vectors=true, can read in vectors (e.g. iVectors).

Usage:  est-pca [options] (<feature-rspecifier>|<vector-rspecifier>) <pca-matrix-out>
e.g.:
utils/shuffle_list.pl data/train/feats.scp | head -n 5000 | sort | \\
  est-pca --dim=50 scp:- some/dir/0.mat </pre> </td> </tr>
<tr> <td> \ref bin/sum-lda-accs.cc "sum-lda-accs" </td><td> <pre> Sum stats obtained with acc-lda.
Usage: sum-lda-accs [options] <stats-out> <stats-in1> <stats-in2> ... </pre> </td> </tr>
<tr> <td> \ref bin/sum-mllt-accs.cc "sum-mllt-accs" </td><td> <pre> Sum stats obtained with gmm-acc-mllt.
Usage: sum-mllt-accs [options] <stats-out> <stats-in1> <stats-in2> ... </pre> </td> </tr>
<tr> <td> \ref bin/transform-vec.cc "transform-vec" </td><td> <pre> This program applies a linear or affine transform to individual vectors, e.g.
iVectors.  It is transform-feats, except it works on vectors rather than matrices,
and expects a single transform matrix rather than possibly a table of matrices

Usage: transform-vec [options] <transform-rxfilename> <feats-rspecifier> <feats-wspecifier>
See also: transform-feats, est-pca </pre> </td> </tr>
<tr> <td> \ref bin/align-text.cc "align-text" </td><td> <pre> Computes alignment between two sentences with the same key in the
two given input text-rspecifiers. The current implementation uses
Levenshtein distance as the distance metric.

The input text file looks like follows:
  key1 a b c
  key2 d e

The output alignment file looks like follows:
  key1 a a ; b <eps> ; c c 
  key2 d f ; e e 
where the aligned pairs are separated by ";"

Usage: align-text [options] <text1-rspecifier> <text2-rspecifier> \\
                              <alignment-wspecifier>
 e.g.: align-text ark:text1.txt ark:text2.txt ark,t:alignment.txt </pre> </td> </tr>
<tr> <td> \ref featbin/compute-mfcc-feats.cc "compute-mfcc-feats" </td><td> <pre> Create MFCC feature files.
Usage:  compute-mfcc-feats [options...] <wav-rspecifier> <feats-wspecifier> </pre> </td> </tr>
<tr> <td> \ref featbin/compute-plp-feats.cc "compute-plp-feats" </td><td> <pre> Create PLP feature files.
Usage:  compute-plp-feats [options...] <wav-rspecifier> <feats-wspecifier> </pre> </td> </tr>
<tr> <td> \ref featbin/compute-fbank-feats.cc "compute-fbank-feats" </td><td> <pre> Create Mel-filter bank (FBANK) feature files.
Usage:  compute-fbank-feats [options...] <wav-rspecifier> <feats-wspecifier> </pre> </td> </tr>
<tr> <td> \ref featbin/compute-cmvn-stats.cc "compute-cmvn-stats" </td><td> <pre> Compute cepstral mean and variance normalization statistics
If wspecifier provided: per-utterance by default, or per-speaker if
spk2utt option provided; if wxfilename: global
Usage: compute-cmvn-stats  [options] <feats-rspecifier> (<stats-wspecifier>|<stats-wxfilename>)
e.g.: compute-cmvn-stats --spk2utt=ark:data/train/spk2utt scp:data/train/feats.scp ark,scp:/foo/bar/cmvn.ark,data/train/cmvn.scp
See also: apply-cmvn, modify-cmvn-stats </pre> </td> </tr>
<tr> <td> \ref featbin/add-deltas.cc "add-deltas" </td><td> <pre> Add deltas (typically to raw mfcc or plp features
Usage: add-deltas [options] in-rspecifier out-wspecifier </pre> </td> </tr>
<tr> <td> \ref featbin/remove-mean.cc "remove-mean" </td><td> <pre> Remove mean from each feature file
 [ for per-speaker normalization, use add-cmvn-stats and apply-cmvn ]
Usage: remove-mean [options] in-rspecifier out-wspecifier </pre> </td> </tr>
<tr> <td> \ref featbin/apply-cmvn.cc "apply-cmvn" </td><td> <pre> Apply cepstral mean and (optionally) variance normalization
Per-utterance by default, or per-speaker if utt2spk option provided
Usage: apply-cmvn [options] (<cmvn-stats-rspecifier>|<cmvn-stats-rxfilename>) <feats-rspecifier> <feats-wspecifier>
e.g.: apply-cmvn --utt2spk=ark:data/train/utt2spk scp:data/train/cmvn.scp scp:data/train/feats.scp ark:-
See also: modify-cmvn-stats, matrix-sum, compute-cmvn-stats </pre> </td> </tr>
<tr> <td> \ref featbin/transform-feats.cc "transform-feats" </td><td> <pre> Apply transform (e.g. LDA; HLDA; fMLLR/CMLLR; MLLT/STC)
Linear transform if transform-num-cols == feature-dim, affine if
transform-num-cols == feature-dim+1 (->append 1.0 to features)
Per-utterance by default, or per-speaker if utt2spk option provided
Global if transform-rxfilename provided.
Usage: transform-feats [options] (<transform-rspecifier>|<transform-rxfilename>) <feats-rspecifier> <feats-wspecifier>
See also: transform-vec, copy-feats, compose-transforms </pre> </td> </tr>
<tr> <td> \ref featbin/copy-feats.cc "copy-feats" </td><td> <pre> Copy features [and possibly change format]
Usage: copy-feats [options] <feature-rspecifier> <feature-wspecifier>
or:   copy-feats [options] <feats-rxfilename> <feats-wxfilename>
e.g.: copy-feats ark:- ark,scp:foo.ark,foo.scp
 or: copy-feats ark:foo.ark ark,t:txt.ark
See also: copy-matrix, copy-feats-to-htk, copy-feats-to-sphinx, select-feats,
extract-rows, subset-feats, subsample-feats, splice-feats, append-feats </pre> </td> </tr>
<tr> <td> \ref featbin/compose-transforms.cc "compose-transforms" </td><td> <pre> Compose (affine or linear) feature transforms
Usage: compose-transforms [options] (<transform-A-rspecifier>|<transform-A-rxfilename>) (<transform-B-rspecifier>|<transform-B-rxfilename>) (<transform-out-wspecifier>|<transform-out-wxfilename>)
 Note: it does matrix multiplication (A B) so B is the transform that gets applied
  to the features first.  If b-is-affine = true, then assume last column of b corresponds to offset
 e.g.: compose-transforms 1.mat 2.mat 3.mat
   compose-transforms 1.mat ark:2.trans ark:3.trans
   compose-transforms ark:1.trans ark:2.trans ark:3.trans
 See also: transform-feats, transform-vec, extend-transform-dim, est-lda, est-pca </pre> </td> </tr>
<tr> <td> \ref featbin/splice-feats.cc "splice-feats" </td><td> <pre> Splice features with left and right context (e.g. prior to LDA)
Usage: splice-feats [options] in-rspecifier out-wspecifier </pre> </td> </tr>
<tr> <td> \ref featbin/extract-segments.cc "extract-segments" </td><td> <pre> Extract segments from a large audio file in WAV format.
Usage:  extract-segments [options] <wav-rspecifier> <segments-file> <wav-wspecifier>
e.g. extract-segments scp:wav.scp segments ark:- | <some-other-program>
 segments-file format: each line is either
<segment-id> <recording-id> <start-time> <end-time>
e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5
or (less frequently, and not supported in scripts):
<segment-id> <wav-file-name> <start-time> <end-time> <channel>
where <channel> will normally be 0 (left) or 1 (right)
e.g. call-861225-A-0050-0065 call-861225 5.0 6.5 1
And <end-time> of -1 means the segment runs till the end of the WAV file
See also: extract-rows, which does the same thing but to feature files,
 wav-copy, wav-to-duration </pre> </td> </tr>
<tr> <td> \ref featbin/subset-feats.cc "subset-feats" </td><td> <pre> Copy a subset of features (by default, the first n feature files)
Usually used where only a small amount of data is needed
Note: if you want a specific subset, it's usually best to
filter the original .scp file with utils/filter_scp.pl
(possibly with the --exclude option).  The --include and --exclude
options of this program are intended for specialized uses.
The --include and --exclude options are mutually exclusive, 
and both cause the --n option to be ignored.
Usage: subset-feats [options] <in-rspecifier> <out-wspecifier>
e.g.: subset-feats --n=10 ark:- ark:-
or:  subset-feats --include=include_uttlist ark:- ark:-
or:  subset-feats --exclude=exclude_uttlist ark:- ark:-
See also extract-rows, select-feats, subsample-feats </pre> </td> </tr>
<tr> <td> \ref featbin/feat-to-len.cc "feat-to-len" </td><td> <pre> Reads an archive of features and writes a corresponding archive
that maps utterance-id to utterance length in frames, or (with
one argument) print to stdout the total number of frames in the
input archive.
Usage: feat-to-len [options] <in-rspecifier> [<out-wspecifier>]
e.g.: feat-to-len scp:feats.scp ark,t:feats.lengths
or: feat-to-len scp:feats.scp </pre> </td> </tr>
<tr> <td> \ref featbin/feat-to-dim.cc "feat-to-dim" </td><td> <pre> Reads an archive of features.  If second argument is wxfilename, writes
the feature dimension of the first feature file; if second argument is
wspecifier, writes an archive of the feature dimension, indexed by utterance
id.
Usage: feat-to-dim [options] <feat-rspecifier> (<dim-wspecifier>|<dim-wxfilename>)
e.g.: feat-to-dim scp:feats.scp - </pre> </td> </tr>
<tr> <td> \ref featbin/fmpe-apply-transform.cc "fmpe-apply-transform" </td><td> <pre> Apply fMPE transform to features
Usage:  fmpe-apply-transform [options...] <fmpe-object> <feat-rspecifier> <gselect-rspecifier> <feat-wspecifier> </pre> </td> </tr>
<tr> <td> \ref featbin/fmpe-acc-stats.cc "fmpe-acc-stats" </td><td> <pre> Compute statistics for fMPE training
Usage:  fmpe-acc-stats [options...] <fmpe-object> <feat-rspecifier> <feat-diff-rspecifier> <gselect-rspecifier> <stats-out>
Note: gmm-fmpe-acc-stats avoids computing the features an extra time </pre> </td> </tr>
<tr> <td> \ref featbin/fmpe-init.cc "fmpe-init" </td><td> <pre> Initialize fMPE transform (to zero)
Usage: fmpe-init [options...] <diag-gmm-in> <fmpe-out>
E.g. fmpe-init 1.ubm 1.fmpe </pre> </td> </tr>
<tr> <td> \ref featbin/fmpe-est.cc "fmpe-est" </td><td> <pre> Do one iteration of learning (modified gradient descent)
on fMPE transform
Usage: fmpe-est [options...] <fmpe-in> <stats-in> <fmpe-out>
E.g. fmpe-est 1.fmpe 1.accs 2.fmpe </pre> </td> </tr>
<tr> <td> \ref featbin/fmpe-copy.cc "fmpe-copy" </td><td> <pre> Copy fMPE transform
Usage: fmpe-copy [options...] <fmpe-in> <fmpe-out>
E.g. fmpe-copy --binary=false 1.fmpe text.fmpe </pre> </td> </tr>
<tr> <td> \ref featbin/fmpe-sum-accs.cc "fmpe-sum-accs" </td><td> <pre> Sum fMPE stats
Usage: fmpe-sum-accs [options...] <accs-out> <stats-in1> <stats-in2> ... 
E.g. fmpe-sum-accs 1.accs 1.1.accs 1.2.accs 1.3.accs 1.4.accs </pre> </td> </tr>
<tr> <td> \ref featbin/append-feats.cc "append-feats" </td><td> <pre> Append 2 feature-streams [and possibly change format]
Note, this is deprecated; please use paste-feats
Usage: append-feats [options] <in-rspecifier1> <in-rspecifier2> <out-wspecifier>

e.g.: append-feats --feats-offset-in1 5 --num-feats-in1 5 scp:list1.scp scp:list2.scp ark:- </pre> </td> </tr>
<tr> <td> \ref featbin/extend-transform-dim.cc "extend-transform-dim" </td><td> <pre> Read in transform from dimension d -> d (affine or linear), and output a transform
from dimension e -> e (with e >= d, and e controlled by option --new-dimension).
This new transform will leave the extra dimension unaffected, and transform the old
dimensions in the same way.
Usage: extend-transform-dim [options] (transform-A-rspecifier|transform-A-rxfilename) (transform-out-wspecifier|transform-out-wxfilename)
E.g.: extend-transform-dim --new-dimension=117 in.mat big.mat </pre> </td> </tr>
<tr> <td> \ref featbin/get-full-lda-mat.cc "get-full-lda-mat" </td><td> <pre> This is a special-purpose program to be used in "predictive SGMMs".
It takes in an LDA+MLLT matrix, and the original "full" LDA matrix
as output by the --write-full-matrix option of est-lda; and it writes
out a "full" LDA+MLLT matrix formed by the LDA+MLLT matrix plus the
remaining rows of the "full" LDA matrix; and also writes out its inverse
Usage: get-full-lda-mat [options] <lda-mllt-rxfilename> <full-lda-rxfilename> <full-lda-mllt-wxfilename> [<inv-full-lda-mllt-wxfilename>]
E.g.: get-full-lda-mat final.mat full.mat full_lda_mllt.mat full_lda_mllt_inv.mat </pre> </td> </tr>
<tr> <td> \ref featbin/compute-spectrogram-feats.cc "compute-spectrogram-feats" </td><td> <pre> Create spectrogram feature files.
Usage:  compute-spectrogram-feats [options...] <wav-rspecifier> <feats-wspecifier> </pre> </td> </tr>
<tr> <td> \ref featbin/extract-feature-segments.cc "extract-feature-segments" </td><td> <pre> Create feature files by segmenting input files.
Usage:  extract-feature-segments [options...] <feats-rspecifier> <segments-file> <feats-wspecifier>
 (segments-file has lines like: output-utterance-id input-utterance-or-spk-id 1.10 2.36) </pre> </td> </tr>
<tr> <td> \ref featbin/reverse-feats.cc "reverse-feats" </td><td> <pre> Reverse features in time (for backwards decoding)
Usage: reverse-feats [options] in-rspecifier out-wspecifier </pre> </td> </tr>
<tr> <td> \ref featbin/paste-feats.cc "paste-feats" </td><td> <pre> Paste feature files (assuming they have the same lengths);  think of the
unix command paste a b.
Usage: paste-feats <in-rspecifier1> <in-rspecifier2> [<in-rspecifier3> ...] <out-wspecifier>
 or: paste-feats <in-rxfilename1> <in-rxfilename2> [<in-rxfilename3> ...] <out-wxfilename>
 e.g. paste-feats ark:feats1.ark "ark:select-feats 0-3 ark:feats2.ark ark:- |" ark:feats-out.ark
  or: paste-feats foo.mat bar.mat baz.mat
See also: copy-feats, copy-matrix, append-vector-to-feats, concat-feats </pre> </td> </tr>
<tr> <td> \ref featbin/select-feats.cc "select-feats" </td><td> <pre> Select certain dimensions of the feature file;  think of it as the unix
command cut -f ...
Usage: select-feats <selection> <in-rspecifier> <out-wspecifier>
  e.g. select-feats 0,24-22,3-12 scp:feats.scp ark,scp:feat-red.ark,feat-red.scp
See also copy-feats, extract-rows, subset-feats, subsample-feats </pre> </td> </tr>
<tr> <td> \ref featbin/subsample-feats.cc "subsample-feats" </td><td> <pre> Sub-samples features by taking every n'th frame.With negative values of n, will repeat each frame n times
(e.g. --n=-2 will repeat each frame twice)

Usage: subsample-feats [options] <in-rspecifier> <out-wspecifier>
  e.g. subsample-feats --n=2 ark:- ark:- </pre> </td> </tr>
<tr> <td> \ref featbin/process-pitch-feats.cc "process-pitch-feats" </td><td> <pre> This is a rather special-purpose program which processes 2-dimensional
features consisting of (prob-of-voicing, pitch) into something suitable
to put into a speech recognizer.  First use interpolate-feats
Usage:  process-pitch-feats [options...] <feats-rspecifier> <feats-wspecifier> </pre> </td> </tr>
<tr> <td> \ref featbin/interpolate-pitch.cc "interpolate-pitch" </td><td> <pre> This is a rather special-purpose program which processes 2-dimensional
features consisting of (prob-of-voicing, pitch).  By default we do model-based
pitch smoothing and interpolation (see code), or if --linear-interpolation=true,
just linear interpolation across gaps where pitch == 0 (not predicted).
Usage:  interpolate-pitch [options...] <feats-rspecifier> <feats-wspecifier> </pre> </td> </tr>
<tr> <td> \ref featbin/copy-feats-to-htk.cc "copy-feats-to-htk" </td><td> <pre> Save features as HTK files:
Each utterance will be stored as a unique HTK file in a specified directory.
The HTK filename will correspond to the utterance-id (key) in the input table, with the specified extension.
Usage: copy-feats-to-htk [options] in-rspecifier
Example: copy-feats-to-htk --output-dir=/tmp/HTK-features --output-ext=fea  scp:feats.scp </pre> </td> </tr>
<tr> <td> \ref featbin/extract-rows.cc "extract-rows" </td><td> <pre> Extract certain row ranges of matrices.  This is most useful to extract segments
from feature files, for example to modify segmentations or to extract features
corresponding to certain alignments.  The program expects a segments file in the
form of
  segment-name utterance-id start end
where the segment-name is chosen by the user and utterance-id indexes the input matrices.
By default, 'start' and 'end' are row numbers (zero-based), but if you specify the --frame-shift
option (e.g. --frame-shift=0.01), then they represent a time in seconds, which are converted
to integers by dividing by frame-shift.

Usage: extract-rows [options] <segments-file> <features-rspecifier> <features-wspecifier>
  e.g. extract-rows --frame-shift=0.01 segments ark:feats-in.ark ark:feats-out.ark
See also: select-feats, subset-feats, subsample-feats </pre> </td> </tr>
<tr> <td> \ref featbin/apply-cmvn-sliding.cc "apply-cmvn-sliding" </td><td> <pre> Apply sliding-window cepstral mean (and optionally variance)
normalization per utterance.  If center == true, window is centered
on frame being normalized; otherwise it precedes it in time.
Useful for speaker-id; see also apply-cmvn-online

Usage: apply-cmvn-sliding [options] <feats-rspecifier> <feats-wspecifier> </pre> </td> </tr>
<tr> <td> \ref featbin/compute-cmvn-stats-two-channel.cc "compute-cmvn-stats-two-channel" </td><td> <pre> Compute cepstral mean and variance normalization statistics
Specialized for two-sided telephone data where we only accumulate
the louder of the two channels at each frame (and add it to that
side's stats).  Reads a 'reco2file_and_channel' file, normally like
sw02001-A sw02001 A
sw02001-B sw02001 B
sw02005-A sw02005 A
sw02005-B sw02005 B
interpreted as <utterance-id> <call-id> <side> and for each <call-id>
that has two sides, does the 'only-the-louder' computation, else doesn
per-utterance stats in the normal way.
Note: loudness is judged by the first feature component, either energy or c0;
only applicable to MFCCs or PLPs (this code could be modified to handle filterbanks).

Usage: compute-cmvn-stats-two-channel  [options] <reco2file-and-channel> <feats-rspecifier> <stats-wspecifier>
e.g.: compute-cmvn-stats-two-channel data/train_unseg/reco2file_and_channel scp:data/train_unseg/feats.scp ark,t:- </pre> </td> </tr>
<tr> <td> \ref featbin/wav-data.cc "wav-data" </td><td> <pre> Strip the headers from a RIFF wav file
Usage:  wav-data wavfile </pre> </td> </tr>
<tr> <td> \ref featbin/wav-info.cc "wav-info" </td><td> <pre> Get information on a RIFF wav file
Usage:  wav-info wavfile </pre> </td> </tr>
<tr> <td> \ref featbin/copy-feats-to-sphinx.cc "copy-feats-to-sphinx" </td><td> <pre> Save features as Sphinx files:
Each utterance will be stored as a unique Sphinx file in a specified directory.
The Sphinx filename will correspond to the utterance-id (key) in the input table, with the specified extension.
Usage: copy-feats-to-sphinx [options] in-rspecifier
Example: copy-feats-to-sphinx --output-dir=/tmp/sphinx-features --output-ext=fea  scp:feats.scp </pre> </td> </tr>
<tr> <td> \ref featbin/compute-kaldi-pitch-feats.cc "compute-kaldi-pitch-feats" </td><td> <pre> Apply Kaldi pitch extractor, starting from wav input.  Output is 2-dimensional
features consisting of (NCCF, pitch in Hz), where NCCF is between -1 and 1, and
higher for voiced frames.  You will typically pipe this into
process-kaldi-pitch-feats.
Usage: compute-kaldi-pitch-feats [options...] <wav-rspecifier> <feats-wspecifier>
e.g.
compute-kaldi-pitch-feats --sample-frequency=8000 scp:wav.scp ark:- 

See also: process-kaldi-pitch-feats, compute-and-process-kaldi-pitch-feats </pre> </td> </tr>
<tr> <td> \ref featbin/process-kaldi-pitch-feats.cc "process-kaldi-pitch-feats" </td><td> <pre> Post-process Kaldi pitch features, consisting of pitch and NCCF, into
features suitable for input to ASR system.  Default setup produces
3-dimensional features consisting of (pov-feature, pitch-feature,
delta-pitch-feature), where pov-feature is warped NCCF, pitch-feature
is log-pitch with POV-weighted mean subtraction over 1.5 second window,
and delta-pitch-feature is delta feature computed on raw log pitch.
In general, you can select from four features: (pov-feature, 
pitch-feature, delta-pitch-feature, raw-log-pitch), produced in that 
order, by setting the boolean options (--add-pov-feature, 
--add-normalized-log-pitch, --add-delta-pitch and --add-raw-log-pitch)

Usage: process-kaldi-pitch-feats [options...] <feat-rspecifier> <feats-wspecifier>

e.g.: compute-kaldi-pitch-feats [args] ark:- | process-kaldi-pitch-feats ark:- ark:feats.ark

See also: compute-kaldi-pitch-feats, compute-and-process-kaldi-pitch-feats </pre> </td> </tr>
<tr> <td> \ref featbin/compare-feats.cc "compare-feats" </td><td> <pre> Computes relative difference between two sets of features
per dimension and an average difference
Can be used to figure out how different two sets of features are.
Inputs must have same dimension.  Prints to stdout a similarity
metric vector that is 1.0 per dimension if the features identical,
and <1.0 otherwise, and an average overall similarity value.

Usage: compare-feats [options] <in-rspecifier1> <in-rspecifier2>
e.g.: compare-feats ark:1.ark ark:2.ark </pre> </td> </tr>
<tr> <td> \ref featbin/wav-to-duration.cc "wav-to-duration" </td><td> <pre> Read wav files and output an archive consisting of a single float:
the duration of each one in seconds.
Usage:  wav-to-duration [options...] <wav-rspecifier> <duration-wspecifier>
E.g.: wav-to-duration scp:wav.scp ark,t:-
See also: wav-copy extract-segments feat-to-len </pre> </td> </tr>
<tr> <td> \ref featbin/add-deltas-sdc.cc "add-deltas-sdc" </td><td> <pre> Add shifted delta cepstra (typically to raw mfcc or plp features
Usage: add-deltas-sdc [options] in-rspecifier out-wspecifier </pre> </td> </tr>
<tr> <td> \ref featbin/compute-and-process-kaldi-pitch-feats.cc "compute-and-process-kaldi-pitch-feats" </td><td> <pre> Apply Kaldi pitch extractor and pitch post-processor, starting from wav input.
Equivalent to compute-kaldi-pitch-feats | process-kaldi-pitch-feats, except
that it is able to simulate online pitch extraction; see options like
--frames-per-chunk, --simulate-first-pass-online, --recompute-frame.

Usage: compute-and-process-kaldi-pitch-feats [options...] <wav-rspecifier> <feats-wspecifier>
e.g.
compute-and-process-kaldi-pitch-feats --simulate-first-pass-online=true \\
  --frames-per-chunk=10 --sample-frequency=8000 scp:wav.scp ark:- 
See also: compute-kaldi-pitch-feats, process-kaldi-pitch-feats </pre> </td> </tr>
<tr> <td> \ref featbin/modify-cmvn-stats.cc "modify-cmvn-stats" </td><td> <pre> Copy cepstral mean/variance stats so that some dimensions have 'fake' stats
that will skip normalization
Usage: modify-cmvn-stats [options] [<fake-dims>] <in-rspecifier> <out-wspecifier>
e.g.: modify-cmvn-stats 13:14:15 ark:- ark:-
or: modify-cmvn-stats --convert-to-mean-and-var=true ark:- ark:-
See also: compute-cmvn-stats </pre> </td> </tr>
<tr> <td> \ref featbin/wav-copy.cc "wav-copy" </td><td> <pre> Copy archives of wave files

Usage:  wav-copy [options...] <wav-rspecifier> <wav-rspecifier>
e.g. wav-copy scp:wav.scp ark:-
See also: wav-to-duration extract-segments </pre> </td> </tr>
<tr> <td> \ref featbin/append-vector-to-feats.cc "append-vector-to-feats" </td><td> <pre> Append a vector to each row of input feature files

Usage: append-vector-to-feats <in-rspecifier1> <in-rspecifier2> <out-wspecifier>
 or: append-feats <in-rxfilename1> <in-rxfilename2> <out-wxfilename> </pre> </td> </tr>
<tr> <td> \ref featbin/detect-sinusoids.cc "detect-sinusoids" </td><td> <pre> Detect sinusoids (one or two at a time) in waveform input and output
frame-by-frame information on their frequencies and energies.  Useful
as part of DTMF and dialtone detection.  Output is an archive of
matrices; for each file, there is a row per frame, containing
<signal-energy-per-sample> <frequency1> <energy1> <frequency2> <energy2>
where the frequencies and energies may be zero if no sufficiently
dominant sinusoid(s) was/were detected.  If two frequencies were
detected, frequency1 < frequency2.  See options for more detail on
configuration options.

Usage: detect-sinusoids [options] <wav-rspecifier> <matrix-wspecifier>
e.g.: detect-sinusoids scp:wav.scp ark,t:sinusoids.ark </pre> </td> </tr>
<tr> <td> \ref featbin/compute-aperiodic-feats.cc "compute-aperiodic-feats" </td><td> <pre> Apply Aperiodic Energy extractor, starting from wav input.  Output is 2-dimensional
features consisting of (NCCF, pitch in Hz), where NCCF is between -1 and 1, and
higher for voiced frames.  You will typically pipe this into
process-kaldi-pitch-feats.
Usage: compute-aperiodic-feats [options...] <wav-rspecifier> <f0-rspecifier> <feats-wspecifier>
e.g.
compute-aperiodic-feats scp:wav.scp scp:f0.scp ark:- 

See also: process-kaldi-pitch-feats, compute-and-process-kaldi-pitch-feats </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-acc-stats.cc "fgmm-global-acc-stats" </td><td> <pre> Accumulate stats for training a full-covariance GMM.
Usage:  fgmm-global-acc-stats [options] <model-in> <feature-rspecifier> <stats-out>
e.g.: fgmm-global-acc-stats 1.mdl scp:train.scp 1.acc </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-sum-accs.cc "fgmm-global-sum-accs" </td><td> <pre> Sum multiple accumulated stats files for full-covariance GMM training.
Usage: fgmm-global-sum-accs [options] stats-out stats-in1 stats-in2 ... </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-est.cc "fgmm-global-est" </td><td> <pre> Estimate a full-covariance GMM from the accumulated stats.
Usage:  fgmm-global-est [options] <model-in> <stats-in> <model-out> </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-merge.cc "fgmm-global-merge" </td><td> <pre> Combine a number of GMMs into a larger GMM, with #Gauss = 
  sum(individual #Gauss)).  Output full GMM, and a text file with
  sizes of each individual GMM.
Usage: fgmm-global-merge [options] fgmm-out sizes-file-out fgmm-in1 fgmm-in2 ... </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-to-gmm.cc "fgmm-global-to-gmm" </td><td> <pre> Convert single full-covariance GMM to single diagonal-covariance GMM.
Usage: fgmm-global-to-gmm [options] 1.fgmm 1.gmm </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-gselect.cc "fgmm-gselect" </td><td> <pre> Precompute Gaussian indices for pruning
 (e.g. in training UBMs, SGMMs, tied-mixture systems)
 For each frame, gives a list of the n best Gaussian indices,
 sorted from best to worst.
See also: gmm-gselect, copy-gselect, fgmm-gselect-to-post
Usage: 
 fgmm-gselect [options] <model-in> <feature-rspecifier> <gselect-wspecifier>
The --gselect option (which takes an rspecifier) limits selection to a subset
of indices:
e.g.: fgmm-gselect "--gselect=ark:gunzip -c bigger.gselect.gz|" --n=20 1.gmm "ark:feature-command |" "ark,t:|gzip -c >1.gselect.gz" </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-get-frame-likes.cc "fgmm-global-get-frame-likes" </td><td> <pre> Print out per-frame log-likelihoods for each utterance, as an archive
of vectors of floats.  If --average=true, prints out the average per-frame
log-likelihood for each utterance, as a single float.
Usage:  fgmm-global-get-frame-likes [options] <model-in> <feature-rspecifier> <likes-out-wspecifier>
e.g.: fgmm-global-get-frame-likes 1.mdl scp:train.scp ark:1.likes </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-acc-stats-twofeats.cc "fgmm-global-acc-stats-twofeats" </td><td> <pre> Accumulate stats for training a full-covariance GMM, two-feature version
Usage:  fgmm-global-acc-stats-twofeats [options] <model-in> <feature1-rspecifier> <feature2-rspecifier> <stats-out>
e.g.: fgmm-global-acc-stats-twofeats 1.mdl scp:train.scp scp:train2.scp 1.acc </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-copy.cc "fgmm-global-copy" </td><td> <pre> Copy a full-covariance GMM
Usage:  fgmm-global-copy [options] <model-in> <model-out>
e.g.: fgmm-global-copy --binary=false 1.model - | less </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-mixdown.cc "fgmm-global-mixdown" </td><td> <pre> Merge Gaussians in a full-covariance GMM to get a smaller number;
this program supports a --gselect option which is used to select
"good" pairs of Gaussians to consider merging (pairs that most often
co-occur in the gselect information are considered).  If no gselect
info supplied, we consider all pairs (very slow for big models).
Usage:  fgmm-global-mixdown [options] <model-in> <model-out>
e.g.: fgmm-global-mixdown --gselect=gselect.1 --mixdown-target=120 1.ubm 2.ubm
Note: --mixdown-target option is required. </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-gselect-to-post.cc "fgmm-global-gselect-to-post" </td><td> <pre> Given features and Gaussian-selection (gselect) information for
a full-covariance GMM, output per-frame posteriors for the selected
indices.  Also supports pruning the posteriors if they are below
a stated threshold, (and renormalizing the rest to sum to one)
See also: gmm-gselect, fgmm-gselect, gmm-global-get-post,
 gmm-global-gselect-to-post

Usage:  fgmm-global-gselect-to-post [options] <model-in> <feature-rspecifier> <gselect-rspecifier> <post-wspecifier>
e.g.: fgmm-global-gselect-to-post 1.ubm ark:- 'ark:gunzip -c 1.gselect|' ark:- </pre> </td> </tr>
<tr> <td> \ref fgmmbin/fgmm-global-info.cc "fgmm-global-info" </td><td> <pre> Write to standard output various properties of full-covariance GMM model
This is for a single mixture of Gaussians, e.g. as used for a UBM.
Usage:  gmm-info [options] <gmm>
e.g.:
 fgmm-info 1.ubm </pre> </td> </tr>
<tr> <td> \ref fstbin/fstdeterminizestar.cc "fstdeterminizestar" </td><td> <pre> Removes epsilons and determinizes in one step

Usage:  fstdeterminizestar [in.fst [out.fst] ]

See also: fstdeterminizelog, lattice-determinize </pre> </td> </tr>
<tr> <td> \ref fstbin/fstrmsymbols.cc "fstrmsymbols" </td><td> <pre> Replaces a subset of symbols with epsilon, wherever they appear on the input side
of an FST (or the output side, with --remove-from-output=true)

Usage:  fstrmsymbols in-disambig-list  [in.fst [out.fst] ]
E.g:  fstrmsymbols in.list  < in.fst > out.fst </pre> </td> </tr>
<tr> <td> \ref fstbin/fstisstochastic.cc "fstisstochastic" </td><td> <pre> Checks whether an FST is stochastic and exits with success if so.
Prints out maximum error (in log units).

Usage:  fstisstochastic [ in.fst ] </pre> </td> </tr>
<tr> <td> \ref fstbin/fstminimizeencoded.cc "fstminimizeencoded" </td><td> <pre> Minimizes FST after encoding [similar to fstminimize, but no weight-pushing]

Usage:  fstminimizeencoded [in.fst [out.fst] ] </pre> </td> </tr>
<tr> <td> \ref fstbin/fstmakecontextfst.cc "fstmakecontextfst" </td><td> <pre> Constructs a context FST with a specified context-width and context-position.
Outputs the context FST, and a file in Kaldi format that describes what the
input labels mean.  Note: this is very inefficient if there are a lot of phones,
better to use fstcomposecontext instead

Usage:  fstmakecontextfst <phones-symbol-table> <subsequential-symbol> <ilabels-output-file> [<out-fst>]
E.g.:   fstmakecontextfst phones.txt 42 ilabels.sym > C.fst </pre> </td> </tr>
<tr> <td> \ref fstbin/fstmakecontextsyms.cc "fstmakecontextsyms" </td><td> <pre> Create input symbols for CLG
Usage: fstmakecontextsyms phones-symtab ilabels_input_file [output-symtab.txt]
E.g.:  fstmakecontextsyms  phones.txt ilabels.sym > context_symbols.txt </pre> </td> </tr>
<tr> <td> \ref fstbin/fstaddsubsequentialloop.cc "fstaddsubsequentialloop" </td><td> <pre> Minimizes FST after encoding [this algorithm applicable to all FSTs in tropical semiring]

Usage:  fstaddsubsequentialloop subseq_sym [in.fst [out.fst] ]
E.g.:   fstaddsubsequentialloop 52 < LG.fst > LG_sub.fst </pre> </td> </tr>
<tr> <td> \ref fstbin/fstaddselfloops.cc "fstaddselfloops" </td><td> <pre> Adds self-loops to states of an FST to propagate disambiguation symbols through it
They are added on each final state and each state with non-epsilon output symbols
on at least one arc out of the state.  Useful in conjunction with predeterminize

Usage:  fstaddselfloops in-disambig-list out-disambig-list  [in.fst [out.fst] ]
E.g:  fstaddselfloops in.list out.list < in.fst > withloops.fst </pre> </td> </tr>
<tr> <td> \ref fstbin/fstrmepslocal.cc "fstrmepslocal" </td><td> <pre> Removes some (but not all) epsilons in an algorithm that will always reduce the number of
arcs+states.  Option to preserves equivalence in tropical or log semiring, and
if in tropical, stochasticit in either log or tropical.

Usage:  fstrmepslocal  [in.fst [out.fst] ] </pre> </td> </tr>
<tr> <td> \ref fstbin/fstcomposecontext.cc "fstcomposecontext" </td><td> <pre> Composes on the left with a dynamically created context FST

Usage:  fstcomposecontext <ilabels-output-file>  [<in.fst> [<out.fst>] ]
E.g:  fstcomposecontext ilabels.sym < LG.fst > CLG.fst </pre> </td> </tr>
<tr> <td> \ref fstbin/fsttablecompose.cc "fsttablecompose" </td><td> <pre> Composition algorithm [between two FSTs of standard type, in tropical
semiring] that is more efficient for certain cases-- in particular,
where one of the FSTs (the left one, if --match-side=left) has large
out-degree

Usage:  fsttablecompose (fst1-rxfilename|fst1-rspecifier) (fst2-rxfilename|fst2-rspecifier) [(out-rxfilename|out-rspecifier)] </pre> </td> </tr>
<tr> <td> \ref fstbin/fstrand.cc "fstrand" </td><td> <pre> Generate random FST

Usage:  fstrand [out.fst] </pre> </td> </tr>
<tr> <td> \ref fstbin/fstfactor.cc "fstfactor" </td><td> <pre> Factor fst into two parts (by removing linear chains)

Usage:  fstfactor in.fst out1.fst out2.fst </pre> </td> </tr>
<tr> <td> \ref fstbin/fstdeterminizelog.cc "fstdeterminizelog" </td><td> <pre> Determinizes in the log semiring

Usage:  fstdeterminizelog [in.fst [out.fst] ]

See also fstdeterminizestar </pre> </td> </tr>
<tr> <td> \ref fstbin/fstphicompose.cc "fstphicompose" </td><td> <pre> Composition, where the right FST has "failure" (phi) transitions
that are only taken where there was no match of a "real" label
You supply the label corresponding to phi.

Usage:  fstphicompose phi-label (fst1-rxfilename|fst1-rspecifier) (fst2-rxfilename|fst2-rspecifier) [(out-rxfilename|out-rspecifier)]
E.g.: fstphicompose 54 a.fst b.fst c.fst
or: fstphicompose 11 ark:a.fsts G.fst ark:b.fsts </pre> </td> </tr>
<tr> <td> \ref fstbin/fstrhocompose.cc "fstrhocompose" </td><td> <pre> Composition, where the right FST has "rest" (rho) transition
that are only taken where there was no match of a "real" label
You supply the label corresponding to rho.

Usage:  fstrhocompose rho-label (fst1-rxfilename|fst1-rspecifier) (fst2-rxfilename|fst2-rspecifier) [(out-rxfilename|out-rspecifier)]
E.g.: fstrhocompose 54 a.fst b.fst c.fst
or: fstrhocompose 11 ark:a.fsts G.fst ark:b.fsts </pre> </td> </tr>
<tr> <td> \ref fstbin/fstpropfinal.cc "fstpropfinal" </td><td> <pre> Propagates final-states through phi transitions

Usage:  fstpropfinal phi-label [in.fst [out.fst] ] </pre> </td> </tr>
<tr> <td> \ref fstbin/fstcopy.cc "fstcopy" </td><td> <pre> Copy tables/archives of FSTs, indexed by a string (e.g. utterance-id)

Usage: fstcopy <fst-rspecifier> <fst-wspecifier> </pre> </td> </tr>
<tr> <td> \ref fstbin/fstpushspecial.cc "fstpushspecial" </td><td> <pre> Pushes weights in an FST such that all the states
in the FST have arcs and final-probs with weights that
sum to the same amount (viewed as being in the log semiring).
Thus, the "extra weight" is distributed throughout the FST.
Tolerance parameter --delta controls how exact this is, and the
speed.

Usage:  fstpushspecial [options] [in.fst [out.fst] ] </pre> </td> </tr>
<tr> <td> \ref fstbin/fsts-to-transcripts.cc "fsts-to-transcripts" </td><td> <pre> Reads a table of FSTs; for each element, finds the best path and prints out the
output-symbol sequence (if --output-side=true), or input-symbol sequenceotherwise.

Usage: fsts-to-transcripts [options] fsts-rspecifier transcriptions-wspecifier
 e.g.: fsts-to-transcripts ark:train.fsts ark,t:train.text </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-init-mono.cc "gmm-init-mono" </td><td> <pre> Initialize monophone GMM.
Usage:  gmm-init-mono <topology-in> <dim> <model-out> <tree-out> 
e.g.: 
 gmm-init-mono topo 39 mono.mdl mono.tree </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est.cc "gmm-est" </td><td> <pre> Do Maximum Likelihood re-estimation of GMM-based acoustic model
Usage:  gmm-est [options] <model-in> <stats-in> <model-out>
e.g.: gmm-est 1.mdl 1.acc 2.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-acc-stats-ali.cc "gmm-acc-stats-ali" </td><td> <pre> Accumulate stats for GMM training.
Usage:  gmm-acc-stats-ali [options] <model-in> <feature-rspecifier> <alignments-rspecifier> <stats-out>
e.g.:
 gmm-acc-stats-ali 1.mdl scp:train.scp ark:1.ali 1.acc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-align.cc "gmm-align" </td><td> <pre> Align features given [GMM-based] models.
Usage:   gmm-align [options] tree-in model-in lexicon-fst-in feature-rspecifier transcriptions-rspecifier alignments-wspecifier
e.g.: 
 gmm-align tree 1.mdl lex.fst scp:train.scp ark:train.tra ark:1.ali </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-decode-faster.cc "gmm-decode-faster" </td><td> <pre> Decode features using GMM-based model.
Usage:  gmm-decode-faster [options] model-in fst-in features-rspecifier words-wspecifier [alignments-wspecifier [lattice-wspecifier]]
Note: lattices, if output, will just be linear sequences; use gmm-latgen-faster
  if you want "real" lattices. </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-decode-simple.cc "gmm-decode-simple" </td><td> <pre> Decode features using GMM-based model.
Viterbi decoding, Only produces linear sequence; any lattice
produced is linear

Usage:   gmm-decode-simple [options] <model-in> <fst-in> <features-rspecifier> <words-wspecifier> [<alignments-wspecifier>] [<lattice-wspecifier>] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-decode-nbest.cc "gmm-decode-nbest" </td><td> <pre> Decode features using GMM-based model, producing N-best lattice output.
Note: this program was mainly intended to validate the lattice generation
algorithm and is not very useful; in general, processing the
lattices into n-best lists will be more efficient.
Usage:
 gmm-decode-nbest [options] <model-in> <fst-in> <features-rspecifier> <nbest-lattice-wspecifier> <words-wspecifier> [<alignments-wspecifier>] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-align-compiled.cc "gmm-align-compiled" </td><td> <pre> Align features given [GMM-based] models.
Usage:   gmm-align-compiled [options] model-in graphs-rspecifier feature-rspecifier alignments-wspecifier [scores-wspecifier]
e.g.: 
 gmm-align-compiled 1.mdl ark:graphs.fsts scp:train.scp ark:1.ali
or:
 compile-train-graphs tree 1.mdl lex.fst ark:train.tra b, ark:- | \\
   gmm-align-compiled 1.mdl ark:- scp:train.scp t, ark:1.ali </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-sum-accs.cc "gmm-sum-accs" </td><td> <pre> Sum multiple accumulated stats files for GMM training.
Usage: gmm-sum-accs [options] <stats-out> <stats-in1> <stats-in2> ...
E.g.: gmm-sum-accs 1.acc 1.1.acc 1.2.acc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-init-trans.cc "gmm-init-trans" </td><td> <pre> Initialize transition model given topo, tree and GMM (used for format conversion from HTK)
Usage:  gmm-init-trans <topology-in> <gmm-in> <tree-in> <model-out> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-regtree-fmllr.cc "gmm-est-regtree-fmllr" </td><td> <pre> Compute FMLLR transforms per-utterance (default) or per-speaker for the supplied set of speakers (spk2utt option).  Note: writes RegtreeFmllrDiagGmm objects
Usage: gmm-est-regtree-fmllr  [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <regression-tree> <transforms-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-acc-stats-twofeats.cc "gmm-acc-stats-twofeats" </td><td> <pre> Accumulate stats for GMM training, computing posteriors with one set of features
but accumulating statistics with another.
First features are used to get posteriors, second to accumulate stats
Usage:  gmm-acc-stats-twofeats [options] <model-in> <feature1-rspecifier> <feature2-rspecifier> <posteriors-rspecifier> <stats-out>
e.g.: 
 gmm-acc-stats-twofeats 1.mdl 1.ali scp:train.scp scp:train_new.scp ark:1.ali 1.acc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-acc-stats.cc "gmm-acc-stats" </td><td> <pre> Accumulate stats for GMM training (reading in posteriors).
Usage:  gmm-acc-stats [options] <model-in> <feature-rspecifier><posteriors-rspecifier> <stats-out>
e.g.: 
 gmm-acc-stats 1.mdl scp:train.scp ark:1.post 1.acc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-init-lvtln.cc "gmm-init-lvtln" </td><td> <pre> Initialize lvtln transforms
Usage:  gmm-init-lvtln [options] <lvtln-out>
e.g.: 
 gmm-init-lvtln --dim=13 --num-classes=21 --default-class=10 1.lvtln </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-lvtln-trans.cc "gmm-est-lvtln-trans" </td><td> <pre> Estimate linear-VTLN transforms, either per utterance or for the supplied set of speakers (spk2utt option).  Reads posteriors. 
Usage: gmm-est-lvtln-trans [options] <model-in> <lvtln-in> <feature-rspecifier> <gpost-rspecifier> <lvtln-trans-wspecifier> [<warp-wspecifier>] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-train-lvtln-special.cc "gmm-train-lvtln-special" </td><td> <pre> Set one of the transforms in lvtln to the minimum-squared-error solution
to mapping feats-untransformed to feats-transformed; posteriors may
optionally be used to downweight/remove silence.
Usage: gmm-train-lvtln-special [options] class-index <lvtln-in> <lvtln-out>  <feats-untransformed-rspecifier> <feats-transformed-rspecifier> [<posteriors-rspecifier>]
e.g.: 
 gmm-train-lvtln-special 5 5.lvtln 6.lvtln scp:train.scp scp:train_warp095.scp ark:nosil.post </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-acc-mllt.cc "gmm-acc-mllt" </td><td> <pre> Accumulate MLLT (global STC) statistics
Usage:  gmm-acc-mllt [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <stats-out>
e.g.: 
 gmm-acc-mllt 1.mdl scp:train.scp ark:1.post 1.macc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-mixup.cc "gmm-mixup" </td><td> <pre> Does GMM mixing up (and Gaussian merging)
Usage:  gmm-mixup [options] <model-in> <state-occs-in> <model-out>
e.g. of mixing up:
 gmm-mixup --mix-up=4000 1.mdl 1.occs 2.mdl
e.g. of merging:
 gmm-mixup --merge=2000 1.mdl 1.occs 2.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-init-model.cc "gmm-init-model" </td><td> <pre> Initialize GMM from decision tree and tree stats
Usage:  gmm-init-model [options] <tree-in> <tree-stats-in> <topo-file> <model-out> [<old-tree> <old-model>]
e.g.: 
  gmm-init-model tree treeacc topo 1.mdl
or (initializing GMMs with old model):
  gmm-init-model tree treeacc topo 1.mdl prev/tree prev/30.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-acc-hlda.cc "gmm-acc-hlda" </td><td> <pre> Accumulate HLDA statistics
Usage:  gmm-acc-hlda [options] <model-in> <orig-transform-in> <orig-feature-rspecifier> <posteriors-rspecifier> <stats-out>
Note: orig-transform-in must be the current truncated HLDA transform (e.g. from LDA).e.g.: 
 gmm-acc-hlda 1.mdl 1.hlda "ark:splice-feats scp:train.scp |" ark:1.post 1.hacc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-hlda.cc "gmm-est-hlda" </td><td> <pre> Do HLDA update
Usage:  gmm-est-hlda [options] <model-in> <full-hlda-mat-in> <model-out> <full-hlda-mat-out> <partial-hlda-mat-out> <stats-in1> <stats-in2> ... 
e.g.: gmm-est-hlda 1.mdl 1.hldafull 2.mdl 2.hldafull 2.hlda 1.0.hacc 1.1.hacc ...  </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-transform-means.cc "gmm-transform-means" </td><td> <pre> Transform GMM means with linear or affine transform
Usage:  gmm-transform-means <transform-matrix> <model-in> <model-out>
e.g.: gmm-transform-means 2.mat 2.mdl 3.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-make-regtree.cc "gmm-make-regtree" </td><td> <pre> Build regression class tree.
Usage: gmm-make-regtree [options] <model-file> <regtree-out>
E.g.: gmm-make-regtree --silphones=1:2:3 --state-occs=1.occs 1.mdl 1.regtree
 [Note: state-occs come from --write-occs option of gmm-est] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-decode-faster-regtree-fmllr.cc "gmm-decode-faster-regtree-fmllr" </td><td> <pre> Decode features using GMM-based model.
Usage: gmm-decode-faster-regtree-fmllr [options] model-in fst-in regtree-in features-rspecifier transforms-rspecifier words-wspecifier [alignments-wspecifier] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-post-to-gpost.cc "gmm-post-to-gpost" </td><td> <pre> Convert state-level posteriors to Gaussian-level posteriors
Usage:  gmm-post-to-gpost [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <gpost-wspecifier>
e.g.: 
 gmm-post-to-gpost 1.mdl scp:train.scp ark:1.post ark:1.gpost </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-fmllr-gpost.cc "gmm-est-fmllr-gpost" </td><td> <pre> Estimate global fMLLR transforms, either per utterance or for the supplied
set of speakers (spk2utt option).  Reads Gaussian-level posteriors.  Writes
to a table of matrices.
Usage: gmm-est-fmllr-gpost [options] <model-in> <feature-rspecifier> <gpost-rspecifier> <transform-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-fmllr.cc "gmm-est-fmllr" </td><td> <pre> Estimate global fMLLR transforms, either per utterance or for the supplied
set of speakers (spk2utt option).  Reads posteriors (on transition-ids).  Writes
to a table of matrices.
Usage: gmm-est-fmllr [options] <model-in> <feature-rspecifier> <post-rspecifier> <transform-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-regtree-fmllr-ali.cc "gmm-est-regtree-fmllr-ali" </td><td> <pre> Compute FMLLR transforms per-utterance (default) or per-speaker for the supplied set of speakers (spk2utt option).  Note: writes RegtreeFmllrDiagGmm objects
Usage: gmm-est-regtree-fmllr-ali  [options] <model-in> <feature-rspecifier> <alignments-rspecifier> <regression-tree> <transforms-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-regtree-mllr.cc "gmm-est-regtree-mllr" </td><td> <pre> Compute MLLR transforms per-utterance (default) or per-speaker for the supplied set of speakers (spk2utt option).  Note: writes RegtreeMllrDiagGmm objects
Usage: gmm-est-regtree-mllr  [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <regression-tree> <transforms-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-compute-likes.cc "gmm-compute-likes" </td><td> <pre> Compute log-likelihoods from GMM-based model
(outputs matrices of log-likelihoods indexed by (frame, pdf)
Usage: gmm-compute-likes [options] model-in features-rspecifier likes-wspecifier </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-decode-faster-regtree-mllr.cc "gmm-decode-faster-regtree-mllr" </td><td> <pre> Decode features using GMM-based model.
Usage: gmm-decode-faster-regtree-mllr [options] model-in fst-in regtree-in features-rspecifier transforms-rspecifier words-wspecifier [alignments-wspecifier] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-latgen-simple.cc "gmm-latgen-simple" </td><td> <pre> Generate lattices using GMM-based model.
Usage: gmm-latgen-simple [options] model-in fst-in features-rspecifier lattice-wspecifier [ words-wspecifier [alignments-wspecifier] ] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-rescore-lattice.cc "gmm-rescore-lattice" </td><td> <pre> Replace the acoustic scores on a lattice using a new model.
Usage: gmm-rescore-lattice [options] <model-in> <lattice-rspecifier> <feature-rspecifier> <lattice-wspecifier>
 e.g.: gmm-rescore-lattice 1.mdl ark:1.lats scp:trn.scp ark:2.lats </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-decode-biglm-faster.cc "gmm-decode-biglm-faster" </td><td> <pre> Decode features using GMM-based model.
User supplies LM used to generate decoding graph, and desired LM;
this decoder applies the difference during decoding
Usage:  gmm-decode-biglm-faster [options] model-in fst-in oldlm-fst-in newlm-fst-in features-rspecifier words-wspecifier [alignments-wspecifier [lattice-wspecifier]] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-gaussians-ebw.cc "gmm-est-gaussians-ebw" </td><td> <pre> Do EBW update for MMI, MPE or MCE discriminative training.
Numerator stats should already be I-smoothed (e.g. use gmm-ismooth-stats)
Usage:  gmm-est-gaussians-ebw [options] <model-in> <stats-num-in> <stats-den-in> <model-out>
e.g.: gmm-est-gaussians-ebw 1.mdl num.acc den.acc 2.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-weights-ebw.cc "gmm-est-weights-ebw" </td><td> <pre> Do EBW update on weights for MMI, MPE or MCE discriminative training.
Numerator stats should not be I-smoothed
Usage:  gmm-est-weights-ebw [options] <model-in> <stats-num-in> <stats-den-in> <model-out>
e.g.: gmm-est-weights-ebw 1.mdl num.acc den.acc 2.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-latgen-faster.cc "gmm-latgen-faster" </td><td> <pre> Generate lattices using GMM-based model.
Usage: gmm-latgen-faster [options] model-in (fst-in|fsts-rspecifier) features-rspecifier lattice-wspecifier [ words-wspecifier [alignments-wspecifier] ] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-copy.cc "gmm-copy" </td><td> <pre> Copy GMM based model (and possibly change binary/text format)
Usage:  gmm-copy [options] <model-in> <model-out>
e.g.:
 gmm-copy --binary=false 1.mdl 1_txt.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-acc-stats.cc "gmm-global-acc-stats" </td><td> <pre> Accumulate stats for training a diagonal-covariance GMM.
Usage:  gmm-global-acc-stats [options] <model-in> <feature-rspecifier> <stats-out>
e.g.: gmm-global-acc-stats 1.mdl scp:train.scp 1.acc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-est.cc "gmm-global-est" </td><td> <pre> Estimate a diagonal-covariance GMM from the accumulated stats.
Usage:  gmm-global-est [options] <model-in> <stats-in> <model-out> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-sum-accs.cc "gmm-global-sum-accs" </td><td> <pre> Sum multiple accumulated stats files for diagonal-covariance GMM training.
Usage: gmm-global-sum-accs [options] stats-out stats-in1 stats-in2 ... </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-gselect.cc "gmm-gselect" </td><td> <pre> Precompute Gaussian indices for pruning
 (e.g. in training UBMs, SGMMs, tied-mixture systems)
 For each frame, gives a list of the n best Gaussian indices,
 sorted from best to worst.
See also: gmm-global-get-post, fgmm-global-gselect-to-post,
copy-gselect, fgmm-gselect
Usage: 
 gmm-gselect [options] <model-in> <feature-rspecifier> <gselect-wspecifier>
The --gselect option (which takes an rspecifier) limits selection to a subset
of indices:
e.g.: gmm-gselect "--gselect=ark:gunzip -c bigger.gselect.gz|" --n=20 1.gmm "ark:feature-command |" "ark,t:|gzip -c >gselect.1.gz" </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-latgen-biglm-faster.cc "gmm-latgen-biglm-faster" </td><td> <pre> Generate lattices using GMM-based model.
User supplies LM used to generate decoding graph, and desired LM;
this decoder applies the difference during decoding
Usage: gmm-latgen-biglm-faster [options] model-in (fst-in|fsts-rspecifier) oldlm-fst-in newlm-fst-in features-rspecifier lattice-wspecifier [ words-wspecifier [alignments-wspecifier] ] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-ismooth-stats.cc "gmm-ismooth-stats" </td><td> <pre> Apply I-smoothing to statistics, e.g. for discriminative training
Usage:  gmm-ismooth-stats [options] [--smooth-from-model] [<src-stats-in>|<src-model-in>] <dst-stats-in> <stats-out>
e.g.: gmm-ismooth-stats --tau=100 ml.acc num.acc smoothed.acc
or: gmm-ismooth-stats --tau=50 --smooth-from-model 1.mdl num.acc smoothed.acc
or: gmm-ismooth-stats --tau=100 num.acc num.acc smoothed.acc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-get-frame-likes.cc "gmm-global-get-frame-likes" </td><td> <pre> Print out per-frame log-likelihoods for each utterance, as an archive
of vectors of floats.  If --average=true, prints out the average per-frame
log-likelihood for each utterance, as a single float.
Usage:  gmm-global-get-frame-likes [options] <model-in> <feature-rspecifier> <likes-out-wspecifier>
e.g.: gmm-global-get-frame-likes 1.mdl scp:train.scp ark:1.likes </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-est-fmllr.cc "gmm-global-est-fmllr" </td><td> <pre> Estimate global fMLLR transforms, either per utterance or for the supplied
set of speakers (spk2utt option).  Reads features, and (with --weights option)
weights for each frame (also see --gselect option)
Usage: gmm-global-est-fmllr [options] <gmm-in> <feature-rspecifier> <transform-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-to-fgmm.cc "gmm-global-to-fgmm" </td><td> <pre> Convert single diagonal-covariance GMM to single full-covariance GMM.
Usage: gmm-global-to-fgmm [options] 1.gmm 1.fgmm </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-acc-stats-twofeats.cc "gmm-global-acc-stats-twofeats" </td><td> <pre> Accumulate stats for training a diagonal-covariance GMM, two-feature version
First features are used to get posteriors, second to accumulate stats
Usage:  gmm-global-acc-stats-twofeats [options] <model-in> <feature1-rspecifier> <feature2-rspecifier> <stats-out>
e.g.: gmm-global-acc-stats-twofeats 1.mdl scp:train.scp scp:train2.scp 1.acc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-copy.cc "gmm-global-copy" </td><td> <pre> Copy a diagonal-covariance GMM
Usage:  gmm-global-copy [options] <model-in> <model-out>
e.g.: gmm-global-copy --binary=false 1.model - | less </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-get-feat-deriv.cc "gmm-get-feat-deriv" </td><td> <pre> From GMM model and posteriors (which don't have to be positive),
output for each utterance a matrix of likelihood derivatives w.r.t.
the features.
E.g. used in feature-space discriminative training.

Usage:  gmm-get-feat-deriv [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <feature-deriv-wspecifier>
e.g.: 
 gmm-get-feat-deriv 1.mdl "$feats" ark:1.post ark:1.deriv </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-fmpe-acc-stats.cc "gmm-fmpe-acc-stats" </td><td> <pre> Accumulate stats for fMPE training, using GMM model.  Note: this could
be done using gmm-get-feat-deriv and fmpe-acc-stats (but you'd be computing
the features twice).  Features input should be pre-fMPE features.

Usage:  gmm-fmpe-acc-stats [options] <model-in> <fmpe-in> <feature-rspecifier> <gselect-rspecifier> <posteriors-rspecifier> <fmpe-stats-out>
e.g.: 
 gmm-fmpe-acc-stats --model-derivative 1.accs 1.mdl 1.fmpe "$feats" ark:1.gselect ark:1.post 1.fmpe_stats </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-acc-stats2.cc "gmm-acc-stats2" </td><td> <pre> Accumulate stats for GMM training (from posteriors)
This version writes two accumulators (e.g. num and den),
and puts the positive accumulators in num, negative in den
Usage:  gmm-acc-stats2 [options] <model> <feature-rspecifier><posteriors-rspecifier> <num-stats-out> <den-stats-out>
e.g.:
gmm-acc-stats 1.mdl "$feats" ark:1.post 1.num_acc 1.den_acc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-init-model-flat.cc "gmm-init-model-flat" </td><td> <pre> Initialize GMM, with Gaussians initialized to mean and variance
of some provided example data (or to 0,1 if not provided: in that
case, provide --dim option)
Usage:  gmm-init-model-flat [options] <tree-in> <topo-file> <model-out> [<features-rspecifier>]
e.g.: 
  gmm-init-model-flat tree topo 1.mdl ark:feats.scp </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-info.cc "gmm-info" </td><td> <pre> Write to standard output various properties of GMM-based model
Usage:  gmm-info [options] <model-in>
e.g.:
 gmm-info 1.mdl
See also: gmm-global-info, am-info </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-scale-accs.cc "gmm-scale-accs" </td><td> <pre> Scale GMM accumulators
Usage: gmm-scale-accs [options] scale stats-in stats-out
e.g.: gmm-scale-accs 0.5 1.stats half.stats </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-get-stats-deriv.cc "gmm-get-stats-deriv" </td><td> <pre> Get statistics derivative for GMM models
(used in fMPE/fMMI feature-space discriminative training)
Usage:  gmm-get-stats-deriv [options] <model-in> <num-stats-in> <den-stats-in> <ml-stats-in> <deriv-out>
e.g. (for fMMI/fBMMI): gmm-get-stats-deriv 1.mdl 1.acc 2.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-rescale.cc "gmm-est-rescale" </td><td> <pre> Do "re-scaling" re-estimation of GMM-based model
 (this update changes the model as features change, but preserves
  the difference between the model and the features, to keep
  the effect of any prior discriminative training).  Used in fMPE.
  Does not update the transitions or weights.
Usage: gmm-est-rescale [options] <model-in> <old-stats-in> <new-stats-in> <model-out>
e.g.: gmm-est-rescale 1.mdl old.acc new.acc 2.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-boost-silence.cc "gmm-boost-silence" </td><td> <pre> Modify GMM-based model to boost (by a certain factor) all
probabilities associated with the specified phones (could be
all silence phones, or just the ones used for optional silence).
Note: this is done by modifying the GMM weights.  If the silence
model shares a GMM with other models, then it will modify the GMM
weights for all models that may correspond to silence.

Usage:  gmm-boost-silence [options] <silence-phones-list> <model-in> <model-out>
e.g.: gmm-boost-silence --boost=1.5 1:2:3 1.mdl 1_boostsil.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-diff-accs.cc "gmm-diff-accs" </td><td> <pre> Compute difference between accumulators
Usage: gmm-diff-accs [options] plus-stats minus-stats diff-stats-out </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-basis-fmllr-accs.cc "gmm-basis-fmllr-accs" </td><td> <pre> Accumulate gradient scatter from training set, either per utterance or 
for the supplied set of speakers (spk2utt option). Reads posterior to accumulate 
fMLLR stats for each speaker/utterance. Writes gradient scatter matrix.
Usage: gmm-basis-fmllr-accs [options] <model-in> <feature-rspecifier><post-rspecifier> <accs-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-basis-fmllr-training.cc "gmm-basis-fmllr-training" </td><td> <pre> Estimate fMLLR basis representation. Reads a set of gradient scatter
accumulations. Outputs basis matrices.
Usage: gmm-basis-fmllr-training [options] <model-in> <basis-wspecifier><accs-in1> <accs-in2> ... </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-basis-fmllr.cc "gmm-est-basis-fmllr" </td><td> <pre> Perform basis fMLLR adaptation in testing stage, either per utterance or
for the supplied set of speakers (spk2utt option). Reads posterior to
accumulate fMLLR stats for each speaker/utterance. Writes to a table of
matrices.
Usage: gmm-est-basis-fmllr [options] <model-in> <basis-rspecifier> <feature-rspecifier> <post-rspecifier> <transform-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-map.cc "gmm-est-map" </td><td> <pre> Do Maximum A Posteriori re-estimation of GMM-based acoustic model
Usage:  gmm-est-map [options] <model-in> <stats-in> <model-out>
e.g.: gmm-est-map 1.mdl 1.acc 2.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-adapt-map.cc "gmm-adapt-map" </td><td> <pre> Compute MAP estimates per-utterance (default) or per-speaker for
the supplied set of speakers (spk2utt option).  This will typically
be piped into gmm-latgen-map

Usage: gmm-adapt-map  [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <map-am-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-latgen-map.cc "gmm-latgen-map" </td><td> <pre> Decode features using GMM-based model.  Note: the input
<gmms-rspecifier> will typically be piped in from gmm-est-map.
Note: <model-in> is only needed for the transition-model, which isn't
included in <gmms-rspecifier>.

Usage: gmm-latgen-map [options] <model-in> <gmms-rspecifier> <fsts-rxfilename|fsts-rspecifier> <features-rspecifier> <lattice-wspecifier> [ <words-wspecifier> [ <alignments-wspecifier> ] ] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-basis-fmllr-accs-gpost.cc "gmm-basis-fmllr-accs-gpost" </td><td> <pre> Accumulate gradient scatter from training set, either per utterance or 
for the supplied set of speakers (spk2utt option). Reads Gaussian-level 
posterior to accumulate fMLLR stats for each speaker/utterance. Writes 
gradient scatter matrix.
Usage: gmm-basis-fmllr-accs-gpost [options] <model-in> <feature-rspecifier><post-rspecifier> <accs-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-basis-fmllr-gpost.cc "gmm-est-basis-fmllr-gpost" </td><td> <pre> Perform basis fMLLR adaptation in testing stage, either per utterance or
for the supplied set of speakers (spk2utt option). Reads Gaussian-level
posterior to accumulate fMLLR stats for each speaker/utterance. Writes
to a table of matrices.
Usage: gmm-est-basis-fmllr-gpost [options] <model-in> <basis-rspecifier> <feature-rspecifier> <post-rspecifier> <transform-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-latgen-tracking.cc "gmm-latgen-tracking" </td><td> <pre> Generate lattices using GMM-based model, using arc lattices from forward path.
Usage: gmm-latgen-tracking [options] model-in (fst-in|fsts-rspecifier) features-rspecifier arcs-rspecifier lattice-wspecifier [ words-wspecifier [alignments-wspecifier] ] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-latgen-faster-parallel.cc "gmm-latgen-faster-parallel" </td><td> <pre> Decode features using GMM-based model.  Uses multiple decoding threads,
but interface and behavior is otherwise the same as gmm-latgen-faster
Usage: gmm-latgen-faster-parallel [options] model-in (fst-in|fsts-rspecifier) features-rspecifier lattice-wspecifier [ words-wspecifier [alignments-wspecifier] ] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-fmllr-raw.cc "gmm-est-fmllr-raw" </td><td> <pre> Estimate fMLLR transforms in the space before splicing and linear transforms
such as LDA+MLLT, but using models in the space transformed by these transforms
Requires the original spliced features, and the full LDA+MLLT (or similar) matrix
including the 'rejected' rows (see the program get-full-lda-mat)
Usage: gmm-est-fmllr-raw [options] <model-in> <full-lda-mat-in> <feature-rspecifier> <post-rspecifier> <transform-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-fmllr-raw-gpost.cc "gmm-est-fmllr-raw-gpost" </td><td> <pre> Estimate fMLLR transforms in the space before splicing and linear transforms
such as LDA+MLLT, but using models in the space transformed by these transforms
Requires the original spliced features, and the full LDA+MLLT (or similar) matrix
including the 'rejected' rows (see the program get-full-lda-mat).  Reads in
Gaussian-level posteriors.
Usage: gmm-est-fmllr-raw-gpost [options] <model-in> <full-lda-mat-in> <feature-rspecifier> <gpost-rspecifier> <transform-wspecifier> </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-init-from-feats.cc "gmm-global-init-from-feats" </td><td> <pre> This program initializes a single diagonal GMM and does multiple iterations of
training from features stored in memory.
Usage:  gmm-global-init-feats [options] <feature-rspecifier> <model-out>
e.g.: gmm-global-init-feats scp:train.scp 1.mdl </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-info.cc "gmm-global-info" </td><td> <pre> Write to standard output various properties of GMM model
This is for a single diagonal GMM, e.g. as used for a UBM.
Usage:  gmm-global-info [options] <gmm>
e.g.:
 gmm-global-info 1.dubm
See also: gmm-info, am-info </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-latgen-faster-regtree-fmllr.cc "gmm-latgen-faster-regtree-fmllr" </td><td> <pre> Generate lattices using GMM-based model and RegTree-FMLLR adaptation.
Usage: gmm-latgen-faster-regtree-fmllr [options] model-in regtree-in (fst-in|fsts-rspecifier) features-rspecifier transform-rspecifier lattice-wspecifier [ words-wspecifier [alignments-wspecifier] ] </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-est-fmllr-global.cc "gmm-est-fmllr-global" </td><td> <pre> Estimate global fMLLR transforms, either per utterance or for the supplied
set of speakers (spk2utt option).  This version is for when you have a single
global GMM, e.g. a UBM.  Writes to a table of matrices.
Usage: gmm-est-fmllr-global [options] <gmm-in> <feature-rspecifier> <transform-wspecifier>
e.g.: gmm-est-fmllr-global 1.ubm scp:feats.scp ark:trans.1 </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-acc-mllt-global.cc "gmm-acc-mllt-global" </td><td> <pre> Accumulate MLLT (global STC) statistics: this version is for where there is
one global GMM (e.g. a UBM)
Usage:  gmm-acc-mllt-global [options] <gmm-in> <feature-rspecifier> <stats-out>
e.g.: 
 gmm-acc-mllt-global 1.dubm scp:feats.scp 1.macc </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-transform-means-global.cc "gmm-transform-means-global" </td><td> <pre> Transform GMM means with linear or affine transform
This version for a single GMM, e.g. a UBM.
Useful when estimating MLLT/STC
Usage:  gmm-transform-means-global <transform-matrix> <gmm-in> <gmm-out>
e.g.: gmm-transform-means-global 2.mat 2.dubm 3.dubm </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-get-post.cc "gmm-global-get-post" </td><td> <pre> Precompute Gaussian indices and convert immediately to top-n
posteriors (useful in iVector extraction with diagonal UBMs)
See also: gmm-gselect, fgmm-gselect, fgmm-global-gselect-to-post
 (e.g. in training UBMs, SGMMs, tied-mixture systems)
 For each frame, gives a list of the n best Gaussian indices,
 sorted from best to worst.
Usage: 
 gmm-global-get-post [options] <model-in> <feature-rspecifier> <post-wspecifier>
e.g.: gmm-global-get-post --n=20 1.gmm "ark:feature-command |" "ark,t:|gzip -c >post.1.gz" </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-gselect-to-post.cc "gmm-global-gselect-to-post" </td><td> <pre> Given features and Gaussian-selection (gselect) information for
a diagonal-covariance GMM, output per-frame posteriors for the selected
indices.  Also supports pruning the posteriors if they are below
a stated threshold, (and renormalizing the rest to sum to one)
See also: gmm-gselect, fgmm-gselect, gmm-global-get-post,
 fgmm-global-gselect-to-post

Usage:  gmm-global-gselect-to-post [options] <model-in> <feature-rspecifier> <gselect-rspecifier> <post-wspecifier>
e.g.: gmm-global-gselect-to-post 1.dubm ark:- 'ark:gunzip -c 1.gselect|' ark:- </pre> </td> </tr>
<tr> <td> \ref gmmbin/gmm-global-est-lvtln-trans.cc "gmm-global-est-lvtln-trans" </td><td> <pre> Estimate linear-VTLN transforms, either per utterance or for the supplied set of speakers (spk2utt option); this version
is for a global diagonal GMM (also known as a UBM).  Reads posteriors
indicating Gaussian indexes in the UBM.

Usage: gmm-global-est-lvtln-trans [options] <gmm-in> <lvtln-in> <feature-rspecifier> <gpost-rspecifier> <lvtln-trans-wspecifier> [<warp-wspecifier>]
e.g.: gmm-global-est-lvtln-trans 0.ubm 0.lvtln '$feats' ark,s,cs:- ark:1.trans ark:1.warp
(where the <gpost-rspecifier> will likely come from gmm-global-get-post or
gmm-global-gselect-to-post </pre> </td> </tr>
<tr> <td> \ref idlaktxpbin/convert-tree-pdf-hts.cc "convert-tree-pdf-hts" </td><td> <pre> Convert kaldi tree and gmm based model to an HTS format
Usage: convert-tree-pdf-hts [options] kaldiphoneset kaldicontextsetup tree model htstreeout htsmodelout </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-extractor-init.cc "ivector-extractor-init" </td><td> <pre> Initialize ivector-extractor
Usage:  ivector-extractor-init [options] <fgmm-in> <ivector-extractor-out>
e.g.:
 ivector-extractor-init 4.fgmm 0.ie </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-extractor-acc-stats.cc "ivector-extractor-acc-stats" </td><td> <pre> Accumulate stats for iVector extractor training
Reads in features and Gaussian-level posteriors (typically from a full GMM)
Supports multiple threads, but won't be able to make use of too many at a time
(e.g. more than about 4)
Usage:  ivector-extractor-acc-stats [options] <model-in> <feature-rspecifier><posteriors-rspecifier> <stats-out>
e.g.: 
 fgmm-global-gselect-to-post 1.fgmm '$feats' 'ark:gunzip -c gselect.1.gz|' ark:- | \\
  ivector-extractor-acc-stats 2.ie '$feats' ark,s,cs:- 2.1.acc </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-extractor-sum-accs.cc "ivector-extractor-sum-accs" </td><td> <pre> Sum accumulators for training of iVector extractor
Usage: ivector-extractor-sum-accs [options] <stats-in1> <stats-in2> ... <stats-inN> <stats-out> </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-extractor-est.cc "ivector-extractor-est" </td><td> <pre> Do model re-estimation of iVector extractor (this is
the update phase of a single pass of E-M)
Usage: ivector-extractor-est [options] <model-in> <stats-in> <model-out> </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-extract.cc "ivector-extract" </td><td> <pre> Extract iVectors for utterances, using a trained iVector extractor,
and features and Gaussian-level posteriors
Usage:  ivector-extract [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <ivector-wspecifier>
e.g.: 
 fgmm-global-gselect-to-post 1.ubm '$feats' 'ark:gunzip -c gselect.1.gz|' ark:- | \\
  ivector-extract final.ie '$feats' ark,s,cs:- ark,t:ivectors.1.ark </pre> </td> </tr>
<tr> <td> \ref ivectorbin/compute-vad.cc "compute-vad" </td><td> <pre> This program reads input features and writes out, for each utterance,
a vector of floats that are 1.0 if we judge the frame voice and 0.0
otherwise.  The algorithm is very simple and is based on thresholding
the log mel energy (and taking the consensus of threshold decisions
within a window centered on the current frame).  See the options for
more details, and egs/sid/s1/run.sh for examples; this program is
intended for use in speaker-ID.

Usage: compute-vad [options] <feats-rspecifier> <vad-wspecifier>
e.g.: compute-vad scp:feats.scp ark:vad.ark </pre> </td> </tr>
<tr> <td> \ref ivectorbin/select-voiced-frames.cc "select-voiced-frames" </td><td> <pre> Select a subset of frames of the input files, based on the output of
compute-vad or a similar program (a vector of length num-frames,
containing 1.0 for voiced, 0.0 for unvoiced).
Usage: select-voiced-frames [options] <feats-rspecifier>  <vad-rspecifier> <feats-wspecifier>
E.g.: select-voiced-frames [options] scp:feats.scp scp:vad.scp ark:- </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-normalize-length.cc "ivector-normalize-length" </td><td> <pre> Normalize length of iVectors to equal sqrt(feature-dimension)

Usage:  ivector-normalize-length [options] <ivector-rspecifier> <ivector-wspecifier>
e.g.: 
 ivector-normalize-length ark:ivectors.ark ark:normalized_ivectors.ark </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-transform.cc "ivector-transform" </td><td> <pre> Multiplies iVectors (on the left) by a supplied transformation matrix

Usage:  ivector-transform [options] <matrix-in> <ivector-rspecifier><ivector-wspecifier>
e.g.: 
 ivector-transform transform.mat ark:ivectors.ark ark:transformed_ivectors.ark </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-compute-dot-products.cc "ivector-compute-dot-products" </td><td> <pre> Computes dot-products between iVectors; useful in application of an
iVector-based system.  The 'trials-file' has lines of the form
<key1> <key2>
and the output will have the form
<key1> <key2> [<dot-product>]
(if either key could not be found, the dot-product field in the output
will be absent, and this program will print a warning)

Usage:  ivector-compute-dot-products [options] <trials-in> <ivector1-rspecifier> <ivector2-rspecifier> <scores-out>
e.g.: 
 ivector-compute-dot-products trials ark:train_ivectors.scp ark:test_ivectors.scp trials.scored
See also: ivector-plda-scoring </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-mean.cc "ivector-mean" </td><td> <pre> With 3 or 4 arguments, averages iVectors over all the
utterances of each speaker using the spk2utt file.
Input the spk2utt file and a set of iVectors indexed by
utterance; output is iVectors indexed by speaker.  If 4
arguments are given, extra argument is a table for the number
of utterances per speaker (can be useful for PLDA).  If 2
arguments are given, computes the mean of all input files and
writes out the mean vector.

Usage: ivector-mean <spk2utt-rspecifier> <ivector-rspecifier> <ivector-wspecifier> [<num-utt-wspecifier>]
or: ivector-mean <ivector-rspecifier> <mean-wxfilename>
e.g.: ivector-mean data/spk2utt exp/ivectors.ark exp/spk_ivectors.ark exp/spk_num_utts.ark
or: ivector-mean exp/ivectors.ark exp/mean.vec
See also: ivector-subtract-global-mean </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-compute-lda.cc "ivector-compute-lda" </td><td> <pre> Compute an LDA matrix for iVector system.  Reads in iVectors per utterance,
and an utt2spk file which it uses to help work out the within-speaker and
between-speaker covariance matrices.  Outputs an LDA projection to a
specified dimension.  By default it will normalize so that the projected
within-class covariance is unit, but if you set --normalize-total-covariance
to true, it will normalize the total covariance.
Note: the transform we produce is actually an affine transform which will
also set the global mean to zero.

Usage:  ivector-compute-lda [options] <ivector-rspecifier> <utt2spk-rspecifier> <lda-matrix-out>
e.g.: 
 ivector-compute-lda ark:ivectors.ark ark:utt2spk lda.mat </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-compute-plda.cc "ivector-compute-plda" </td><td> <pre> Computes a Plda object (for Probabilistic Linear Discriminant Analysis)
from a set of iVectors.  Uses speaker information from a spk2utt file
to compute within and between class variances.

Usage:  ivector-compute-plda [options] <spk2utt-rspecifier> <ivector-rspecifier> <plda-out>
e.g.: 
 ivector-compute-plda ark:spk2utt ark,s,cs:ivectors.ark plda </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-copy-plda.cc "ivector-copy-plda" </td><td> <pre> Copy a PLDA object, possibly applying smoothing to the within-class
covariance

Usage: ivector-copy-plda <plda-in> <plda-out>
e.g.: ivector-copy-plda --smoothing=0.1 plda plda.smooth0.1 </pre> </td> </tr>
<tr> <td> \ref ivectorbin/compute-eer.cc "compute-eer" </td><td> <pre> Computes Equal Error Rate
Input is a series of lines, each with two fields.
The first field must be a numeric score, and the second
either the string 'target' or 'nontarget'. 
The EER will be printed to the standard output.

Usage: compute-eer <scores-in>
e.g.: compute-eer - </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-subtract-global-mean.cc "ivector-subtract-global-mean" </td><td> <pre> Copies a table of iVectors but subtracts the global mean as
it does so.  The mean may be specified as the first argument; if not,
the sum of the input iVectors is used.

Usage: ivector-subtract-global-mean <ivector-rspecifier> <ivector-wspecifier>
or: ivector-subtract-global-mean <mean-rxfliename> <ivector-rspecifier> <ivector-wspecifier>
e.g.: ivector-subtract-global-mean scp:ivectors.scp ark:-
or: ivector-subtract-global-mean mean.vec scp:ivectors.scp ark:-
See also: ivector-mean </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-plda-scoring.cc "ivector-plda-scoring" </td><td> <pre> Computes log-likelihood ratios for trials using PLDA model
Note: the 'trials-file' has lines of the form
<key1> <key2>
and the output will have the form
<key1> <key2> [<dot-product>]
(if either key could not be found, the dot-product field in the output
will be absent, and this program will print a warning)
For training examples, the input is the iVectors averaged over speakers;
a separate archive containing the number of utterances per speaker may be
optionally supplied using the --num-utts option; this affects the PLDA
scoring (if not supplied, it defaults to 1 per speaker).

Usage: ivector-plda-scoring <plda> <train-ivector-rspecifier> <test-ivector-rspecifier>
 <trials-rxfilename> <scores-wxfilename>

e.g.: ivector-plda-scoring --num-utts=ark:exp/train/num_utts.ark plda ark:exp/train/spk_ivectors.ark ark:exp/test/ivectors.ark trials scores
See also: ivector-compute-dot-products, ivector-compute-plda </pre> </td> </tr>
<tr> <td> \ref ivectorbin/logistic-regression-train.cc "logistic-regression-train" </td><td> <pre> Trains a model using Logistic Regression with L-BFGS from
a set of vectors. The class labels in <classes-rspecifier>
must be a set of integers such that there are no gaps in 
its range and the smallest label must be 0.
Usage: logistic-regression-train <vector-rspecifier>
<classes-rspecifier> <model-out> </pre> </td> </tr>
<tr> <td> \ref ivectorbin/logistic-regression-eval.cc "logistic-regression-eval" </td><td> <pre> Evaluates a model on input vectors and outputs either
log posterior probabilities or scores.
Usage1: logistic-regression-eval <model> <input-vectors-rspecifier>
                                <output-log-posteriors-wspecifier>
Usage2: logistic-regression-eval <model> <trials-file> <input-vectors-rspecifier>
                                <output-scores-file> </pre> </td> </tr>
<tr> <td> \ref ivectorbin/logistic-regression-copy.cc "logistic-regression-copy" </td><td> <pre> Copy a logistic-regression model, possibly changing the binary mode;
also supports the --scale-priors option which can scale the prior probabilities
the model assigns to different classes (e.g., you can remove the effect of
unbalanced training data by scaling by the inverse of the class priors in the
training data)
Usage: logistic-regression-copy [options] <model-in> <model-out>
e.g.: echo '[ 2.6 1.7 3.9 1.24 7.5 ]' | logistic-regression-copy --scale-priors=- \\
  1.model scaled_priors.mdl </pre> </td> </tr>
<tr> <td> \ref ivectorbin/create-split-from-vad.cc "create-split-from-vad" </td><td> <pre> Create a feats-segment file specifying splits of an utterance based on a VAD file.
The VAD file is the output of compute-vad or a similar program (a vector 
of length num-frames, containing 1.0 for voiced, 0.0 for unvoiced). Each line of the
feats-segment file is of the following form: 
    <dst-utt> <src-utt> <first-frame> <last-frame>
Usage: create-split-from-vad [options] <vad-rspecifier>
<feats-segment-filename>
E.g.: create-split-from-vad [options] scp:vad.scp feats_segment </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-extract-online.cc "ivector-extract-online" </td><td> <pre> Extract iVectors for utterances, using a trained iVector extractor,
and features and Gaussian-level posteriors.  This version extracts an
iVector every n frames (see the --ivector-period option), by including
all frames up to that point in the utterance.  This is designed to
correspond with what will happen in a streaming decoding scenario;
the iVectors would be used in neural net training.  The iVectors are
output as an archive of matrices, indexed by utterance-id; each row
corresponds to an iVector.
See also ivector-extract-online2

Usage:  ivector-extract-online [options] <model-in> <feature-rspecifier><posteriors-rspecifier> <ivector-wspecifier>
e.g.: 
 gmm-global-get-post 1.dubm '$feats' ark:- | \\
  ivector-extract-online --ivector-period=10 final.ie '$feats' ark,s,cs:- ark,t:ivectors.1.ark </pre> </td> </tr>
<tr> <td> \ref ivectorbin/ivector-adapt-plda.cc "ivector-adapt-plda" </td><td> <pre> Adapt a PLDA object using unsupervised adaptation-data iVectors from a different
domain to the training data.

Usage: ivector-adapt-plda [options] <plda-in> <ivectors-rspecifier> <plda-out>
e.g.: ivector-adapt-plda plda ark:ivectors.ark plda.adapted </pre> </td> </tr>
<tr> <td> \ref kwsbin/lattice-to-kws-index.cc "lattice-to-kws-index" </td><td> <pre> Create an inverted index of the given lattices. The output index is in the T*T*T
semiring. For details for the semiring, please refer to Dogan Can and Muran Saraclar'slattice indexing paper.
Usage: lattice-to-kws-index [options]  utter-symtab-rspecifier lattice-rspecifier index-wspecifier
 e.g.: lattice-to-kws-index ark:utter.symtab ark:1.lats ark:global.idx </pre> </td> </tr>
<tr> <td> \ref kwsbin/kws-index-union.cc "kws-index-union" </td><td> <pre> Take a union of the indexed lattices. The input index is in the T*T*T semiring and
the output index is also in the T*T*T semiring. At the end of this program, encoded
epsilon removal, determinization and minimization will be applied.

Usage: kws-index-union [options]  index-rspecifier index-wspecifier
 e.g.: kws-index-union ark:input.idx ark:global.idx </pre> </td> </tr>
<tr> <td> \ref kwsbin/transcripts-to-fsts.cc "transcripts-to-fsts" </td><td> <pre> Build a linear acceptor for each transcription. Read in the transcriptions in archive
format and write out the linear acceptors in archive format with the same key.

Usage: transcripts-to-fsts [options]  transcriptions-rspecifier fsts-wspecifier
 e.g.: transcripts-to-fsts ark:train.tra ark:train.fsts </pre> </td> </tr>
<tr> <td> \ref kwsbin/kws-search.cc "kws-search" </td><td> <pre> Search the keywords over the index. This program can be executed parallely, either
on the index side or the keywords side; we use a script to combine the final search
results. Note that the index archive has a only key "global".
The output file is in the format:
kw utterance_id beg_frame end_frame negated_log_probs
 e.g.: KW1 1 23 67 0.6074219

Usage: kws-search [options]  index-rspecifier keywords-rspecifier results-wspecifier
 e.g.: kws-search ark:index.idx ark:keywords.fsts ark:results </pre> </td> </tr>
<tr> <td> \ref kwsbin/generate-proxy-keywords.cc "generate-proxy-keywords" </td><td> <pre> Convert the keywords into in-vocabulary words using the given phone
level edit distance fst (E.fst). The large lexicon (L2.fst) and
inverted small lexicon (L1'.fst) are also expected to be present. We
actually use the composed FST L2xE.fst to be more efficient. Ideally
we should have used L2xExL1'.fst but this is quite computationally
expensive at command level. Keywords.int is in the transcription
format. If kwlist-wspecifier is given, the program also prints out
the proxy fst in a format where each line is "kwid weight proxy".

Usage: generate-proxy-keywords [options] <L2xE.fst> <L1'.fst> \\
    <keyword-rspecifier> <proxy-wspecifier> [kwlist-wspecifier] 
 e.g.: generate-proxy-keywords L2xE.fst L1'.fst ark:keywords.int \\
                           ark:proxy.fsts [ark,t:proxy.kwlist.txt] </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-best-path.cc "lattice-best-path" </td><td> <pre> Generate 1-best path through lattices; output as transcriptions and alignments
Note: if you want output as FSTs, use lattice-1best; if you want output
with acoustic and LM scores, use lattice-1best | nbest-to-linear
Usage: lattice-best-path [options]  lattice-rspecifier [ transcriptions-wspecifier [ alignments-wspecifier] ]
 e.g.: lattice-best-path --acoustic-scale=0.1 ark:1.lats ark:1.tra ark:1.ali </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-prune.cc "lattice-prune" </td><td> <pre> Apply beam pruning to lattices
Usage: lattice-prune [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-prune --acoustic-scale=0.1 --beam=4.0 ark:1.lats ark:pruned.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-equivalent.cc "lattice-equivalent" </td><td> <pre> Test whether sets of lattices are equivalent (return with status 0 if
all were equivalent, 1 otherwise, -1 on error)
Usage: lattice-equivalent [options] lattice-rspecifier1 lattice-rspecifier2
 e.g.: lattice-equivalent ark:1.lats ark:2.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-to-nbest.cc "lattice-to-nbest" </td><td> <pre> Work out N-best paths in lattices and write out as FSTs
Note: only guarantees distinct word sequences if distinct paths in
input lattices had distinct word-sequences (this will not be true if
you produced lattices with --determinize-lattice=false, i.e. state-level
lattices).
Usage: lattice-to-nbest [options] <lattice-rspecifier> <lattice-wspecifier>
 e.g.: lattice-to-nbest --acoustic-scale=0.1 --n=10 ark:1.lats ark:nbest.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-lmrescore.cc "lattice-lmrescore" </td><td> <pre> Add lm_scale * [cost of best path through LM FST] to graph-cost of
paths through lattice.  Does this by composing with LM FST, then
lattice-determinizing (it has to negate weights first if lm_scale<0)
Usage: lattice-lmrescore [options] lattice-rspecifier lm-fst-in lattice-wspecifier
 e.g.: lattice-lmrescore --lm-scale=-1.0 ark:in.lats 'fstproject --project_output=true data/lang/G.fst|' ark:out.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-scale.cc "lattice-scale" </td><td> <pre> Apply scaling to lattice weights
Usage: lattice-scale [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-scale --lm-scale=0.0 ark:1.lats ark:scaled.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-union.cc "lattice-union" </td><td> <pre> Takes two archives of lattices (indexed by utterances) and computes the union of the individual lattice pairs (one from each archive).
Usage: lattice-union [options] lattice-rspecifier1 lattice-rspecifier2 lattice-wspecifier
 e.g.: lattice-union ark:den.lats ark:num.lats ark:union.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-to-post.cc "lattice-to-post" </td><td> <pre> Do forward-backward and collect posteriors over lattices.
Usage: lattice-to-post [options] lats-rspecifier posts-wspecifier [loglikes-wspecifier]
 e.g.: lattice-to-post --acoustic-scale=0.1 ark:1.lats ark:1.post </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-determinize.cc "lattice-determinize" </td><td> <pre> This program is deprecated, please used lattice-determinize-pruned.
lattice-determinize lattices (and apply a pruning beam)
 (see http://kaldi.sourceforge.net/lattices.html for more explanation)
 note: this program is tyically only useful if you generated state-level
 lattices, e.g. called gmm-latgen-simple with --determinize=false

Usage: lattice-determinize [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-determinize --acoustic-scale=0.1 --beam=15.0 ark:1.lats ark:det.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-oracle.cc "lattice-oracle" </td><td> <pre> Finds the path having the smallest edit-distance between two lattices.
For efficiency put the smallest lattices first (for example reference strings).
Usage: lattice-oracle [options] <test-lattice-rspecifier> <reference-rspecifier> <transcriptions-wspecifier> [<edit-distance-wspecifier>]
 e.g.: lattice-oracle ark:lat.1 'ark:sym2int.pl -f 2- data/lang/words.txt <data/test/text' ark,t:-
Note: you can use this program to compute the n-best oracle WER by first piping
the input lattices through lattice-to-nbest and then nbest-to-lattice. </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-rmali.cc "lattice-rmali" </td><td> <pre> Remove state-sequences from lattice weights
Usage: lattice-rmali [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-rmali  ark:1.lats ark:proj.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-compose.cc "lattice-compose" </td><td> <pre> Composes lattices (in transducer form, as type Lattice).  Depending
on the command-line arguments, either composes lattices with lattices,
or lattices with FSTs (rspecifiers are assumed to be lattices, and
rxfilenames are assumed to be FSTs, which have their weights interpreted
as "graph weights" when converted into the Lattice format.

Usage: lattice-compose [options] lattice-rspecifier1 (lattice-rspecifier2|fst-rxfilename2) lattice-wspecifier
 e.g.: lattice-compose ark:1.lats ark:2.lats ark:composed.lats
 or: lattice-compose ark:1.lats G.fst ark:composed.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-boost-ali.cc "lattice-boost-ali" </td><td> <pre> Boost graph likelihoods (decrease graph costs) by b * #frame-phone-errors
on each arc in the lattice.  Useful for discriminative training, e.g.
boosted MMI.  Modifies input lattices.  This version takes the reference
in the form of alignments.  Needs the model (just the transitions) to
transform pdf-ids to phones.  Takes the --silence-phones option and these
phones appearing in the lattice are always assigned zero error, or with the
--max-silence-error option, at most this error-count per frame
(--max-silence-error=1 is equivalent to not specifying --silence-phones).

Usage: lattice-boost-ali [options] model lats-rspecifier ali-rspecifier lats-wspecifier
 e.g.: lattice-boost-ali --silence-phones=1:2:3 --b=0.05 1.mdl ark:1.lats ark:1.ali ark:boosted.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-copy.cc "lattice-copy" </td><td> <pre> Copy lattices (e.g. useful for changing to text mode or changing
format to standard from compact lattice.)
Usage: lattice-copy [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-copy --write-compact=false ark:1.lats ark,t:text.lats
See also: lattice-to-fst, and the script egs/wsj/s5/utils/convert_slf.pl </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-to-fst.cc "lattice-to-fst" </td><td> <pre> Turn lattices into normal FSTs, retaining only the word labels
By default, removes all weights and also epsilons (configure with
with --acoustic-scale, --lm-scale and --rm-eps)
Usage: lattice-to-fst [options] lattice-rspecifier fsts-wspecifier
 e.g.: lattice-to-fst  ark:1.lats ark:1.fsts </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-to-phone-lattice.cc "lattice-to-phone-lattice" </td><td> <pre> Convert the words or transition-ids into phones, which are worked out
from the transition-ids.  If --replace-words=true (true by default),
replaces the words with phones, otherwise replaces the transition-ids.
If --replace-words=false, it will preserve the alignment of transition-ids/phones
to words, so that if you do 
lattice-align-words | lattice-to-phone-lattice --replace-words=false,
you can get the phones corresponding to each word in the lattice.

Usage: lattice-to-phone-lattice [options] model lattice-rspecifier lattice-wspecifier
 e.g.: lattice-to-phone-lattice 1.mdl ark:1.lats ark:phones.lats
See also: lattice-align-words, lattice-align-phones </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-interp.cc "lattice-interp" </td><td> <pre> Takes two archives of lattices (indexed by utterances) and combines
the individual lattice pairs (one from each archive).  Keeps the alignments
from the first lattice.  Equivalent to
projecting the second archive on words (lattice-project), then composing
the pairs of lattices (lattice-compose), then scaling graph and acoustic
costs by 0.5 (lattice-scale).  You can control the individual scales with
--alpha, which is the scale of the first lattices (the second is 1-alpha).
Usage: lattice-interp [options] lattice-rspecifier-a lattice-rspecifier-b lattice-wspecifier
 e.g.: lattice-compose ark:1.lats ark:2.lats ark:composed.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-project.cc "lattice-project" </td><td> <pre> Project lattices (in their transducer form); by default makes them
word->word transducers (set --project-output=false for tid->tid).
Usage: lattice-project [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-project ark:1.lats ark:word2word.lats
or: lattice-project --project-output=false ark:1.lats ark:tid2tid.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-add-trans-probs.cc "lattice-add-trans-probs" </td><td> <pre> Add transition probabilities into graph part of lattice scores,
controlled by options --transition-scale and --self-loop-scale, which
for compatibility with the original graph, would normally be set to the same
values used in graph compilatoin

Usage: lattice-add-trans-probs [options] model lattice-rspecifier lattice-wspecifier
 e.g.: lattice-add-trans-probs --transition-scale=1.0 --self-loop-scale=0.1 1.mdl ark:in.lats ark:out.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-difference.cc "lattice-difference" </td><td> <pre> Compute FST difference on lattices (remove sequences in first lattice
 that appear in second lattice)
Useful for the denominator lattice for MCE.
Usage: lattice-difference [options] lattice1-rspecifier lattice2-rspecifier lattice-wspecifier
 e.g.: lattice-difference ark:den.lats ark:num.lats ark:den_mce.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-word-align.cc "lattice-word-align" </td><td> <pre> (note: from the s5 scripts onward, this is deprecated, see lattice-align-words)
Create word-aligned lattices (in which the arcs correspond with
word boundaries)
Usage: lattice-word-align [options] <model> <lattice-rspecifier> <lattice-wspecifier>
 e.g.: lattice-word-align --silence-phones=1:2 --wbegin-phones=2:6:10:14 \\
   --wend-phones=3:7:11:15 --winternal-phones=4:8:12:16 --wbegin-and-end-phones=5:9:13:17 \\
   --silence-label=2 --partial-word-label=16342 \\
   final.mdl ark:1.lats ark:aligned.lats </pre> </td> </tr>
<tr> <td> \ref latbin/nbest-to-linear.cc "nbest-to-linear" </td><td> <pre> Takes as input lattices/n-bests which must be linear (single path);
convert from lattice to up to 4 archives containing transcriptions, alignments,
and acoustic and LM costs (note: use ark:/dev/null for unwanted outputs)
Usage: nbest-to-linear [options] <nbest-rspecifier> <alignments-wspecifier> [<transcriptions-wspecifier> [<lm-cost-wspecifier> [<ac-cost-wspecifier>]]]
 e.g.: lattice-to-nbest --n=10 ark:1.lats ark:- | \\
   nbest-to-linear ark:1.lats ark,t:1.ali ark,t:1.tra </pre> </td> </tr>
<tr> <td> \ref latbin/nbest-to-lattice.cc "nbest-to-lattice" </td><td> <pre> Read in a Table containing N-best entries from a lattices (i.e. individual
lattices with a linear structure, one for each N-best entry, indexed by
utt_id_a-1, utt_id_a-2, etc., and take the union of them for each utterance
id (e.g. utt_id_a), outputting a lattice for each.
Usage:  nbest-to-lattice <nbest-rspecifier> <lattices-wspecifier>
 e.g.: nbest-to-lattice ark:1.nbest ark:1.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-1best.cc "lattice-1best" </td><td> <pre> Compute best path through lattices and write out as FSTs
Note: differs from lattice-nbest with --n=1 because we won't
append -1 to the utterance-ids.  Differs from lattice-best-path
because output is FST.

Usage: lattice-1best [options] <lattice-rspecifier> <lattice-wspecifier>
 e.g.: lattice-1best --acoustic-scale=0.1 ark:1.lats ark:1best.lats </pre> </td> </tr>
<tr> <td> \ref latbin/linear-to-nbest.cc "linear-to-nbest" </td><td> <pre> This does the opposite of nbest-to-linear.  It takes 4 archives,
containing alignments, word-sequences, and acoustic and LM costs,
and turns it into a single archive containing FSTs with a linear
structure.  The program is called linear-to-nbest because very often
the archives concerned will represent N-best lists
Usage:  linear-to-nbest [options] <alignments-rspecifier> <transcriptions-rspecifier> (<lm-cost-rspecifier>|'') (<ac-cost-rspecifier>|'') <nbest-wspecifier>
Note: if the rspecifiers for lm-cost or ac-cost are the empty string,
these value will default to zero.
 e.g.: linear-to-nbest ark:1.ali ark:1.tra ark:1.lmscore ark:1.acscore ark:1.nbest </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-mbr-decode.cc "lattice-mbr-decode" </td><td> <pre> Do Minimum Bayes Risk decoding (decoding that aims to minimize the 
expected word error rate).  Possible outputs include the 1-best path
(i.e. the word-sequence, as a sequence of ints per utterance), the
computed Bayes Risk for each utterance, and the sausage stats as
(for each utterance) std::vector<std::vector<std::pair<int32, float> > >
for which we use the same I/O routines as for posteriors (type Posterior).
times-wspecifier writes pairs of (start-time, end-time) in frames, for
each sausage position, or for each one-best entry if --one-best-times=true.
Note: use ark:/dev/null or the empty string for unwanted outputs.
Note: times will only be very meaningful if you first use lattice-word-align.
If you need ctm-format output, don't use this program but use lattice-to-ctm-conf
with --decode-mbr=true.

Usage: lattice-mbr-decode [options]  lattice-rspecifier transcriptions-wspecifier [ bayes-risk-wspecifier [ sausage-stats-wspecifier [ times-wspecifier] ] ] 
 e.g.: lattice-mbr-decode --acoustic-scale=0.1 ark:1.lats ark:1.tra ark:/dev/null ark:1.sau </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-align-words.cc "lattice-align-words" </td><td> <pre> Convert lattices so that the arcs in the CompactLattice format correspond with
words (i.e. aligned with word boundaries).  Note: it will generally be more
efficient if you apply 'lattice-push' before this program.
Usage: lattice-align-words [options] <word-boundary-file> <model> <lattice-rspecifier> <lattice-wspecifier>
 e.g.: lattice-align-words  --silence-label=4320 --partial-word-label=4324 \\
   data/lang/phones/word_boundary.int final.mdl ark:1.lats ark:aligned.lats
Note: word-boundary file has format (on each line):
<integer-phone-id> [begin|end|singleton|internal|nonword]
See also: lattice-align-words-lexicon, for use in cases where phones
don't have word-position information. </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-to-mpe-post.cc "lattice-to-mpe-post" </td><td> <pre> Do forward-backward and collect frame level MPE posteriors over
lattices, which can be fed into gmm-acc-stats2 to do MPE traning.
Caution: this is not really MPE, this is MPFE (minimum phone frame
error).  The posteriors may be positive or negative.
Usage: lattice-to-mpe-post [options] <model> <num-posteriors-rspecifier>
 <lats-rspecifier> <posteriors-wspecifier> 
e.g.: lattice-to-mpe-post --acoustic-scale=0.1 1.mdl ark:num.post
 ark:1.lats ark:1.post </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-copy-backoff.cc "lattice-copy-backoff" </td><td> <pre> Copy a table of lattices (1st argument), but for any keys that appear
in the table from the 2nd argument, use the one from the 2nd argument.
If the sets of keys are identical, this is equivalent to copying the 2nd
table.  Note: the arguments are in this order due to the convention that
sequential access is always over the 1st argument.

Usage: lattice-copy-backoff [options] <lat-rspecifier1> <lat-rspecifier2> <lat-wspecifier>
 e.g.: lattice-copy-backoff ark:bad_but_complete.lat ark:good_but_incomplete.lat ark:out.lat </pre> </td> </tr>
<tr> <td> \ref latbin/nbest-to-ctm.cc "nbest-to-ctm" </td><td> <pre> Takes as input lattices which must be linear (single path),
and must be in CompactLattice form where the transition-ids on the arcs
have been aligned with the word boundaries... typically the input will
be a lattice that has been piped through lattice-1best and then
lattice-align-words.  It outputs ctm format (with integers in place of words),
assuming the frame length is 0.01 seconds by default (change this with the
--frame-length option).  Note: the output is in the form
<utterance-id> 1 <begin-time> <end-time> <word-id>
and you can post-process this to account for segmentation issues and to 
convert ints to words; note, the times are relative to start of the utterance.

Usage: nbest-to-ctm [options] <aligned-linear-lattice-rspecifier> <ctm-wxfilename>
e.g.: lattice-1best --acoustic-weight=0.08333 ark:1.lats | \\
      lattice-align-words data/lang/phones/word_boundary.int exp/dir/final.mdl ark:- ark:- | \\
      nbest-to-ctm ark:- 1.ctm </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-determinize-pruned.cc "lattice-determinize-pruned" </td><td> <pre> Determinize lattices, keeping only the best path (sequence of acoustic states)
for each input-symbol sequence.  This version does pruning as part of the
determinization algorithm, which is more efficient and prevents blowup.
See http://kaldi.sourceforge.net/lattices.html for more information on lattices.

Usage: lattice-determinize-pruned [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-determinize-pruned --acoustic-scale=0.1 --beam=6.0 ark:in.lats ark:det.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-to-ctm-conf.cc "lattice-to-ctm-conf" </td><td> <pre> Generate 1-best from lattices and convert into ctm with confidences.
If --decode-mbr=true, does Minimum Bayes Risk decoding, else normal
Maximum A Posteriori (but works out the confidences based on posteriors
in the lattice, using the MBR code).  Note: if you don't need confidences,
you can do lattice-1best and pipe to nbest-to-ctm. 
Note: the ctm this produces will be relative to the utterance-id.
Note: the times will only be correct if you do lattice-align-words
on the input

Usage: lattice-to-ctm-conf [options]  <lattice-rspecifier> <ctm-wxfilename>
 e.g.: lattice-to-ctm-conf --acoustic-scale=0.1 ark:1.lats 1.ctm
See also: lattice-mbr-decode, and the scripts steps/get_ctm.sh and
 steps/get_train_ctm.sh </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-arcgraph.cc "lattice-arcgraph" </td><td> <pre> Compose decoding graph with given lattices to obtain active-arc lattices.
Usage: lattice-arcgraph [options] <model-file> <decoding-graph-fst> <lattice-rspecifier> <arcs-wspecifier>
 e.g.: lattice-arcgraph final.mdl HCLG.fst ark:in.lats ark:out.arcs </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-combine.cc "lattice-combine" </td><td> <pre> Combine lattices generated by different systems by removing the total
cost of all paths (backward cost) from individual lattices and doing
a union of the reweighted lattices.  Note: the acoustic and LM scales
that this program applies are not removed before outputting the lattices.
Intended for use in system combination prior to MBR decoding, see comments
in code.
Usage: lattice-combine [options] lattice-rspecifier1 lattice-rspecifier2 [lattice-rspecifier3 ... ] lattice-wspecifier </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-reverse.cc "lattice-reverse" </td><td> <pre> Time reversal of compact lattice and write out as lattice
Usage: lattice-reverse [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-reverse ark:1.lats ark:1.reverse.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-rescore-mapped.cc "lattice-rescore-mapped" </td><td> <pre> Replace the acoustic scores on a lattice using log-likelihoods read in
as a matrix for each utterance, indexed (frame, pdf-id).  This does the same
as (e.g.) gmm-rescore-lattice, but from a matrix.  The "mapped" means that
the transition-model is used to map transition-ids to pdf-ids.  (c.f.
latgen-faster-mapped).  Note: <transition-model-in> can be any type of
model file, e.g. GMM-based or neural-net based; only the transition model is read.

Usage: lattice-rescore-mapped [options] <transition-model-in> <lattice-rspecifier> <loglikes-rspecifier> <lattice-wspecifier>
 e.g.: nnet-logprob [args] .. | lattice-rescore-mapped final.mdl ark:1.lats ark:- ark:2.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-depth.cc "lattice-depth" </td><td> <pre> Compute the lattice depths in terms of the average number of arcs that
cross a frame.  See also lattice-depth-per-frame
Usage: lattice-depth <lattice-rspecifier> [<depth-wspecifier>]
E.g.: lattice-depth ark:- ark,t:- </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-align-phones.cc "lattice-align-phones" </td><td> <pre> Convert lattices so that the arcs in the CompactLattice format correspond with
phones.  The output symbols are still words, unless you specify --replace-output-symbols=true
Usage: lattice-align-phones [options] <model> <lattice-rspecifier> <lattice-wspecifier>
 e.g.: lattice-align-phones final.mdl ark:1.lats ark:phone_aligned.lats
See also: lattice-to-phone-lattice, lattice-align-words, lattice-align-words-lexicon
Note: if you just want the phone alignment from a lattice, the easiest path is
 lattice-1best | nbest-to-linear [keeping only alignment] | ali-to-phones
If you want the words and phones jointly (i.e. pronunciations of words, with word
alignment), try
 lattice-1best | nbest-to-prons </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-to-smbr-post.cc "lattice-to-smbr-post" </td><td> <pre> Do forward-backward and collect frame level posteriors for
the state-level minimum Bayes Risk criterion (SMBR), which
is like MPE with the criterion at a context-dependent state level.
The output may be fed into gmm-acc-stats2 or similar to train the
models discriminatively.  The posteriors may be positive or negative.
Usage: lattice-to-smbr-post [options] <model> <num-posteriors-rspecifier>
 <lats-rspecifier> <posteriors-wspecifier> 
e.g.: lattice-to-smbr-post --acoustic-scale=0.1 1.mdl ark:num.post
 ark:1.lats ark:1.post </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-determinize-pruned-parallel.cc "lattice-determinize-pruned-parallel" </td><td> <pre> Determinize lattices, keeping only the best path (sequence of acoustic states)
for each input-symbol sequence.  This is a version of lattice-determnize-pruned
that accepts the --num-threads option.  These programs do pruning as part of the
determinization algorithm, which is more efficient and prevents blowup.
See http://kaldi.sourceforge.net/lattices.html for more information on lattices.

Usage: lattice-determinize-pruned-parallel [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-determinize-pruned-parallel --acoustic-scale=0.1 --beam=6.0 ark:in.lats ark:det.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-add-penalty.cc "lattice-add-penalty" </td><td> <pre> Add word insertion penalty to the lattice.
Note: penalties are negative log-probs, base e, and are added to the
'language model' part of the cost.

Usage: lattice-add-penalty [options] <lattice-rspecifier> <lattice-wspecifier>
 e.g.: lattice-add-penalty --word-ins-penalty=1.0 ark:- ark:- </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-align-words-lexicon.cc "lattice-align-words-lexicon" </td><td> <pre> Convert lattices so that the arcs in the CompactLattice format correspond with
words (i.e. aligned with word boundaries).  This is the newest form, that
reads in a lexicon in integer format, where each line is (integer id of)
 word-in word-out phone1 phone2 ... phoneN
(note: word-in is word before alignment, word-out is after, e.g. for replacing
<eps> with SIL or vice versa)
This may be more efficient if you first apply 'lattice-push'.
Usage: lattice-align-words-lexicon [options] <lexicon-file> <model> <lattice-rspecifier> <lattice-wspecifier>
 e.g.: lattice-align-words-lexicon  --partial-word-label=4324 --max-expand 10.0 --test true \\
   data/lang/phones/align_lexicon.int final.mdl ark:1.lats ark:aligned.lats
See also: lattice-align-words, which is only applicable if your phones have word-position
markers, i.e. each phone comes in 5 versions like AA_B, AA_I, AA_W, AA_S, AA. </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-push.cc "lattice-push" </td><td> <pre> Push lattices, in CompactLattice format, so that the strings are as
close to the start as possible, and the lowest cost weight for each
state except the start state is (0, 0).  This can be helpful prior to
word-alignment (in this case, only strings need to be pushed)

Usage: lattice-push [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-push ark:1.lats ark:2.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-minimize.cc "lattice-minimize" </td><td> <pre> Minimize lattices, in CompactLattice format.  Should be applied to
determinized lattices (e.g. produced with --determinize-lattice=true)
Note: by default this program
pushes the strings and weights prior to minimization.Usage: lattice-minimize [options] lattice-rspecifier lattice-wspecifier
 e.g.: lattice-minimize ark:1.lats ark:2.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-limit-depth.cc "lattice-limit-depth" </td><td> <pre> Limit the number of arcs crossing any frame, to a specified maximum.
Requires an acoustic scale, because forward-backward Viterbi probs are
needed, which will be affected by this.

Usage: lattice-limit-depth [options] <lattice-rspecifier> <lattice-wspecifier>
E.g.: lattice-limit-depth --max-arcs-per-frame=1000 --acoustic-scale=0.1 ark:- ark:- </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-depth-per-frame.cc "lattice-depth-per-frame" </td><td> <pre> For each lattice, compute a vector of length (num-frames) saying how
may arcs cross each frame.  See also lattice-depth
Usage: lattice-depth-per-frame <lattice-rspecifier> <depth-wspecifier>
E.g.: lattice-depth-per-frame ark:- ark,t:- </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-confidence.cc "lattice-confidence" </td><td> <pre> Compute sentence-level lattice confidence measures for each lattice.
The output is simly the difference between the total costs of the best and
second-best paths in the lattice (or a very large value if the lattice
had only one path).  Caution: this is not necessarily a very good confidence
measure.  You almost certainly want to specify the acoustic scale.
If the input is a state-level lattice, you need to specify
--read-compact-lattice=false, or the confidences will be very small
(and wrong).  You can get word-level confidence info from lattice-mbr-decode.

Usage: lattice-confidence <lattice-rspecifier> <confidence-wspecifier>
E.g.: lattice-confidence --acoustic-scale=0.08333 ark:- ark,t:- </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-determinize-phone-pruned.cc "lattice-determinize-phone-pruned" </td><td> <pre> Determinize lattices, keeping only the best path (sequence of
acoustic states) for each input-symbol sequence. This version does
phone inertion when doing a first pass determinization, it then
removes the inserted symbols and does a second pass determinization.
It also does pruning as part of the determinization algorithm, which
is more efficient and prevents blowup.

Usage: lattice-determinize-phone-pruned [options] <model> \\
                  <lattice-rspecifier> <lattice-wspecifier>
 e.g.: lattice-determinize-phone-pruned --acoustic-scale=0.1 \\
                            final.mdl ark:in.lats ark:det.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-determinize-phone-pruned-parallel.cc "lattice-determinize-phone-pruned-parallel" </td><td> <pre> Determinize lattices, keeping only the best path (sequence of
acoustic states) for each input-symbol sequence. This is a version
of lattice-determinize-phone-pruned that accepts the --num-threads
option. The program does phone insertion when doing a first pass
determinization, it then removes the inserted symbols and does a
second pass determinization. It also does pruning as part of the
determinization algorithm, which is more efficient and prevents
blowup.

Usage: lattice-determinize-phone-pruned-parallel [options] \\
                 <model> <lattice-rspecifier> <lattice-wspecifier>
 e.g.: lattice-determinize-phone-pruned-parallel \\
           --acoustic-scale=0.1 final.mdl ark:in.lats ark:det.lats </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-expand-ngram.cc "lattice-expand-ngram" </td><td> <pre> Expand lattices so that each arc has a unique n-label history, for
a specified n (defaults to 3).
Usage: lattice-expand-ngram [options] lattice-rspecifier lattice-wspecifier
e.g.: lattice-expand-ngram --n=3 ark:lat ark:expanded_lat </pre> </td> </tr>
<tr> <td> \ref latbin/lattice-lmrescore-const-arpa.cc "lattice-lmrescore-const-arpa" </td><td> <pre> Rescores lattice with the ConstArpaLm format language model. The LM
will be wrapped into the DeterministicOnDemandFst interface and the
rescoring is done by composing with the wrapped LM using a special
type of composition algorithm. Determinization will be applied on
the composed lattice.

Usage: lattice-lmrescore-const-arpa [options] lattice-rspecifier \\
                                   const-arpa-in lattice-wspecifier
 e.g.: lattice-lmrescore-const-arpa --lm-scale=-1.0 ark:in.lats \\
                                   const_arpa ark:out.lats </pre> </td> </tr>
<tr> <td> \ref latbin/nbest-to-prons.cc "nbest-to-prons" </td><td> <pre> Reads lattices which must be linear (single path), and must be in
CompactLattice form where the transition-ids on the arcs
have been aligned with the word boundaries (see lattice-align-words*)
and outputs a vaguely ctm-like format where each line is of the form:
<utterance-id> <begin-frame> <num-frames> <word> <phone1> <phone2> ... <phoneN>
where the words and phones will both be written as integers.  For arcs
in the input lattice that don't correspond to words, <word> may be zero; this
will typically be the case for the optional silences.

Usage: nbest-to-prons [options] <model> <aligned-linear-lattice-rspecifier> <output-wxfilename>
e.g.: lattice-1best --acoustic-weight=0.08333 ark:1.lats | \\
      lattice-align-words data/lang/phones/word_boundary.int exp/dir/final.mdl ark:- ark:- | \\
      nbest-to-prons exp/dir/final.mdl ark:- 1.prons
Note: the type of the model doesn't matter as only the transition-model is read. </pre> </td> </tr>
<tr> <td> \ref lmbin/arpa-to-const-arpa.cc "arpa-to-const-arpa" </td><td> <pre> Converts an Arpa format language model into ConstArpaLm format,
which is an in-memory representation of the pre-built Arpa language
model. The output language model can then be read in by a program
that wants to rescore lattices. We assume that the words in the
input arpa language model has been converted to integers.

The program is used joinly with utils/map_arpa_lm.pl to build
ConstArpaLm format language model. We first map the words in an Arpa
format language model to integers using utils/map_arpa_m.pl, and
then use this program to build a ConstArpaLm format language model.

Usage: arpa-to-const-arpa [opts] <input-arpa> <const-arpa>
 e.g.: arpa-to-const-arpa --bos-symbol=1 --eos-symbol=2 \\
                          arpa.txt const_arpa </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-info.cc "nnet-am-info" </td><td> <pre> Print human-readable information about the neural network
acoustic model to the standard output
Usage:  nnet-am-info [options] <nnet-in>
e.g.:
 nnet-am-info 1.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-init.cc "nnet-init" </td><td> <pre> Initialize the neural network from a config file with a line for each
component.  Note, this only outputs the neural net itself, not the associated
information such as the transition-model; you'll probably want to pipe
the output into something like nnet-am-init.

Usage:  nnet-init [options] <config-in> <raw-nnet-out>
e.g.:
 nnet-init tree topo nnet.config 1.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-train-simple.cc "nnet-train-simple" </td><td> <pre> Train the neural network parameters with backprop and stochastic
gradient descent using minibatches.  Training examples would be
produced by nnet-get-egs.

Usage:  nnet-train-simple [options] <model-in> <training-examples-in> <model-out>

e.g.:
nnet-train-simple 1.nnet ark:1.egs 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-train-ensemble.cc "nnet-train-ensemble" </td><td> <pre> Train an ensemble of neural networks with backprop and stochastic
gradient descent using minibatches.  Modified version of nnet-train-simple.
Implements parallel gradient descent with a term that encourages the nnets to
produce similar outputs.

Usage:  nnet-train-ensemble [options] <model-in-1> <model-in-2> ... <model-in-n>  <training-examples-in> <model-out-1> <model-out-2> ... <model-out-n> 

e.g.:
 nnet-train-ensemble 1.1.nnet 2.1.nnet ark:egs.ark 2.1.nnet 2.2.nnet  </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-train-transitions.cc "nnet-train-transitions" </td><td> <pre> Train the transition probabilities of a neural network acoustic model

Usage:  nnet-train-transitions [options] <nnet-in> <alignments-rspecifier> <nnet-out>
e.g.:
 nnet-train-transitions 1.nnet "ark:gunzip -c ali.*.gz|" 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-latgen-faster.cc "nnet-latgen-faster" </td><td> <pre> Generate lattices using neural net model.
Usage: nnet-latgen-faster [options] <nnet-in> <fst-in|fsts-rspecifier> <features-rspecifier> <lattice-wspecifier> [ <words-wspecifier> [<alignments-wspecifier>] ] </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-copy.cc "nnet-am-copy" </td><td> <pre> Copy a (nnet2) neural net and its associated transition model,
possibly changing the binary mode
Also supports multiplying all the learning rates by a factor
(the --learning-rate-factor option) and setting them all to a given
value (the --learning-rate options)

Usage:  nnet-am-copy [options] <nnet-in> <nnet-out>
e.g.:
 nnet-am-copy --binary=false 1.mdl text.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-init.cc "nnet-am-init" </td><td> <pre> Initialize the neural network acoustic model and its associated
transition-model, from a tree, a topology file, and a neural-net
without an associated acoustic model.
See example scripts to see how this works in practice.

Usage:  nnet-am-init [options] <tree-in> <topology-in> <raw-nnet-in> <nnet-am-out>
or:  nnet-am-init [options] <transition-model-in> <raw-nnet-in> <nnet-am-out>
e.g.:
 nnet-am-init tree topo "nnet-init nnet.config - |" 1.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-insert.cc "nnet-insert" </td><td> <pre> Insert components into a neural network-based acoustic model.
This is mostly intended for adding new hidden layers to neural networks.
You can either specify the option --insert-at=n (specifying the index of
the component after which you want your neural network inserted), or by
default this program will insert it just before the component before the
softmax component.  CAUTION: It will also randomize the parameters of the
component before the softmax (typically AffineComponent), with stddev equal
to the --stddev-factor option (default 0.1), times the inverse square root
of the number of inputs to that component.
Set --randomize-next-component=false to turn this off.

Usage:  nnet-insert [options] <nnet-in> <raw-nnet-to-insert-in> <nnet-out>
e.g.:
 nnet-insert 1.nnet "nnet-init hidden_layer.config -|" 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-align-compiled.cc "nnet-align-compiled" </td><td> <pre> Align features given neural-net-based model
Usage:   nnet-align-compiled [options] model-in graphs-rspecifier feature-rspecifier alignments-wspecifier
e.g.: 
 nnet-align-compiled 1.mdl ark:graphs.fsts scp:train.scp ark:1.ali
or:
 compile-train-graphs tree 1.mdl lex.fst ark:train.tra b, ark:- | \\
   nnet-align-compiled 1.mdl ark:- scp:train.scp t, ark:1.ali </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-compute-prob.cc "nnet-compute-prob" </td><td> <pre> Computes and prints the average log-prob per frame of the given data with a
neural net.  The input of this is the output of e.g. nnet-get-egs
Aside from the logging output, which goes to the standard error, this program
prints the average log-prob per frame to the standard output.
Also see nnet-logprob, which produces a matrix of log-probs for each utterance.

Usage:  nnet-compute-prob [options] <model-in> <training-examples-in>
e.g.: nnet-compute-prob 1.nnet ark:valid.egs </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-copy-egs.cc "nnet-copy-egs" </td><td> <pre> Copy examples (typically single frames) for neural network training,
possibly changing the binary mode.  Supports multiple wspecifiers, in
which case it will write the examples round-robin to the outputs.

Usage:  nnet-copy-egs [options] <egs-rspecifier> <egs-wspecifier1> [<egs-wspecifier2> ...]

e.g.
nnet-copy-egs ark:train.egs ark,t:text.egs
or:
nnet-copy-egs ark:train.egs ark:1.egs ark:2.egs </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-shrink.cc "nnet-shrink" </td><td> <pre> Using a validation set, compute optimal scaling parameters for each
class of neural network parameters (i.e. each updatable component), to
maximize validation-set objective function.

Usage:  nnet-shrink [options] <model-in> <valid-examples-in> <model-out>

e.g.:
 nnet-shrink 1.nnet ark:valid.egs 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-combine.cc "nnet-combine" </td><td> <pre> Using a validation set, compute an optimal combination of a number of
neural nets (the combination weights are separate for each layer and
do not have to sum to one).  The optimization is BFGS, which is initialized
from the best of the individual input neural nets (or as specified by
--initial-model)

Usage:  nnet-combine [options] <model-in1> <model-in2> ... <model-inN> <valid-examples-in> <model-out>

e.g.:
 nnet-combine 1.1.nnet 1.2.nnet 1.3.nnet ark:valid.egs 2.nnet
Caution: the first input neural net must not be a gradient. </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-combine-a.cc "nnet-combine-a" </td><td> <pre> This is a "special case" of neural net combination.  It takes a previous
iteration's model, and N other models that have been trained in
parallel with SGD on different batches.  If there are L updatable components,
it first uses the validation set to train L parameters \\alpha_l, consisting of step-lengths
along the direction (old-model) -> (average of trained models).  There is a threshold
"valid-impr-thresh" (default 0.5).  If the validation-set improvement is more than
this, we skip the next step.  The next step is to "overshoot" by a specified factor,
e.g. 1.8.  (This should be strictly less than 2).  Once we have the resulting parameters
\\alpha_l, we multiply the per-layer learning rates by those factors, subject to sanity-preserving
limits on the changes and a minimum learning-rate (see the other options)

Usage:  nnet-combine-a [options] <old-model> <new-model1> <new-model2> ... <new-modelN> <valid-examples-in> <model-out>

e.g.:
 nnet-combine 1.1.nnet 1.2.nnet 1.3.nnet ark:valid.egs 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-average.cc "nnet-am-average" </td><td> <pre> This program averages (or sums, if --sum=true) the parameters over a
number of neural nets.  If you supply the option --skip-last-layer=true,
the parameters of the last updatable layer are copied from <model1> instead
of being averaged (useful in multi-language scenarios).
The --weights option can be used to weight each model differently.

Usage:  nnet-am-average [options] <model1> <model2> ... <modelN> <model-out>

e.g.:
 nnet-am-average 1.1.nnet 1.2.nnet 1.3.nnet 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-combine.cc "nnet-am-combine" </td><td> <pre> Using a validation set, compute an optimal combination of a number of
neural nets (the combination weights are separate for each layer and
do not have to sum to one).  The optimization is BFGS, which is initialized
from the best of the individual input neural nets.

Usage:  nnet-am-combine [options] <model-in1> <model-in2> ... <model-inN> <valid-examples-in> <model-out>

e.g.:
 nnet-am-combine 1.1.nnet 1.2.nnet 1.3.nnet ark:valid.egs 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-compute.cc "nnet-am-compute" </td><td> <pre> Does the neural net computation for each file of input features, and
outputs as a matrix the result.  Used mostly for debugging.
Note: if you want it to apply a log (e.g. for log-likelihoods), use
--apply-log=true

Usage:  nnet-am-compute [options] <model-in> <feature-rspecifier> <feature-or-loglikes-wspecifier> </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-shrink.cc "nnet-am-shrink" </td><td> <pre> Using a validation set, compute optimal scaling parameters for each
class of neural network parameters (i.e. each updatable component), to
maximize validation-set objective function.

Usage:  nnet-am-shrink [options] <model-in> <valid-examples-in> <model-out>

e.g.:
 nnet-am-shrink 1.nnet ark:valid.egs 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-mixup.cc "nnet-am-mixup" </td><td> <pre> Add mixture-components to a neural net (comparable to mixtures in a Gaussian
mixture model).  Number of mixture components must be greater than the number
of pdfs

Usage:  nnet-am-mixup [options] <nnet-in> <nnet-out>
e.g.:
 nnet-am-mixup --power=0.3 --num-mixtures=5000 1.mdl 2.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-get-egs.cc "nnet-get-egs" </td><td> <pre> Get frame-by-frame examples of data for neural network training.
Essentially this is a format change from features and posteriors
into a special frame-by-frame format.  To split randomly into
different subsets, do nnet-copy-egs with --random=true, but
note that this does not randomize the order of frames.

Usage:  nnet-get-egs [options] <features-rspecifier> <pdf-post-rspecifier> <training-examples-out>

An example [where $feats expands to the actual features]:
nnet-get-egs --left-context=8 --right-context=8 "$feats" \\
  "ark:gunzip -c exp/nnet/ali.1.gz | ali-to-pdf exp/nnet/1.nnet ark:- ark:- | ali-to-post ark:- ark:- |" \\
   ark:- 
Note: the --left-context and --right-context would be derived from
the output of nnet-info. </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-train-parallel.cc "nnet-train-parallel" </td><td> <pre> Train the neural network parameters with backprop and stochastic
gradient descent using minibatches.  As nnet-train-simple, but
uses multiple threads in a Hogwild type of update (for CPU, not GPU).

Usage:  nnet-train-parallel [options] <model-in> <training-examples-in> <model-out>

e.g.:
nnet-train-parallel --num-threads=8 1.nnet ark:1.1.egs 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-gradient.cc "nnet-gradient" </td><td> <pre> Compute neural net gradient using backprop.  Can use multiple threads
using --num-threads option.   Note: this is in addition to any BLAS-level
multi-threading.  Note: the model gradient is written in the same format
as the model, with the transition-model included.

Usage:  nnet-gradient [options] <model-in> <training-examples-in> <model-gradient-out>

e.g.:  nnet-gradient 1.nnet ark:1.egs 1.gradient </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-select-egs.cc "nnet-select-egs" </td><td> <pre> Select a subset of the input examples, every k'th example of n.
More precisely, numbering examples from 0, selects every example such
that the number m of the example is equivalent to k modulo n (so k
does not have to be < n).
Usage:  nnet-select-egs [options] <egs-rspecifier> <egs-wspecifier1>

e.g.
nnet-select-egs --n=3 --k=1 ark:train.egs ark:- </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-combine-fast.cc "nnet-combine-fast" </td><td> <pre> Using a validation set, compute an optimal combination of a number of
neural nets (the combination weights are separate for each layer and
do not have to sum to one).  The optimization is BFGS, which is initialized
from the best of the individual input neural nets (or as specified by
--initial-model)

Usage:  nnet-combine-fast [options] <model-in1> <model-in2> ... <model-inN> <valid-examples-in> <model-out>

e.g.:
 nnet-combine-fast 1.1.nnet 1.2.nnet 1.3.nnet ark:valid.egs 2.nnet
Caution: the first input neural net must not be a gradient. </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-subset-egs.cc "nnet-subset-egs" </td><td> <pre> Creates a random subset of the input examples, of a specified size.
Uses no more memory than the size of the subset.

Usage:  nnet-subset-egs [options] <egs-rspecifier> [<egs-wspecifier2> ...]

e.g.
nnet-subset-egs [args] ark:- | nnet-subset-egs --n=1000 ark:- ark:subset.egs </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-shuffle-egs.cc "nnet-shuffle-egs" </td><td> <pre> Copy examples (typically single frames) for neural network training,
from the input to output, but randomly shuffle the order.  This program will keep
all of the examples in memory at once, so don't give it too many.

Usage:  nnet-shuffle-egs [options] <egs-rspecifier> <egs-wspecifier>

nnet-shuffle-egs --srand=1 ark:train.egs ark:shuffled.egs </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-fix.cc "nnet-am-fix" </td><td> <pre> Copy a (cpu-based) neural net and its associated transition model,
but modify it to remove certain pathologies.  We use the average
derivative statistics stored with the layers derived from
NonlinearComponent.  Note: some processes, such as nnet-combine-fast,
may not process these statistics correctly, and you may have to recover
them using the --stats-from option of nnet-am-copy before you use.
this program.

Usage:  nnet-am-fix [options] <nnet-in> <nnet-out>
e.g.:
 nnet-am-fix 1.mdl 1_fixed.mdl
or:
 nnet-am-fix --get-counts-from=1.gradient 1.mdl 1_shrunk.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-logprob.cc "nnet-logprob" </td><td> <pre> Do the forward computation for a neural net acoustic model (and division by
the prior, if --divide-by-priors=true), and output as an archive the matrix
of log probabilities for each utterance, e.g. for input to latgen-faster-mapped
(note: you can also directly use nnet-latgen-faster.

Usage: nnet-logprob [options] <model-in> <features-rspecifier> <logprobs-wspecifier>

e.g.: nnet-logprob 1.nnet "$feats" ark:- | latgen-faster-mapped ...  </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-logprob2.cc "nnet-logprob2" </td><td> <pre> Do the forward computation for a neural net acoustic model, and output
matrix of logprobs.  This version of the program outputs to two tables,
one table of probabilities without prior division and one table of
log-probs with prior division.  It is intended for use in discriminative
training.

Usage: nnet-logprob2 [options] <model-in> <features-rspecifier> <probs-wspecifier-not-divided> <logprobs-wspecifier-divided>

e.g.: nnet-logprob2 1.nnet "$feats" ark:- "ark:|logprob-to-post ark:- 1.post" ark:- \\        | latgen-faster-mapped [args] </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-logprob2-parallel.cc "nnet-logprob2-parallel" </td><td> <pre> Do the forward computation for a neural net acoustic model, and output
matrix of logprobs.  This version of the program outputs to two tables,
one table of probabilities without prior division and one table of
log-probs with prior division.  It is intended for use in discriminative
training.  This version supports multi-threaded operation (--num-threads
option)

Usage: nnet-logprob2-parallel [options] <model-in> <features-rspecifier> <probs-wspecifier-not-divided> <logprobs-wspecifier-divided>

e.g.: nnet-logprob2-parallel 1.nnet "$feats" ark:- "ark:|logprob-to-post ark:- 1.post" ark:- \\        | latgen-faster-mapped [args] </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-logprob-parallel.cc "nnet-logprob-parallel" </td><td> <pre> Do the forward computation for a neural net acoustic model, and output
matrix of logprobs (including division by prior).

Usage: nnet-logprob-parallel [options] <model-in> <features-rspecifier> <logprobs-wspecifier>

e.g.: nnet-logprob-parallel 1.nnet "$feats" ark:- | latgen-faster-mapped [args] </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-stats.cc "nnet-am-stats" </td><td> <pre> Print some statistics about the average derivatives of the sigmoid layers
of the neural net, that are stored in the net

Usage:  nnet-am-stats [options] <nnet-in>
e.g.:
 nnet-am-stats 1.mdl 1_fixed.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-rescale.cc "nnet-am-rescale" </td><td> <pre> Rescale the parameters in a neural net to achieve certain target
statistics, relating to the average derivative of the sigmoids
measured at some supplied data.  This relates to how saturated
the sigmoids are (we try to match the statistics of `good' neural
nets).

Usage:  nnet-am-rescale [options] <nnet-in> <examples-in> <nnet-out>
e.g.:
 nnet-am-rescale 1.mdl valid.egs 1_rescaled.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-limit-rank.cc "nnet-am-limit-rank" </td><td> <pre> Copy a (cpu-based) neural net and its associated transition model,
but modify it to reduce the effective parameter count by limiting
the rank of weight matrices.

Usage:  nnet-am-limit-rank [options] <nnet-in> <nnet-out>
e.g.:
 nnet-am-limit-rank 1.mdl 1_limited.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-latgen-faster-parallel.cc "nnet-latgen-faster-parallel" </td><td> <pre> Generate lattices using neural net model.
Usage: nnet-latgen-faster-parallel [options] <nnet-in> <fst-in|fsts-rspecifier> <features-rspecifier> <lattice-wspecifier> [ <words-wspecifier> [<alignments-wspecifier>] ] </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-to-raw-nnet.cc "nnet-to-raw-nnet" </td><td> <pre> Copy a (cpu-based) neural net: reads the AmNnet with its transition model, but
writes just the Nnet with no transition model (i.e. the raw neural net.)

Usage:  nnet-to-raw-nnet [options] <nnet-in> <raw-nnet-out>
e.g.:
 nnet-to-raw-nnet --binary=false 1.mdl 1.raw </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-compute.cc "nnet-compute" </td><td> <pre> Does the neural net computation for each file of input features, and
outputs as a matrix the result.  Used mostly for debugging.
Note: if you want it to apply a log (e.g. for log-likelihoods), use
--apply-log=true.  Unlike nnet-am-compute, this version reads a 'raw'
neural net

Usage:  nnet-compute [options] <raw-nnet-in> <feature-rspecifier> <feature-or-loglikes-wspecifier> </pre> </td> </tr>
<tr> <td> \ref nnet2bin/raw-nnet-concat.cc "raw-nnet-concat" </td><td> <pre> Concatenate two 'raw' neural nets, e.g. as output by nnet-init or
nnet-to-raw-nnet

Usage:  raw-nnet-concat [options] <raw-nnet-in1> <raw-nnet-in2> <raw-nnet-out>
e.g.:
 raw-nnet-concat nnet1 nnet2 nnet_concat </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-limit-rank-final.cc "nnet-am-limit-rank-final" </td><td> <pre> Copy a (cpu-based) neural net and its associated transition model,
but modify it to reduce the effective parameter count by limiting
the rank of the last affine component, which is replaced with two
affine components.  You specify the number of singlular values to
retain as e.g. --dim=200

Usage:  nnet-am-limit-rank-final [options] <nnet-in> <nnet-out>
e.g.:
 nnet-am-limit-rank-final --dim=200 1.mdl 1_limited.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/raw-nnet-info.cc "raw-nnet-info" </td><td> <pre> Print human-readable information about the raw neural network
to the standard output
Usage:  raw-nnet-info [options] <nnet-in>
e.g.:
 raw-nnet-info 1.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-get-feature-transform.cc "nnet-get-feature-transform" </td><td> <pre> Get feature-projection transform using stats obtained with acc-lda.
See comments in the code of nnet2/get-feature-transform.h for more
information.

Usage:  nnet-get-feature-transform [options] <matrix-out> <lda-acc-1> <lda-acc-2> ... </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-compute-from-egs.cc "nnet-compute-from-egs" </td><td> <pre> Does the neural net computation, taking as input the nnet-training examples
(typically an archive with the extension .egs), ignoring the labels; it
outputs as a matrix the result.  Used mostly for debugging.

Usage:  nnet-compute-from-egs [options] <raw-nnet-in> <egs-rspecifier> <feature-wspecifier>
e.g.:  nnet-compute-from-egs 'nnet-to-raw-nnet final.mdl -|' egs.10.1.ark ark:- </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-widen.cc "nnet-am-widen" </td><td> <pre> Copy a (cpu-based) neural net and its associated transition model,
possibly changing the binary mode
Also supports multiplying all the learning rates by a factor
(the --learning-rate-factor option) and setting them all to a given
value (the --learning-rate options)

Usage:  nnet-am-widen [options] <nnet-in> <nnet-out>
e.g.:
 nnet-am-widen --hidden-layer-dim=1024 1.mdl 2.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-show-progress.cc "nnet-show-progress" </td><td> <pre> Given an old and a new model and some training examples (possibly held-out),
show the average objective function given the mean of the two models,
and the breakdown by component of why this happened (computed from
derivative information).  Also shows parameter differences per layer.
If training examples not provided, only shows parameter differences per
layer.

Usage:  nnet-show-progress [options] <old-model-in> <new-model-in> [<training-examples-in>]
e.g.: nnet-show-progress 1.nnet 2.nnet ark:valid.egs </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-limit-degradation.cc "nnet-limit-degradation" </td><td> <pre> Given an old and a new model and some training examples (possibly held-out),
produce an output model that will generally be the same as the new model,
but in cases where changes a particular component led to an objective function
change per example that's less than -t for a particular threshold t>=0
(default --threshold=0.0001), will regress towards the old model.  Does this
by repeatedly downscaling by a scale (default --scale=0.75) until we satisfy
the criterion.
Usage:  nnet-limit-degradation [options] <old-model-in> <new-model-in> <training-examples-in> <model-out>
e.g.: nnet-limit-degradation 1.nnet 2.nnet ark:valid.egs 2new.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-get-feature-transform-multi.cc "nnet-get-feature-transform-multi" </td><td> <pre> Get feature-projection transform using stats obtained with acc-lda.
The file <index-list> contains a series of line, each containing a list
of integer indexes.  For each line we create a transform of the same type
as nnet-get-feature-transform would produce, taking as input just the
listed feature dimensions.  The output transform will be the concatenation
of all these transforms.  The output-dim will be the number of integers in
the file <index-list> (the individual transforms are not dimension-reducing).
Do not set the --dim option.Usage:  nnet-get-feature-transform-multi [options] <index-list> <lda-acc-1> <lda-acc-2> ... <lda-acc-n> <matrix-out> </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-copy-egs-discriminative.cc "nnet-copy-egs-discriminative" </td><td> <pre> Copy examples for discriminative neural
network training.  Supports multiple wspecifiers, in
which case it will write the examples round-robin to the outputs.

Usage:  nnet-copy-egs-discriminative [options] <egs-rspecifier> <egs-wspecifier1> [<egs-wspecifier2> ...]

e.g.
nnet-copy-egs-discriminative ark:train.degs ark,t:text.degs
or:
nnet-copy-egs-discriminative ark:train.degs ark:1.degs ark:2.degs </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-get-egs-discriminative.cc "nnet-get-egs-discriminative" </td><td> <pre> Get examples of data for discriminative neural network training;
each one corresponds to part of a file, of variable (and configurable)
length.

Usage:  nnet-get-egs-discriminative [options] <model> <features-rspecifier> <ali-rspecifier> <den-lat-rspecifier> <training-examples-out>

An example [where $feats expands to the actual features]:
nnet-get-egs-discriminative --acoustic-scale=0.1 \\
  1.mdl '$feats' 'ark,s,cs:gunzip -c ali.1.gz|' 'ark,s,cs:gunzip -c lat.1.gz|' ark:1.degs </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-shuffle-egs-discriminative.cc "nnet-shuffle-egs-discriminative" </td><td> <pre> Copy examples (typically single frames) for neural network training,
from the input to output, but randomly shuffle the order.  This program will keep
all of the examples in memory at once, so don't give it too many.

Usage:  nnet-shuffle-egs-discriminative [options] <egs-rspecifier> <egs-wspecifier>

nnet-shuffle-egs-discriminative --srand=1 ark:train.degs ark:shuffled.degs </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-compare-hash-discriminative.cc "nnet-compare-hash-discriminative" </td><td> <pre> Compares two archives of discriminative training examples and checks
that they behave the same way for purposes of discriminative training.
This program was created as a way of testing nnet-get-egs-discriminative
The model is only needed for its transition-model.

Usage:  nnet-compare-hash-discriminative [options] <model-rxfilename> <egs-rspecifier1> <egs-rspecifier2>

Note: options --drop-frames and --criterion should be matched with the
command line of nnet-get-egs-discriminative used to get the examples
nnet-compare-hash-discriminative --drop-frames=true --criterion=mmi ark:1.degs ark:2.degs </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-combine-egs-discriminative.cc "nnet-combine-egs-discriminative" </td><td> <pre> Copy examples for discriminative neural network training,
and combine successive examples if their combined length will
be less than --max-length.  This can help to improve efficiency
(--max-length corresponds to minibatch size)

Usage:  nnet-combine-egs-discriminative [options] <egs-rspecifier> <egs-wspecifier>

e.g.
nnet-combine-egs-discriminative --max-length=512 ark:temp.1.degs ark:1.degs </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-train-discriminative-simple.cc "nnet-train-discriminative-simple" </td><td> <pre> Train the neural network parameters with a discriminative objective
function (MMI, SMBR or MPFE).  This uses training examples prepared with
nnet-get-egs-discriminative

Usage:  nnet-train-discriminative-simple [options] <model-in> <training-examples-in> <model-out>
e.g.:
nnet-train-discriminative-simple 1.nnet ark:1.degs 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-train-discriminative-parallel.cc "nnet-train-discriminative-parallel" </td><td> <pre> Train the neural network parameters with a discriminative objective
function (MMI, SMBR or MPFE).  This uses training examples prepared with
nnet-get-egs-discriminative
This version uses multiple threads (but no GPU)
Usage:  nnet-train-discriminative-parallel [options] <model-in> <training-examples-in> <model-out>
e.g.:
nnet-train-discriminative-parallel --num-threads=8 1.nnet ark:1.degs 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-modify-learning-rates.cc "nnet-modify-learning-rates" </td><td> <pre> This program modifies the learning rates so as to equalize the
relative changes in parameters for each layer, while keeping their
geometric mean the same (or changing it to a value specified using
the --average-learning-rate option).

Usage: nnet-modify-learning-rates [options] <prev-model> \\
                                  <cur-model> <modified-cur-model>
e.g.: nnet-modify-learning-rates --average-learning-rate=0.0002 \\
                                 5.mdl 6.mdl 6.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-normalize-stddev.cc "nnet-normalize-stddev" </td><td> <pre> This program first identifies any affine or block affine layers that
are followed by pnorm and then renormalize layers. Then it rescales
those layers such that the parameter stddev is 1.0 after scaling
(the target stddev is configurable by the --stddev option).
If you supply the option --stddev-from=<model-filename>, it rescales
those layers to match the standard deviations of corresponding layers
in the specified model.

Usage: nnet-normalize-stddev [options] <model-in> <model-out>
 e.g.: nnet-normalize-stddev final.mdl final.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-perturb-egs.cc "nnet-perturb-egs" </td><td> <pre> Copy examples, perturbing them by adding a specified amount (--noise-factor)
times the within-class covariance of the examples. the Cholesky factor of
the examples (obtained from the --write-cholesky option of
nnet-get-feature-transform) must be supplied.

Usage:  nnet-perturb-egs [options] <cholesky> <egs-rspecifier> <egs-wspecifier>

nnet-perturb-egs --noise-factor=0.2 exp/nnet5/cholesky.tpmat ark:- ark:- </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-perturb-egs-fmllr.cc "nnet-perturb-egs-fmllr" </td><td> <pre> Copy examples, perturbing them by multiplying by a randomly chosen fMLLR
transform from a fixed set.  The option --noise-factor interpolates the
un-transformed feature (times 1.0 - noise-factor) with the fMLLR feature
(times noise-factor)

Usage:  nnet-perturb-egs-fmllr [options] <fmllr-rspecifier> <egs-rspecifier> <egs-wspecifier>

nnet-perturb-egs-fmllr --noise-factor=0.2 'ark:cat exp/tri4_ali/trans.*|' ark:- ark:- </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-get-weighted-egs.cc "nnet-get-weighted-egs" </td><td> <pre> Get frame-by-frame examples of data for neural network training.
Essentially this is a format change from features and posteriors
into a special frame-by-frame format.  To split randomly into
different subsets, do nnet-copy-egs with --random=true, but
note that this does not randomize the order of frames.

Usage:  nnet-get-weighted-egs [options] <features-rspecifier> <pdf-post-rspecifier> <weights-rspecifier> <training-examples-out>

An example [where $feats expands to the actual features]:
nnet-get-weighted-egs --left-context=8 --right-context=8 "$feats" \\
  "ark:gunzip -c exp/nnet/ali.1.gz | ali-to-pdf exp/nnet/1.nnet ark:- ark:- | ali-to-post ark:- ark:- |" \\
   ark:- 
Note: the --left-context and --right-context would be derived from
the output of nnet-info. </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-adjust-priors.cc "nnet-adjust-priors" </td><td> <pre> Set the priors of the neural net to the computed posterios from the net,
on typical data (e.g. training data). This is correct under more general
circumstances than using the priors of the class labels in the training data

Typical usage of this program will involve computation of an average pdf-level
posterior with nnet-compute or nnet-compute-from-egs, piped into nnet-sum-rows
and then vector-sum, to compute the average posterior

Usage: nnet-adjust-priors [options] <nnet-in> <summed-posterior-vector-in> <nnet-out>
e.g.:
 nnet-adjust-priors final.mdl prior.vec final.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/cuda-compiled.cc "cuda-compiled" </td><td> <pre> This program returns exit status 0 (success) if the code
was compiled with CUDA support, and 1 otherwise.  To support CUDA, you
must run 'configure' on a machine that has the CUDA compiler 'nvcc'
available. </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-replace-last-layers.cc "nnet-replace-last-layers" </td><td> <pre> This program is for adding new layers to a neural-network acoustic model.
It removes the last --remove-layers layers, and adds the layers from the
supplied raw-nnet.  The typical use is to remove the last two layers
(the softmax, and the affine component before it), and add in replacements
for them newly initialized by nnet-init.  This program is a more flexible
way of adding layers than nnet-insert, but the inserted network needs to
contain replacements for the removed layers.

Usage:  nnet-replace-last-layers [options] <nnet-in> <raw-nnet-to-insert-in> <nnet-out>
e.g.:
 nnet-replace-last-layers 1.nnet "nnet-init hidden_layer.config -|" 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-switch-preconditioning.cc "nnet-am-switch-preconditioning" </td><td> <pre> Copy a (cpu-based) neural net and its associated transition model,
and switch it to online preconditioning, i.e. change any components
derived from AffineComponent to components of type
AffineComponentPreconditionedOnline.

Usage:  nnet-am-switch-preconditioning [options] <nnet-in> <nnet-out>
e.g.:
 nnet-am-switch-preconditioning --binary=false 1.mdl text.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-train-simple-perturbed.cc "nnet-train-simple-perturbed" </td><td> <pre> Train the neural network parameters with backprop and stochastic
gradient descent using minibatches.  As nnet-train-simple, but
perturbs the input features by going a certain distance down the
backprop-ed gradient.  Can be helpful for small datasets.

Usage:  nnet-train-simple-perturbed [options] <model-in> <training-examples-in> <model-out>
note: the option --within-covar=<file> is needed

e.g.:
nnet-train-simple-perturbed --within-covar=within.spmat --target-objf-change=0.2 \\
  1.nnet ark:1.1.egs 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-train-parallel-perturbed.cc "nnet-train-parallel-perturbed" </td><td> <pre> Train the neural network parameters with backprop and stochastic
gradient descent using minibatches.  As nnet-train-parallel but
perturbs the input features by going a certain distance down the
gradient obtained by backprop (can help for small datasets)

Usage:  nnet-train-parallel-perturbed [options] <model-in> <training-examples-in> <model-out>

e.g.:
nnet-train-parallel-pertured \\
 --within-covar=within.spmat --num-threads=8 --target-objf-change=0.2 1.nnet ark:1.egs 2.nnet </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet1-to-raw-nnet.cc "nnet1-to-raw-nnet" </td><td> <pre> Convert nnet1 neural net to nnet2 'raw' neural net

Usage:  nnet1-to-raw-nnet [options] <nnet1-in> <nnet2-out>
e.g.:
 nnet1-to-raw-nnet srcdir/final.nnet - | nnet-am-init dest/tree dest/topo - dest/0.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/raw-nnet-copy.cc "raw-nnet-copy" </td><td> <pre> Copy a raw neural net (this version works on raw nnet2 neural nets,
without the transition model.  Supports the 'truncate' option.

Usage:  raw-nnet-copy [options] <raw-nnet-in> <raw-nnet-out>
e.g.:
 raw-nnet-copy --binary=false 1.mdl text.mdl
See also: nnet-to-raw-nnet, nnet-am-copy </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-relabel-egs.cc "nnet-relabel-egs" </td><td> <pre> Relabel neural network egs with the read pdf-id alignments, zero-based..
Usage: nnet-relabel-egs [options] <pdf-aligment-rspecifier> <egs_rspecifier1> ... <egs_rspecifierN> <egs_wspecifier1> ... <egs_wspecifierN>
e.g.: 
 nnet-relabel-egs ark:1.ali egs_in/egs.1.ark egs_in/egs.2.ark egs_out/egs.1.ark egs_out/egs.2.ark
See also: nnet-get-egs, nnet-copy-egs, steps/nnet2/relabel_egs.sh </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet-am-reinitialize.cc "nnet-am-reinitialize" </td><td> <pre> This program can used when transferring a neural net from one language
to another (or one tree to another).  It takes a neural net and a
transition model from a different neural net, resizes the last layer
to match the new transition model, zeroes it, and writes out the new,
resized .mdl file.  If the original model had been 'mixed-up', the associated
SumGroupComponent will be removed.

Usage:  nnet-am-reinitialize [options] <nnet-in> <new-transition-model> <nnet-out>
e.g.:
 nnet-am-reinitialize 1.mdl exp/tri6/final.mdl 2.mdl </pre> </td> </tr>
<tr> <td> \ref nnet2bin/nnet2-boost-silence.cc "nnet2-boost-silence" </td><td> <pre> This program can be used to change the acoustic probabilities associated 
with a certain set of phones by a given factor, for nnet2 models. 
Can be useful to control the amount of silence, noise, and so on. 
It is implemented by dividing the corresponding priors by that
factor (since we divide by the prior when we evaluate likelihoods).

Usage:
 nnet2-boost-silence [options] <silence-phones-list> <model-in> <model-out>
e.g.:   nnet2-boost-silence --boost=0.2 1:2:3 final.mdl final_boost.mdl
See also: gmm-boost-silence </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-train-frmshuff.cc "nnet-train-frmshuff" </td><td> <pre> Perform one iteration of Neural Network training by mini-batch Stochastic Gradient Descent.
This version use pdf-posterior as targets, prepared typically by ali-to-post.
Usage:  nnet-train-frmshuff [options] <feature-rspecifier> <targets-rspecifier> <model-in> [<model-out>]
e.g.: 
 nnet-train-frmshuff scp:feature.scp ark:posterior.ark nnet.init nnet.iter1 </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-train-perutt.cc "nnet-train-perutt" </td><td> <pre> Perform one iteration of Neural Network training by Stochastic Gradient Descent.
This version use pdf-posterior as targets, prepared typically by ali-to-post.
The updates are done per-utternace, shuffling options are dummy for compatibility reason.

Usage:  nnet-train-perutt [options] <feature-rspecifier> <targets-rspecifier> <model-in> [<model-out>]
e.g.: 
 nnet-train-perutt scp:feature.scp ark:posterior.ark nnet.init nnet.iter1 </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-train-mmi-sequential.cc "nnet-train-mmi-sequential" </td><td> <pre> Perform one iteration of DNN-MMI training by stochastic gradient descent.
The network weights are updated on each utterance.
Usage:  nnet-train-mmi-sequential [options] <model-in> <transition-model-in> <feature-rspecifier> <den-lat-rspecifier> <ali-rspecifier> [<model-out>]
e.g.: 
 nnet-train-mmi-sequential nnet.init trans.mdl scp:train.scp scp:denlats.scp ark:train.ali nnet.iter1 </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-train-mpe-sequential.cc "nnet-train-mpe-sequential" </td><td> <pre> Perform iteration of Neural Network MPE/sMBR training by stochastic gradient descent.
The network weights are updated on each utterance.
Usage:  nnet-train-mpe-sequential [options] <model-in> <transition-model-in> <feature-rspecifier> <den-lat-rspecifier> <ali-rspecifier> [<model-out>]
e.g.: 
 nnet-train-mpe-sequential nnet.init trans.mdl scp:train.scp scp:denlats.scp ark:train.ali nnet.iter1 </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-train-lstm-streams.cc "nnet-train-lstm-streams" </td><td> <pre> Perform one iteration of LSTM training by Stochastic Gradient Descent.
This version use pdf-posterior as targets, prepared typically by ali-to-post.
The updates are done per-utternace, shuffling options are dummy for compatibility reason.

Usage:  bd-nnet-train-lstm-streams [options] <feature-rspecifier> <targets-rspecifier> <model-in> [<model-out>]
e.g.: 
 bd-nnet-train-lstm-streams scp:feature.scp ark:posterior.ark nnet.init nnet.iter1 </pre> </td> </tr>
<tr> <td> \ref nnetbin/rbm-train-cd1-frmshuff.cc "rbm-train-cd1-frmshuff" </td><td> <pre> Train RBM by Contrastive Divergence alg. with 1 step of Markov Chain Monte-Carlo.
The tool can perform several iterations (--num-iters) or it can subsample the training dataset (--drop-data)
Usage:  rbm-train-cd1-frmshuff [options] <model-in> <feature-rspecifier> <model-out>
e.g.: 
 rbm-train-cd1-frmshuff 1.rbm.init scp:train.scp 1.rbm </pre> </td> </tr>
<tr> <td> \ref nnetbin/rbm-convert-to-nnet.cc "rbm-convert-to-nnet" </td><td> <pre> Convert RBM to <affinetransform> and <sigmoid>
Usage:  rbm-convert-to-nnet [options] <rbm-in> <nnet-out>
e.g.:
 rbm-convert-to-nnet --binary=false rbm.mdl nnet.mdl </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-forward.cc "nnet-forward" </td><td> <pre> Perform forward pass through Neural Network.

Usage:  nnet-forward [options] <model-in> <feature-rspecifier> <feature-wspecifier>
e.g.: 
 nnet-forward nnet ark:features.ark ark:mlpoutput.ark </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-copy.cc "nnet-copy" </td><td> <pre> Copy Neural Network model (and possibly change binary/text format)
Usage:  nnet-copy [options] <model-in> <model-out>
e.g.:
 nnet-copy --binary=false nnet.mdl nnet_txt.mdl </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-info.cc "nnet-info" </td><td> <pre> Print human-readable information about the neural network
acoustic model to the standard output
Usage:  nnet-info [options] <nnet-in>
e.g.:
 nnet-info 1.nnet </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-concat.cc "nnet-concat" </td><td> <pre> Concatenate Neural Networks (and possibly change binary/text format)
Usage:  nnet-concat [options] <model-in1> <...> <model-inN> <model-out>
e.g.:
 nnet-concat --binary=false nnet.1 nnet.2 nnet.1.2 </pre> </td> </tr>
<tr> <td> \ref nnetbin/transf-to-nnet.cc "transf-to-nnet" </td><td> <pre> Convert transformation matrix to <affine-transform>
Usage:  transf-to-nnet [options] <transf-in> <nnet-out>
e.g.:
 transf-to-nnet --binary=false transf.mat nnet.mdl </pre> </td> </tr>
<tr> <td> \ref nnetbin/cmvn-to-nnet.cc "cmvn-to-nnet" </td><td> <pre> Convert cmvn-stats into <AddShift> and <Rescale> components.
Usage:  cmvn-to-nnet [options] <transf-in> <nnet-out>
e.g.:
 cmvn-to-nnet --binary=false transf.mat nnet.mdl </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-initialize.cc "nnet-initialize" </td><td> <pre> Copy Neural Network model (and possibly change binary/text format)
Usage:  nnet-initialize [options] <nnet-config-in> <nnet-out>
e.g.:
 nnet-copy --binary=false nnet.conf nnet.init </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-kl-hmm-acc.cc "nnet-kl-hmm-acc" </td><td> <pre> Collect the statistics for the Kl-HMM trainign.
Usage:  nnet-kl-hmm-acc [options] <feature-rspecifier> <alignments-rspecifier> <kl-hmm-accumulator>
e.g.: 
 nnet-kl-hmm-acc scp:train.scp ark:train.ali kl-hmm.acc </pre> </td> </tr>
<tr> <td> \ref nnetbin/nnet-kl-hmm-mat-to-component.cc "nnet-kl-hmm-mat-to-component" </td><td> <pre> Convert matrix of KL-HMM training to nnet component.
Usage: nnet-kl-hmm-mat-to-component [options] nnet-component matrix </pre> </td> </tr>
<tr> <td> \ref nnetbin/feat-to-post.cc "feat-to-post" </td><td> <pre> Convert features into posterior format, is used as NN training targets (Karel's nnet1).
(speed is not an issue for reasonably low NN-output dimensions)
Usage:  feat-to-post [options] feat-rspecifier posteriors-wspecifier
e.g.:
 feat-to-post scp:feats.scp ark:feats.post </pre> </td> </tr>
<tr> <td> \ref nnetbin/paste-post.cc "paste-post" </td><td> <pre> paste-post : paste N posterior streams.
Useful for multi-task or multi-lingual DNN trainig.
Usage: paste-post featlen-rspecifier dims-csl post1-rspecifier ... postN-rspecifier post-wspecifier
e.g.:
 post-concat featlen.ark N1:N2:N3 ark:post1.ark ... ark:postN.ark ark:pasted.ark </pre> </td> </tr>
<tr> <td> \ref nnetbin/train-transitions.cc "train-transitions" </td><td> <pre> Train the transition probabilities in transition-model (used in nnet1 recipe)

Usage:  train-transitions [options] <trans-model-in> <alignments-rspecifier> <trans-model-out>
e.g.:
 train-transitions 1.mdl "ark:gunzip -c ali.*.gz|" 2.mdl </pre> </td> </tr>
<tr> <td> \ref online2bin/online2-wav-gmm-latgen-faster.cc "online2-wav-gmm-latgen-faster" </td><td> <pre> Reads in wav file(s) and simulates online decoding, including
basis-fMLLR adaptation and endpointing.  Writes lattices.
Models are specified via options.

Usage: online2-wav-gmm-latgen-faster [options] <fst-in> <spk2utt-rspecifier> <wav-rspecifier> <lattice-wspecifier>
Run egs/rm/s5/local/run_online_decoding.sh for example </pre> </td> </tr>
<tr> <td> \ref online2bin/apply-cmvn-online.cc "apply-cmvn-online" </td><td> <pre> Apply online cepstral mean (and possibly variance) computation online,
using the same code as used for online decoding in the 'new' setup in
online2/ and online2bin/.  If the --spk2utt option is used, it uses
prior utterances from the same speaker to back off to at the utterance
beginning.  See also apply-cmvn-sliding.

Usage: apply-cmvn-online [options] <global-cmvn-stats> <feature-rspecifier> <feature-wspecifier>
e.g. apply-cmvn-online 'matrix-sum scp:data/train/cmvn.scp -|' data/train/split8/1/feats.scp ark:-
or: apply-cmvn-online --spk2utt=ark:data/train/split8/1/spk2utt 'matrix-sum scp:data/train/cmvn.scp -|'  data/train/split8/1/feats.scp ark:- </pre> </td> </tr>
<tr> <td> \ref online2bin/extend-wav-with-silence.cc "extend-wav-with-silence" </td><td> <pre> Extend wave data with a fairly long silence at the end (e.g. 5 seconds).
The input waveforms are assumed having silences at the begin/end and those
segments are extracted and appended to the end of the utterance.
Note this is for use in testing endpointing in decoding.

Usage: extend-wav-with-silence [options] <wav-rspecifier> <wav-wspecifier> </pre> </td> </tr>
<tr> <td> \ref online2bin/compress-uncompress-speex.cc "compress-uncompress-speex" </td><td> <pre> Demonstrating how to use the Speex wrapper in Kaldi by compressing input waveforms 
chunk by chunk and then decompressing them.

Usage: compress-uncompress-speex [options] <wav-rspecifier> <wav-wspecifier> </pre> </td> </tr>
<tr> <td> \ref online2bin/online2-wav-nnet2-latgen-faster.cc "online2-wav-nnet2-latgen-faster" </td><td> <pre> Reads in wav file(s) and simulates online decoding with neural nets
(nnet2 setup), with optional iVector-based speaker adaptation and
optional endpointing.  Note: some configuration values and inputs are
set via config files whose filenames are passed as options

Usage: online2-wav-nnet2-latgen-faster [options] <nnet2-in> <fst-in> <spk2utt-rspecifier> <wav-rspecifier> <lattice-wspecifier>
The spk2utt-rspecifier can just be <utterance-id> <utterance-id> if
you want to decode utterance by utterance.
See egs/rm/s5/local/run_online_decoding_nnet2.sh for example
See also online2-wav-nnet2-latgen-threaded </pre> </td> </tr>
<tr> <td> \ref online2bin/ivector-extract-online2.cc "ivector-extract-online2" </td><td> <pre> Extract iVectors for utterances every --ivector-period frames, using a trained
iVector extractor and features and Gaussian-level posteriors.  Similar to
ivector-extract-online but uses the actual online decoder code to do it,
and does everything in-memory instead of using multiple processes.
Note: the value of the --use-most-recent-ivector config variable is ignored
it's set to false.  The <spk2utt-rspecifier> is mandatory, to simplify the code;
if you want to do it separately per utterance, just make it of the form
<utterance-id> <utterance-id>.
The iVectors are output as an archive of matrices, indexed by utterance-id;
each row corresponds to an iVector.  If --repeat=true, outputs the whole matrix
of iVectors, not just every (ivector-period)'th frame
The input features are the raw, non-cepstral-mean-normalized features, e.g. MFCC.

Usage:  ivector-extract-online2 [options] <spk2utt-rspecifier> <feature-rspecifier> <ivector-wspecifier>
e.g.: 
  ivector-extract-online2 --config=exp/nnet2_online/nnet_online/conf/ivector_extractor.conf \\
    ark:data/train/spk2utt scp:data/train/feats.scp ark,t:ivectors.1.ark </pre> </td> </tr>
<tr> <td> \ref online2bin/online2-wav-dump-features.cc "online2-wav-dump-features" </td><td> <pre> Reads in wav file(s) and processes them as in online2-wav-nnet2-latgen-faster,
but instead of decoding, dumps the features.  Most of the parameters
are set via configuration variables.

Usage: online2-wav-dump-features [options] <spk2utt-rspecifier> <wav-rspecifier> <feature-wspecifier>
The spk2utt-rspecifier can just be <utterance-id> <utterance-id> if
you want to generate features utterance by utterance.
Alternate usage: online2-wav-dump-features [options] --print-ivector-dim=true
See steps/online/nnet2/{dump_nnet_activations,get_egs.sh} for examples. </pre> </td> </tr>
<tr> <td> \ref online2bin/ivector-randomize.cc "ivector-randomize" </td><td> <pre> Copy matrices of online-estimated iVectors, but randomize them;
this is intended primarily for training the online nnet2 setup
with iVectors.  For each input matrix, each row with index t is,
with probability given by the option --randomize-prob, replaced
with the contents an input row chosen randomly from the interval [t, T]
where T is the index of the last row of the matrix.

Usage: ivector-randomize [options] <ivector-rspecifier> <ivector-wspecifier>
 e.g.: ivector-randomize ark:- ark:-
See also: ivector-extract-online, ivector-extract-online2, subsample-feats </pre> </td> </tr>
<tr> <td> \ref online2bin/online2-wav-nnet2-am-compute.cc "online2-wav-nnet2-am-compute" </td><td> <pre> Simulates the online neural net computation for each file of input
features, and outputs as a matrix the result, with optional
iVector-based speaker adaptation. Note: some configuration values
and inputs are set via config files whose filenames are passed as
options.  Used mostly for debugging.
Note: if you want it to apply a log (e.g. for log-likelihoods), use
--apply-log=true.

Usage:  online2-wav-nnet2-am-compute [options] <nnet-in>
<spk2utt-rspecifier> <wav-rspecifier> <feature-or-loglikes-wspecifier>
The spk2utt-rspecifier can just be <utterance-id> <utterance-id> if
you want to compute utterance by utterance. </pre> </td> </tr>
<tr> <td> \ref online2bin/online2-wav-nnet2-latgen-threaded.cc "online2-wav-nnet2-latgen-threaded" </td><td> <pre> Reads in wav file(s) and simulates online decoding with neural nets
(nnet2 setup), with optional iVector-based speaker adaptation and
optional endpointing.  This version uses multiple threads for decoding.
Note: some configuration values and inputs are set via config files
whose filenames are passed as options

Usage: online2-wav-nnet2-latgen-threaded [options] <nnet2-in> <fst-in> <spk2utt-rspecifier> <wav-rspecifier> <lattice-wspecifier>
The spk2utt-rspecifier can just be <utterance-id> <utterance-id> if
you want to decode utterance by utterance.
See egs/rm/s5/local/run_online_decoding_nnet2.sh for example
See also online2-wav-nnet2-latgen-faster </pre> </td> </tr>
<tr> <td> \ref onlinebin/online-net-client.cc "online-net-client" </td><td> <pre> Takes input using a microphone(PortAudio), extracts features and sends them
to a speech recognition server over a network connection

Usage: online-net-client server-address server-port </pre> </td> </tr>
<tr> <td> \ref onlinebin/online-server-gmm-decode-faster.cc "online-server-gmm-decode-faster" </td><td> <pre> Decode speech, using feature batches received over a network connection

Utterance segmentation is done on-the-fly.
Feature splicing/LDA transform is used, if the optional(last) argument is given.
Otherwise delta/delta-delta(2-nd order) features are produced.

Usage: online-server-gmm-decode-faster [options] model-infst-in word-symbol-table silence-phones udp-port [lda-matrix-in]

Example: online-server-gmm-decode-faster --rt-min=0.3 --rt-max=0.5 --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 model HCLG.fst words.txt '1:2:3:4:5' 1234 lda-matrix </pre> </td> </tr>
<tr> <td> \ref onlinebin/online-gmm-decode-faster.cc "online-gmm-decode-faster" </td><td> <pre> Decode speech, using microphone input(PortAudio)

Utterance segmentation is done on-the-fly.
Feature splicing/LDA transform is used, if the optional(last) argument is given.
Otherwise delta/delta-delta(2-nd order) features are produced.

Usage: online-gmm-decode-faster [options] <model-in><fst-in> <word-symbol-table> <silence-phones> [<lda-matrix-in>]

Example: online-gmm-decode-faster --rt-min=0.3 --rt-max=0.5 --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 model HCLG.fst words.txt '1:2:3:4:5' lda-matrix </pre> </td> </tr>
<tr> <td> \ref onlinebin/online-wav-gmm-decode-faster.cc "online-wav-gmm-decode-faster" </td><td> <pre> Reads in wav file(s) and simulates online decoding.
Writes .tra and .ali files for WER computation. Utterance segmentation is done on-the-fly.
Feature splicing/LDA transform is used, if the optional(last) argument is given.
Otherwise delta/delta-delta(i.e. 2-nd order) features are produced.
Caution: the last few frames of the wav file may not be decoded properly.
Hence, don't use one wav file per utterance, but rather use one wav file per show.

Usage: online-wav-gmm-decode-faster [options] wav-rspecifier model-infst-in word-symbol-table silence-phones transcript-wspecifier alignments-wspecifier [lda-matrix-in]

Example: ./online-wav-gmm-decode-faster --rt-min=0.3 --rt-max=0.5 --max-active=4000 --beam=12.0 --acoustic-scale=0.0769 scp:wav.scp model HCLG.fst words.txt '1:2:3:4:5' ark,t:trans.txt ark,t:ali.txt </pre> </td> </tr>
<tr> <td> \ref onlinebin/online-audio-server-decode-faster.cc "online-audio-server-decode-faster" </td><td> <pre> Starts a TCP server that receives RAW audio and outputs aligned words.
A sample client can be found in: onlinebin/online-audio-client

Usage: ./online-audio-server-decode-faster [options] model-in fst-in word-symbol-table silence-phones word_boundary_file tcp-port [lda-matrix-in]

example: online-audio-server-decode-faster --verbose=1 --rt-min=0.5 --rt-max=3.0 --max-active=6000
--beam=72.0 --acoustic-scale=0.0769 final.mdl graph/HCLG.fst graph/words.txt '1:2:3:4:5'
graph/word_boundary.int 5000 final.mat </pre> </td> </tr>
<tr> <td> \ref onlinebin/online-audio-client.cc "online-audio-client" </td><td> <pre> Sends an audio file to the KALDI audio server (onlinebin/online-audio-server-decode-faster)
and prints the result optionally saving it to an HTK label file or WebVTT subtitles file

e.g.: ./online-audio-client 192.168.50.12 9012 'scp:wav_files.scp' </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-init.cc "sgmm2-init" </td><td> <pre> Initialize an SGMM from a trained full-covariance UBM and a specified model topology.
Usage: sgmm2-init [options] <topology> <tree> <init-model> <sgmm-out>
The <init-model> argument can be a UBM (the default case) or another
SGMM (if the --init-from-sgmm flag is used).
For systems with two-level tree, use --pdf-map argument. </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-gselect.cc "sgmm2-gselect" </td><td> <pre> Precompute Gaussian indices for SGMM training Usage: sgmm2-gselect [options] <model-in> <feature-rspecifier> <gselect-wspecifier>
e.g.: sgmm2-gselect 1.sgmm "ark:feature-command |" ark:1.gs
Note: you can do the same thing by combining the programs sgmm2-write-ubm, fgmm-global-to-gmm,
gmm-gselect and fgmm-gselect </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-acc-stats.cc "sgmm2-acc-stats" </td><td> <pre> Accumulate stats for SGMM training.
Usage: sgmm2-acc-stats [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <stats-out>
e.g.: sgmm2-acc-stats --gselect=ark:gselect.ark 1.mdl 1.ali scp:train.scp 'ark:ali-to-post 1.ali ark:-|' 1.acc
(note: gselect option is mandatory) </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-est.cc "sgmm2-est" </td><td> <pre> Estimate SGMM model parameters from accumulated stats.
Usage: sgmm2-est [options] <model-in> <stats-in> <model-out> </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-sum-accs.cc "sgmm2-sum-accs" </td><td> <pre> Sum multiple accumulated stats files for SGMM training.
Usage: sgmm2-sum-accs [options] stats-out stats-in1 stats-in2 ... </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-align-compiled.cc "sgmm2-align-compiled" </td><td> <pre> Align features given [SGMM-based] models.
Usage: sgmm2-align-compiled [options] model-in graphs-rspecifier feature-rspecifier alignments-wspecifier
e.g.: sgmm2-align-compiled 1.mdl ark:graphs.fsts scp:train.scp ark:1.ali </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-est-spkvecs.cc "sgmm2-est-spkvecs" </td><td> <pre> Estimate SGMM speaker vectors, either per utterance or for the supplied set of speakers (with spk2utt option).
Reads Gaussian-level posteriors. Writes to a table of vectors.
Usage: sgmm2-est-spkvecs [options] <model-in> <feature-rspecifier> <post-rspecifier> <vecs-wspecifier>
note: --gselect option is required. </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-post-to-gpost.cc "sgmm2-post-to-gpost" </td><td> <pre> Convert posteriors to Gaussian-level posteriors for SGMM training.
Usage: sgmm2-post-to-gpost [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <gpost-wspecifier>
e.g.: sgmm2-post-to-gpost 1.mdl 1.ali scp:train.scp 'ark:ali-to-post ark:1.ali ark:-|' ark:- </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-acc-stats-gpost.cc "sgmm2-acc-stats-gpost" </td><td> <pre> Accumulate stats for SGMM training, given Gaussian-level posteriors
Usage: sgmm2-acc-stats-gpost [options] <model-in> <feature-rspecifier> <gpost-rspecifier> <stats-out>
e.g.: sgmm2-acc-stats-gpost 1.mdl 1.ali scp:train.scp ark, s, cs:- 1.acc </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-latgen-faster.cc "sgmm2-latgen-faster" </td><td> <pre> Decode features using SGMM-based model.
Usage:  sgmm2-latgen-faster [options] <model-in> (<fst-in>|<fsts-rspecifier>) <features-rspecifier> <lattices-wspecifier> [<words-wspecifier> [<alignments-wspecifier>] ] </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-est-spkvecs-gpost.cc "sgmm2-est-spkvecs-gpost" </td><td> <pre> Estimate SGMM speaker vectors, either per utterance or for the supplied set of speakers (with spk2utt option).
Reads Gaussian-level posteriors. Writes to a table of vectors.
Usage: sgmm2-est-spkvecs-gpost [options] <model-in> <feature-rspecifier> <gpost-rspecifier> <vecs-wspecifier> </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-rescore-lattice.cc "sgmm2-rescore-lattice" </td><td> <pre> Replace the acoustic scores on a lattice using a new model.
Usage: sgmm2-rescore-lattice [options] <model-in> <lattice-rspecifier> <feature-rspecifier> <lattice-wspecifier>
 e.g.: sgmm2-rescore-lattice 1.mdl ark:1.lats scp:trn.scp ark:2.lats </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-copy.cc "sgmm2-copy" </td><td> <pre> Copy SGMM (possibly changing binary/text format)
Usage: sgmm2-copy [options] <model-in> <model-out>
e.g.: sgmm2-copy --binary=false 1.mdl 1_text.mdl </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-info.cc "sgmm2-info" </td><td> <pre> Print various information about an SGMM.
Usage: sgmm2-info [options] <model-in> [model-in2 ... ] </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-est-ebw.cc "sgmm2-est-ebw" </td><td> <pre> Estimate SGMM model parameters discriminatively using Extended
Baum-Welch style of update
Usage: sgmm2-est-ebw [options] <model-in> <num-stats-in> <den-stats-in> <model-out> </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-acc-stats2.cc "sgmm2-acc-stats2" </td><td> <pre> Accumulate numerator and denominator stats for discriminative training
of SGMMs (input is posteriors of mixed sign)
Usage: sgmm2-acc-stats2 [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <num-stats-out> <den-stats-out>
e.g.: sgmm2-acc-stats2 1.mdl 1.ali scp:train.scp ark:1.posts num.acc den.acc </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-comp-prexform.cc "sgmm2-comp-prexform" </td><td> <pre> Compute "pre-transform" parameters required for estimating fMLLR with
SGMMs, and write to a model file, after the SGMM.
Usage: sgmm2-comp-prexform [options] <sgmm2-in> <occs-in> <sgmm-out> </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-est-fmllr.cc "sgmm2-est-fmllr" </td><td> <pre> Estimate FMLLR transform for SGMMs, either per utterance or for the supplied set of speakers (with spk2utt option).
Reads state-level posteriors. Writes to a table of matrices.
--gselect option is mandatory.
Usage: sgmm2-est-fmllr [options] <model-in> <feature-rspecifier> <post-rspecifier> <mats-wspecifier> </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-project.cc "sgmm2-project" </td><td> <pre> Compute SGMM model projection that only models a part of a pre-LDA space.
Used in predictive SGMMs.  Takes as input an LDA+MLLT transform,
and outputs a transform from the pre-LDA+MLLT space to the space that
we want to model
Usage: sgmm2-project [options] <model-in> <lda-mllt-mat-in> <model-out> <new-projection-out>
e.g.: sgmm2-project --start-dim=0 --end-dim=52 final.mdl final.inv_full_mat final_proj1.mdl proj1.mat </pre> </td> </tr>
<tr> <td> \ref sgmm2bin/sgmm2-latgen-faster-parallel.cc "sgmm2-latgen-faster-parallel" </td><td> <pre> Decode features using SGMM-based model.  This version accepts the --num-threads
option but otherwise behaves identically to sgmm2-latgen-faster
Usage:  sgmm2-latgen-faster-parallel [options] <model-in> (<fst-in>|<fsts-rspecifier>) <features-rspecifier> <lattices-wspecifier> [<words-wspecifier> [<alignments-wspecifier>] ] </pre> </td> </tr>
<tr> <td> \ref sgmmbin/init-ubm.cc "init-ubm" </td><td> <pre> Cluster the Gaussians in a diagonal-GMM acoustic model
to a single full-covariance or diagonal-covariance GMM.
Usage: init-ubm [options] <model-file> <state-occs> <gmm-out> </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-align-compiled.cc "sgmm-align-compiled" </td><td> <pre> Align features given [SGMM-based] models.
Usage: sgmm-align-compiled [options] model-in graphs-rspecifier feature-rspecifier alignments-wspecifier
e.g.: sgmm-align-compiled 1.mdl ark:graphs.fsts scp:train.scp ark:1.ali </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-acc-stats-ali.cc "sgmm-acc-stats-ali" </td><td> <pre> Accumulate stats for SGMM training.
Usage: sgmm-acc-stats-ali [options] <model-in> <feature-rspecifier> <alignments-rspecifier> <stats-out>
e.g.: sgmm-acc-stats-ali 1.mdl 1.ali scp:train.scp ark:1.ali 1.acc </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-sum-accs.cc "sgmm-sum-accs" </td><td> <pre> Sum multiple accumulated stats files for SGMM training.
Usage: sgmm-sum-accs [options] stats-out stats-in1 stats-in2 ... </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-est.cc "sgmm-est" </td><td> <pre> Estimate SGMM model parameters from accumulated stats.
Usage: sgmm-est [options] <model-in> <stats-in> <model-out> </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-decode-faster.cc "sgmm-decode-faster" </td><td> <pre> Decode features using SGMM-based model.
Usage:  sgmm-decode-faster [options] <model-in> <fst-in> <features-rspecifier> <words-wspecifier> [alignments-wspecifier] </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-init.cc "sgmm-init" </td><td> <pre> Initialize an SGMM from a trained full-covariance UBM and a specified model topology.
Usage: sgmm-init [options] <topology> <tree> <init-model> <sgmm-out>
The <init-model> argument can be a UBM (the default case) or another
SGMM (if the --init-from-sgmm flag is used). </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-gselect.cc "sgmm-gselect" </td><td> <pre> Precompute Gaussian indices for SGMM training Usage: sgmm-gselect [options] <model-in> <feature-rspecifier> <gselect-wspecifier>
e.g.: sgmm-gselect 1.sgmm "ark:feature-command |" ark:1.gs
Note: you can do the same thing by combining the programs sgmm-write-ubm, fgmm-global-to-gmm,
gmm-gselect and fgmm-gselect </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-est-fmllr.cc "sgmm-est-fmllr" </td><td> <pre> Estimate FMLLR transform for SGMMs, either per utterance or for the supplied set of speakers (with spk2utt option).
Reads state-level posteriors. Writes to a table of matrices.
Usage: sgmm-est-fmllr [options] <model-in> <feature-rspecifier> <post-rspecifier> <mats-wspecifier> </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-acc-stats.cc "sgmm-acc-stats" </td><td> <pre> Accumulate stats for SGMM training.
Usage: sgmm-acc-stats [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <stats-out>
e.g.: sgmm-acc-stats 1.mdl 1.ali scp:train.scp 'ark:ali-to-post 1.ali ark:-|' 1.acc </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-est-spkvecs.cc "sgmm-est-spkvecs" </td><td> <pre> Estimate SGMM speaker vectors, either per utterance or for the supplied set of speakers (with spk2utt option).
Reads Gaussian-level posteriors. Writes to a table of vectors.
Usage: sgmm-est-spkvecs [options] <model-in> <feature-rspecifier> <post-rspecifier> <vecs-wspecifier> </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-post-to-gpost.cc "sgmm-post-to-gpost" </td><td> <pre> Convert posteriors to Gaussian-level posteriors for SGMM training.
Usage: sgmm-post-to-gpost [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <gpost-wspecifier>
e.g.: sgmm-post-to-gpost 1.mdl 1.ali scp:train.scp 'ark:ali-to-post ark:1.ali ark:-|' ark:- </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-acc-stats-gpost.cc "sgmm-acc-stats-gpost" </td><td> <pre> Accumulate stats for SGMM training, given Gaussian-level posteriors
Usage: sgmm-acc-stats-gpost [options] <model-in> <feature-rspecifier> <gpost-rspecifier> <stats-out>
e.g.: sgmm-acc-stats-gpost 1.mdl 1.ali scp:train.scp ark, s, cs:- 1.acc </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-est-spkvecs-gpost.cc "sgmm-est-spkvecs-gpost" </td><td> <pre> Estimate SGMM speaker vectors, either per utterance or for the supplied set of speakers (with spk2utt option).
Reads Gaussian-level posteriors. Writes to a table of vectors.
Usage: sgmm-est-spkvecs-gpost [options] <model-in> <feature-rspecifier> <gpost-rspecifier> <vecs-wspecifier> </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-comp-prexform.cc "sgmm-comp-prexform" </td><td> <pre> Compute "pre-transform" parameters required for estimating fMLLR with
SGMMs, and write to a model file, after the SGMM.
Usage: sgmm-comp-prexform [options] <sgmm-in> <occs-in> <sgmm-out> </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-est-fmllr-gpost.cc "sgmm-est-fmllr-gpost" </td><td> <pre> Estimate FMLLR transform for SGMMs, either per utterance or for the supplied set of speakers (with spk2utt option).
Reads Gaussian-level posteriors. Writes to a table of matrices.
Usage: sgmm-est-fmllr-gpost [options] <model-in> <feature-rspecifier> <gpost-rspecifier> <mats-wspecifier> </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-acc-fmllrbasis-ali.cc "sgmm-acc-fmllrbasis-ali" </td><td> <pre> Accumulate stats for FMLLR bases training.
Usage: sgmm-acc-fmllrbasis-ali [options] <model-in> <feature-rspecifier> <alignments-rspecifier> <spk2utt-rspecifier> <stats-out>
e.g.: sgmm-acc-fmllrbasis-ali 1.mdl scp:train.scp ark:1.ali 1.acc </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-est-fmllrbasis.cc "sgmm-est-fmllrbasis" </td><td> <pre> Sum multiple accumulated stats files for SGMM training.
Usage: sgmm-est-fmllrbasis [options] <model-in> <model-out> <stats-in1> [stats-in2 ...] </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-calc-distances.cc "sgmm-calc-distances" </td><td> <pre> Compute matrix of approximated K-L divergences between states
Only works properly if a single substate per state.
Usage: sgmm-calc-distances [options] model-in occs-in distances-out </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-normalize.cc "sgmm-normalize" </td><td> <pre> Renormalize SGMM so that within certain subsets of UBM Gaussians (typically 
corresponding to gender), probabilities sum to one; write it out, including
normalizers.Note: gaussians-rspecifier will normally be "ark:foo" where foo looks like
  m  0 1 2 3 4 5
  f  6 7 8 9 10
Usage: sgmm-normalize [options] <model-in> <gaussians-rspecifier> <model-out> </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-latgen-simple.cc "sgmm-latgen-simple" </td><td> <pre> Decode features using SGMM-based model.
Usage:  sgmm-latgen-simple [options] <model-in> <fst-in> <features-rspecifier> <lattices-wspecifier> [<words-wspecifier> [<alignments-wspecifier>] ] </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-latgen-faster.cc "sgmm-latgen-faster" </td><td> <pre> Decode features using SGMM-based model.
Usage:  sgmm-latgen-faster [options] <model-in> (<fst-in>|<fsts-rspecifier>) <features-rspecifier> <lattices-wspecifier> [<words-wspecifier> [<alignments-wspecifier>] ] </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-rescore-lattice.cc "sgmm-rescore-lattice" </td><td> <pre> Replace the acoustic scores on a lattice using a new model.
Usage: sgmm-rescore-lattice [options] <model-in> <lattice-rspecifier> <feature-rspecifier> <lattice-wspecifier>
 e.g.: sgmm-rescore-lattice 1.mdl ark:1.lats scp:trn.scp ark:2.lats </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-copy.cc "sgmm-copy" </td><td> <pre> Copy SGMM (possibly changing binary/text format)
Usage: sgmm-copy [options] <model-in> <model-out>
e.g.: sgmm-copy --binary=false 1.mdl 1_text.mdl </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-write-ubm.cc "sgmm-write-ubm" </td><td> <pre> Write out the full-covariance UBM of the SGMM
Usage: sgmm-write-ubm [options] <model-in> <ubm-out>
e.g.: sgmm-write-ubm 1.mdl 1.ubm </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-mixup.cc "sgmm-mixup" </td><td> <pre> Increase number of sub-states or dimensions in SGMM
Usage: sgmm-mixup [options] <model-in> <model-out>
E.g. of mixing up:
 sgmm-mixup --read-occs=1.occs --num-substates=10000 1.mdl 2.mdl
E.g. of increasing phonetic dim:
 sgmm-mixup --increase-phn-dim=50 1.mdl 2.mdl
E.g. of increasing speaker dim:
 sgmm-mixup --increase-spk-dim=50 1.mdl 2.mdl
E.g. of removing speaker space:
 sgmm-mixup --remove-speaker-space 1.mdl 2.mdl
These modes may be combined. </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-info.cc "sgmm-info" </td><td> <pre> Print various information about an SGMM.
Usage: sgmm-info [options] <model-in> [model-in2 ... ] </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-acc-tree-stats.cc "sgmm-acc-tree-stats" </td><td> <pre> Accumulate statistics for decision tree training.
This version accumulates statistics in the form of state-specific SGMM stats; you need to use the program sgmm-build-tree to build the tree (and sgmm-sum-tree-accs to sum the stats).
Usage:  sgmm-acc-tree-stats [options] sgmm-model-in features-rspecifier alignments-rspecifier [tree-accs-out]
e.g.: sgmm-acc-tree-stats --ci-phones=48:49 1.mdl scp:train.scp ark:1.ali 1.tacc </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-sum-tree-stats.cc "sgmm-sum-tree-stats" </td><td> <pre> Sum SGMM-type statistics used for phonetic decision tree building.
Usage:  sgmm-sum-tree-stats [options] tree-accs-out trea-accs-in1 tree-accs-in2 ...
e.g.: sgmm-sum-tree-stats treeacc 1.streeacc 2.streeacc 3.streeacc </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-build-tree.cc "sgmm-build-tree" </td><td> <pre> Train decision tree
Usage: sgmm-build-tree [options] <old-sgmm-in> <tree-stats-in> <roots-file> <questions-file> <tree-out> [<sgmm-out>]
e.g.: sgmm-build-tree 0.sgmm streeacc roots.txt 1.qst tree </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-cluster-phones.cc "sgmm-cluster-phones" </td><td> <pre> Cluster phones (or sets of phones) into sets for various purposes
Usage: sgmm-cluster-phones [options] <sgmm-in> <tree-stats-in> <phone-sets-in> <clustered-phones-out>
e.g.: sgmm-cluster-phones 0.sgmm 1.tacc phonesets.txt questions.txt </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-init-from-tree-stats.cc "sgmm-init-from-tree-stats" </td><td> <pre> Initialize an SGMM from a previously built SGMM, a tree, 
and SGMM-type tree stats
Usage: sgmm-init-from-tree-stats [options] <old-sgmm> <tree> <sgmm-tree-stats> <sgmm-out> </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-est-ebw.cc "sgmm-est-ebw" </td><td> <pre> Estimate SGMM model parameters discriminatively using Extended
Baum-Welch style of update
Usage: sgmm-est-ebw [options] <model-in> <num-stats-in> <den-stats-in> <model-out> </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-acc-stats2.cc "sgmm-acc-stats2" </td><td> <pre> Accumulate numerator and denominator stats for discriminative training
of SGMMs (input is posteriors of mixed sign)
Usage: sgmm-acc-stats2 [options] <model-in> <feature-rspecifier> <posteriors-rspecifier> <num-stats-out> <den-stats-out>
e.g.: sgmm-acc-stats2 1.mdl 1.ali scp:train.scp ark:1.posts num.acc den.acc </pre> </td> </tr>
<tr> <td> \ref sgmmbin/sgmm-est-multi.cc "sgmm-est-multi" </td><td> <pre> Estimate multiple SGMM models from corresponding stats, such that the global parameters
(phone-, speaker-, and weight-projections and covariances) are tied across models.
Usage: sgmm-est-multi [options] <model1> <stats1> <model1_out> <occs1_out> [<model2> <stats2> <model2_out> <occs2_out> ...] </pre> </td> </tr>
